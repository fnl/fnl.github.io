<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>fnl en España - Florian Leitner</title>
        <link rel="stylesheet" href="http://fnl.es/theme/css/main.css" />
        <link href="http://fnl.es/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="fnl en España Atom Feed" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="http://fnl.es/">fnl en España </a></h1>
                <nav><ul>
                    <li><a href="http://fnl.es/pages/about.html">About</a></li>
                    <li><a href="http://fnl.es/pages/projects.html">Projects</a></li>
                    <li><a href="http://fnl.es/category/machine-learning.html">Machine Learning</a></li>
                    <li><a href="http://fnl.es/category/miscellaneous.html">Miscellaneous</a></li>
                    <li><a href="http://fnl.es/category/programming.html">Programming</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="http://fnl.es/a-quick-reference-for-working-with-tensorflow.html">A quick reference for working with TensorFlow</a></h1>
<footer class="post-info">
        <abbr class="published" title="2018-02-15T00:00:00+01:00">
                Published: Thu 15 February 2018
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/machine-learning.html">Machine Learning</a>.</p>
<p>tags: <a href="http://fnl.es/tag/tensorflow.html">tensorflow</a> <a href="http://fnl.es/tag/machine-learning.html">machine learning</a> </p>
</footer><!-- /.post-info --><div class="section" id="introduction">
<h2>Introduction</h2>
<p>Given the last few years of hype around Deep Learning, knowing one of those frameworks is probably no longer an option, at least if you are a professional Machine Learning engineer. Personally, I always favor free, open source solutions, so Apache MXNet would be the natural fit. However, I must admit that for research &amp; development, Facebook's PyTorch is probably the nicer API, and for operations &amp; production, Google's TensorFlow is beyond doubt the best fit right now. And, as I've mostly moved out of academia/research lately, and am pretty dedicated to consulting by now (and loving it!), I am mostly interested in a tooling that I can put to good use in my day-to-day work. Hence, I have been - not without remorse - focusing on TensorFlow and want to share my personal notes with you, to use as a quick reference when setting up a new tensor graph.</p>
<p>I mostly learned how to use TensorFlow from Aurélien Géron's book &quot;<a class="reference external" href="http://shop.oreilly.com/product/0636920052289.do">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>&quot;; So if you are familiar with it and my notes below remind you of it, that is not by accident: These are my modified excerpts, all taken from that book. In fact, if you really are interested in using TensorFlow, I cannot emphasize enough how much I recommend reading this book. And, if you are not familiar with the SciKit-Learn world, or simply want to learn how you can combine these two essential Machine Learning frameworks into &quot;one ring to rule them all&quot;, you need to read this book. Full-stop. In fact, if you are a serious Machine Learning practitioner, you either should have read it, or at least make sure you know the techniques presented in it already. Beyond just giving you tips, the book is chock-a-block full of up-to-date examples and highly relevant exercises. Aurélien even recently <a class="reference external" href="https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb">released more example code</a>, to produce G. E. Hinton's Capsule Network architecture (using dynamic routing) with TensorFlow, and generally ensures the practical material is very much up to date with the latest TensorFlow releases. (Disclaimer: I am in no way affiliated with Aurélien, O'Reilly, or would otherwise benefit from sales of this book!)</p>
<p>That being said, let's dive right in!</p>
</div>
<div class="section" id="setup">
<h2>Setup</h2>
<div class="section" id="miniconda">
<h3>Miniconda</h3>
<p>I strongly recommend using the <a class="reference external" href="https://www.anaconda.com/download/">Anaconda</a> distribution to set up TensorFlow, and Python in general. And, for the more expert users, do go directly to <a class="reference external" href="https://conda.io/miniconda.html">miniconda</a> (&quot;don't go over Anaconda, and don't collect 200MB of (irrelevant) bytes&quot;). The packages you want to install (and that will also fetch all other, relevant dependencies, like Jupyter, SciPy, or NumPy) are:</p>
<div class="highlight"><pre><span></span>conda install -c conda-forge tensorflow
conda install -c conda-forge scikit-learn
conda install -c conda-forge matplotlib
conda install -c conda-forge jupyter_contrib_nbextensions
</pre></div>
</div>
</div>
<div class="section" id="graph-design">
<h2>Graph Design</h2>
<div class="section" id="back-propagation-basics">
<h3>Back-propagation Basics</h3>
<p>Input is feed into TensorFlow's (static) computational graphs via <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/placeholder">tf.placeholder</a> (<strong>placeholder</strong>) nodes. Typically, those placeholders will be accepting a tensor <tt class="docutils literal">X</tt> and some array or matrix of labels <tt class="docutils literal">y</tt>. Input is part of the <a class="reference external" href="https://www.tensorflow.org/api_guides/python/io_ops">io_ops</a> package; Input values for placeholders must be provided (see &quot;Feeding the Graph&quot;) during graph evaluation, or cause exceptions if left unset.</p>
<p><strong>Weights</strong> <tt class="docutils literal">W</tt> and <strong>bias</strong> <tt class="docutils literal">B</tt> tensors are represented by <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Variable">tf.Variable</a> nodes, via the <a class="reference external" href="https://www.tensorflow.org/api_guides/python/state_ops">state_ops</a> package; <strong>Variables</strong> are stateful, that is, they maintain their values across session runs.
<strong>Operations</strong> are defined by either applying standard Python operators (+, -, /, *) to nodes, or by using the <tt class="docutils literal">tf.add</tt>, <tt class="docutils literal">tf.matmul</tt>, etc. functions from the TF <a class="reference external" href="https://www.tensorflow.org/api_guides/python/math_ops">math_ops</a> packages.</p>
<p>The final operation typically evaluates the error or cost between the predicted <tt class="docutils literal">y_hat</tt>, modeled as a state node, and [minus] the true <tt class="docutils literal">y</tt>, modeled as an io node, as shown above.
Common loss functions for this task are found in TensorFlow's <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/losses">losses</a> package.
The outcome of this operation (by following Aurélien nomenclature, the <tt class="docutils literal">cost_op</tt> node) is the one you typically want to plot on your TensorBoard (more on that at the end of this post).</p>
<div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span> <span class="c1"># n variables + 1 constant bias input</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="c1"># ... graph setup with tf.Variables ...</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="c1"># some last step...</span>
<span class="n">cost_op</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span>
</pre></div>
<p>The final touch is to create and append the optimizer (e.g, <tt class="docutils literal">tf.train.GradientDescentOptimizer</tt>):
That creates the <tt class="docutils literal">training_op</tt> (for minimizing the cost/error), which typically will be the node that gets sent to evaluations of a TensorFlow graph via a <strong>Session</strong> (<tt class="docutils literal">sess.run</tt>):</p>
<div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">()</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost_op</span><span class="p">)</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
</pre></div>
</div>
<div class="section" id="placeholder-nodes">
<h3>Placeholder Nodes</h3>
<p>As discussed already, to supply data to your TF graph you designed, you insert <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/placeholder">tf.placeholder</a> nodes in the graph (e.g., in the bottom-most layer of your net). Placeholders don’t perform any computation, they just output any data you tell them to, during the graph evaluation (&quot;execution&quot;) phase. Optionally, you can also specify the node's <tt class="docutils literal">shape</tt> , if you want to enforce it: And, if you furthermore specify <tt class="docutils literal">None</tt> for any tensor dimension, that dimension will adapt to any size (according to the next node's input).</p>
<p>In the example shown below, the placeholder <tt class="docutils literal">A</tt> must be of rank 2 (i.e., two-dimensional), and the tensor must have three columns, but it can have any number of rows (which typically will be the current batch' examples).</p>
<div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<div class="section" id="reassignable-variables">
<h3>Reassignable Variables</h3>
<p>Note that it is possible to feed values into variables, too, not just to placeholders, even if it is a tad unusual.
To set a variable to some value during graph evaluation, use the <a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/assign">tf.assign</a> state operator:</p>
<div class="highlight"><pre><span></span><span class="c1"># sample a random variable:</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">minval</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>
<span class="c1"># or feed a variable:</span>
<span class="n">x_new_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_assign</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_new_val</span><span class="p">)</span>

<span class="c1"># now, you can feed new values into X:</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">():</span>
    <span class="n">x_assign</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x_new_val</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
    <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span> <span class="c1"># always 0.5</span>
</pre></div>
</div>
<div class="section" id="optimizing-the-graph">
<h3>Optimizing the Graph</h3>
<p>The <a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/gradients">tf.gradients</a> function takes a cost operator (e.g., to calculate the MSE) and a list of variables to optimize, and creates a list of ops (one per variable) to compute the gradients of each op with regard to each variable, returning the desired list of gradients (aka. performa <em>reverse-mode autodiff</em>):</p>
<div class="highlight"><pre><span></span><span class="p">[</span><span class="n">var1grad</span><span class="p">,</span> <span class="n">var2grad</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">cost_op</span><span class="p">,</span> <span class="p">[</span><span class="n">var1</span><span class="p">,</span> <span class="n">var2</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>
</pre></div>
<p>However, TensorFlow provides a good number of <a class="reference external" href="https://www.tensorflow.org/api_guides/python/train#Optimizers">optimizers</a> right out of the box, for example, the Adam optimizer, saving you the need to explicitly calculate gradients or update variables yourself:</p>
<div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">()</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost_op</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="adding-regularization">
<h3>Adding Regularization</h3>
<p>Regularization (typically, L1 or L2) prevents overfitting and therefore allows you to train your model for more epochs. Simply add the appropriate operations to your graph, to get to a regularized cost (or <tt class="docutils literal">loss</tt>):</p>
<div class="highlight"><pre><span></span><span class="o">...</span> <span class="c1"># construct the neural network</span>
<span class="n">base_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">xentropy</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;avg_xentropy&quot;</span><span class="p">)</span>
<span class="n">reg_losses</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weights1</span><span class="p">))</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weights2</span><span class="p">))</span> <span class="c1"># L1</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">base_loss</span><span class="p">,</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">reg_losses</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
</pre></div>
<p>However, if you have many layers, this approach quickly becomes inconvenient. Instead,
most TensorFlow functions in the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers">tf.contrib.layers</a> package that create variables accept a <tt class="docutils literal"><span class="pre">..._regularizer</span></tt> argument for their weights and biases.
Those arguments need to be functions that takes the weights as their argument, and return the regularization losses of that layer.</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">arg_scope</span><span class="p">(</span>
        <span class="p">[</span><span class="n">fully_connected</span><span class="p">],</span>
        <span class="n">weights_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">l1_regularizer</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)):</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span> <span class="n">n_hidden2</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden2&quot;</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;out&quot;</span><span class="p">)</span>
</pre></div>
<p>TensorFlow automatically adds these nodes to a special collection containing all the regularization losses.
Then, when calculating the final loss, you add them up to find your overall, final loss:</p>
<div class="highlight"><pre><span></span><span class="n">reg_losses</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">REGULARIZATION_LOSSES</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">([</span><span class="n">base_loss</span><span class="p">]</span> <span class="o">+</span> <span class="n">reg_losses</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
</pre></div>
<p><strong>Max-norm regularization</strong> (clipping the L2-norm at some threshold) has become quite popular, yet TensorFlow does not provide an off-the-shelf max-norm regularizer. The following code creates a node <tt class="docutils literal">clip_weights</tt> that will clip your <tt class="docutils literal">weights</tt> along their second axis ( <tt class="docutils literal">axes=1</tt> ), so that every resulting row vector will have a max-norm of 1.0:</p>
<div class="highlight"><pre><span></span><span class="n">threshold</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">clipped_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">clip_norm</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clip_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">clipped_weights</span><span class="p">)</span>
</pre></div>
<p>The issue then becomes accessing the weights of a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers">tf.contrib.layers</a> module; A better solution therefore is to create a function equivalent to the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/l1_regularizer">l1_regularizer</a> found in the layers module:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">max_norm_regularizer</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;max_norm&quot;</span><span class="p">,</span>
                         <span class="n">collection</span><span class="o">=</span><span class="s2">&quot;max_norm&quot;</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">max_norm</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
        <span class="n">clipped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">clip_norm</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">)</span>
        <span class="n">clip_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">clipped</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">clip_weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">None</span>  <span class="c1"># there is no regularization loss term</span>
    <span class="k">return</span> <span class="n">max_norm</span>
</pre></div>
<p>Now, this regularization function can be used as an argument like any other regularizer would be:</p>
<div class="highlight"><pre><span></span><span class="n">hidden1</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;hidden1&quot;</span><span class="p">,</span>
                          <span class="n">weights_regularizer</span><span class="o">=</span><span class="n">max_norm_regularizer</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>
</pre></div>
<p>And to actually clip weights using max-norm during session evaluation, finally, add this to your execution phase:</p>
<div class="highlight"><pre><span></span><span class="n">clip_all_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s2">&quot;max_norm&quot;</span><span class="p">)</span> <span class="c1"># note we used &quot;max_norm&quot; above</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="p">[</span><span class="o">...</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="p">[</span><span class="o">...</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_batches</span><span class="p">,</span> <span class="n">y_batches</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">clip_all_weights</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="scopes-modules-and-shared-variables">
<h3>Scopes, Modules, and Shared Variables</h3>
<p>Variables and nodes created within a named scope are prefixed with the scopes' name, and such scopes are collapsed into single nodes on the TensorBoard:</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">error</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">error</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="n">loss</span><span class="o">/</span><span class="n">sub</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">mse</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="n">loss</span><span class="o">/</span><span class="n">mse</span>
</pre></div>
<p>E.g., to define a ReLU within a named scope (just in case: normally, you would use <tt class="docutils literal">tf.nn.relu</tt> ):</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">):</span>
        <span class="n">w_shape</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weights&quot;</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
</pre></div>
<p>If you want to share a variable between various components of your graph, (e.g., thresholds, biases, etc.), TensorFlow provides the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_variable">tf.get_variable</a> function, that creates a shared variable if it does not exist, or reuses it, if it does. The desired behavior (creating or reusing) is controlled by an attribute of the current <a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/variable_scope">tf.variable_scope</a>, <tt class="docutils literal">reuse</tt> . Note that <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_variable">tf.get_variable</a> raises an exception if <tt class="docutils literal">reuse</tt> is <tt class="docutils literal">False</tt> or <tt class="docutils literal">scope.reuse_variables()</tt> has not been set.</p>
<div class="highlight"><pre><span></span><span class="c1"># as attribute:</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;threshold&quot;</span><span class="p">)</span>

<span class="c1"># as function:</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="n">scope</span><span class="o">.</span><span class="n">reuse_variables</span><span class="p">()</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;threshold&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="xavier-and-he-initialization-of-model-weights">
<h3>Xavier and He Initialization of Model Weights</h3>
<p>You need to use an initializer to avoid the uniform initialization of weights to all the same values.</p>
<p>By default, TensorFlow's layers are initialized using Xavier initialization: The fully connected layers in <tt class="docutils literal">tf.contrib.layers</tt> use Xavier initialization, with a normal dist. using µ = <tt class="docutils literal">0</tt>, sigma = <tt class="docutils literal">sqrt[ 2 / (n_in + n_out) ]</tt> or a uniform dist. using <tt class="docutils literal">+/- sqrt[ 6 / (n_in + n_out) ]</tt> , where the <tt class="docutils literal">n</tt>'s are the sizes of the input/output connections.</p>
<p>To use He initialization instead (which is mostly a matter of preference, but has been made popular with ResNet), you can use variance scaling initialization:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tf.contrib.layers</span> <span class="kn">import</span> <span class="n">fully_connected</span><span class="p">,</span> <span class="n">variance_scaling_initializer</span>
<span class="n">initializer</span> <span class="o">=</span> <span class="n">variance_scaling_initializer</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;FAN_AVG&quot;</span><span class="p">)</span>
<span class="n">hidden1</span> <span class="o">=</span> <span class="n">fully_connected</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_hidden1</span><span class="p">,</span> <span class="n">weights_initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&quot;h1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="implementing-a-learning-rate-scheduler">
<h3>Implementing a Learning Rate Scheduler</h3>
<p>Normally, it is not necessary to add a learning rate scheduler, because the AdaGrad, RMSProp, and Adam optimizers automatically reduce the learning rate for you during training. Yet, implementing a learning rate scheduler is fairly straightforward with TensorFlow; Typically, exponential decay is recommended, because it is easy to tune and will converge (slightly) faster than the optimal solution. Here, we adapt Momentum to use a dynamic learning rate: Note how the decay depends on the current global step that is set by the optimizer's minimization function.</p>
<div class="highlight"><pre><span></span><span class="n">initial_learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">decay_steps</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">decay_rate</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">exponential_decay</span><span class="p">(</span><span class="n">initial_learning_rate</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span>
                                           <span class="n">decay_steps</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">training_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost_op</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="interactive-sessions">
<h2>Interactive Sessions</h2>
<p>Before we get to the actual evaluation of TF graphs with sessions, let me add in a few tips that come in handy when working in interactive Python sessions.</p>
<div class="section" id="resetting-the-default-graph">
<h3>Resetting the Default Graph</h3>
<div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
</pre></div>
<p>In Jupyter (or if using TF in a Python shell), it is common to run the same commands more than once while you are experimenting. As a result, you may end up with a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_default_graph">default graph</a> containing many duplicate nodes. One solution is to restart the Jupyter kernel (or the Python shell), but a more convenient solution is to just reset the default graph by running:</p>
<div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
</pre></div>
<p>In single-process TensorFlow, multiple sessions do not share any state, even if they reuse the same graph (and each session gets its own copy of every variable). Beware though, that in distributed TensorFlow, variable state is stored on the servers, not in the sessions, so multiple sessions can <em>share</em> the same <em>variables</em> (actually, that is a good, desired thing, obviously).</p>
</div>
<div class="section" id="using-tensorflow-s-interactive-sessions">
<h3>Using TensorFlow's [Interactive] Sessions</h3>
<p>Use <tt class="docutils literal">InteractiveSession</tt> in notebooks to automatically set a default session, relieving you from the need of a <tt class="docutils literal">with</tt> block for the evaluation/execution phase. But do remember to close the session manually when you are done with it!</p>
<div class="highlight"><pre><span></span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">InteractiveSession</span><span class="p">()</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="o">...</span> <span class="c1"># do graph setup</span>
<span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="o">...</span> <span class="c1"># do evaluation</span>
<span class="n">sess</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
<p>When running TensorFlow <strong>locally</strong>, the sessions manage your variable values. So if you create a graph, then start two threads, and open a local session in either thread, both will use the same graph, yet each session will have its <em>own</em> copy of the variables.</p>
<p>However, in <strong>distributed</strong> TensorFlow sessions, variable values are stored in containers managed by the TF cluster (see <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_variable">tf.get_variable</a>). So if both sessions connect to the same cluster and use the same container, then they will share the same variable value for w.</p>
</div>
</div>
<div class="section" id="model-evaluation">
<h2>Model Evaluation</h2>
<div class="section" id="scaling-variables">
<h3>Scaling Variables</h3>
<p>When using a Gradient Descent method, remember that it is important to <strong>normalize</strong> the input feature vectors, or else training may progress much slower.
You can do this using TensorFlow, NumPy, Scikit-Learn’s StandardScaler, or any other solution you prefer. In fact, with NumPy arrays, this is pretty straightforward:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">scaled</span> <span class="o">=</span> <span class="n">data</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="running-the-graph">
<h3>Running the Graph</h3>
<p>Once the graph has been designed (incl. a <tt class="docutils literal">training_op</tt> node) and the initializer (an <tt class="docutils literal">init</tt> node) has been set up (see Graph Design), a typical snippet for the (batched) executing of the training phase is:</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">num_examples</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
</pre></div>
</div>
<div class="section" id="feeding-the-graph-with-data">
<h3>Feeding the Graph with Data</h3>
<p>When you evaluate the graph, you pass a feed dictionary (<tt class="docutils literal">feed_dict</tt>) to the target (&quot;output&quot;) node's <tt class="docutils literal">eval()</tt> method. And you specify the value of the placeholder (input) node by using the node itself as the key of the feed dictionary.</p>
<div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="o">...</span> <span class="c1"># more graph setup, down to the training_op</span>
<span class="n">training_op</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">A</span><span class="p">:</span> <span class="n">data</span><span class="p">})</span>  <span class="c1"># &lt;- feeding</span>
</pre></div>
<p>Note that you can feed data into <em>any</em> kind of node, not just placeholders. Note that when using other nodes, TensorFlow will not evaluate their operations; If fed to, TF uses the values you feed to that node, only (see Reassignable Variables in Graph Design for more info).</p>
</div>
<div class="section" id="mini-batching-with-tensorflow">
<h3>Mini-batching with TensorFlow</h3>
<p>Instead of feeding all data at once, you typically will mini-batch your data as follows:</p>
<ol class="arabic simple">
<li>Create a session (<tt class="docutils literal">with tf.Session() as sess</tt>)</li>
<li>Run the variable initializer (<tt class="docutils literal">sess.run(init)</tt>)</li>
<li>Loop over the epochs and batches, feeding each mini-batch to the session (<tt class="docutils literal">sess.run(training_op, <span class="pre">feed_dict={X:</span> X_batch, y: y_batch})</tt>)</li>
<li>Optionally: Write a summary every n mini-batches, to visualize the progress on your TensorBoard.</li>
</ol>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="c1"># somehow fetch data and labels (numpy arrays) to feed...</span>
    <span class="k">return</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span>

<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>
        <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
</pre></div>
</div>
<div class="section" id="saving-and-restoring-models">
<h3>Saving and Restoring Models</h3>
<p>Create a Saver node at the end of the construction phase (after all variable nodes are created); Then, during the execution phase, call the node's <tt class="docutils literal">save()</tt> method whenever you want to save the model, passing it the session and path of the <strong>checkpoint</strong> file to create:</p>
<div class="highlight"><pre><span></span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/my_classifier.tfckpt&quot;</span>
<span class="n">checkpoint_epoch_path</span> <span class="o">=</span> <span class="n">checkpoint_path</span> <span class="o">+</span> <span class="s2">&quot;.tfepoch&quot;</span>
<span class="n">final_model_path</span> <span class="o">=</span> <span class="s2">&quot;./my_classifier.tfmodel&quot;</span>
<span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">infty</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">checkpoint_epoch_path</span><span class="p">):</span>
        <span class="c1"># if the checkpoint file exists, restore the model and load the epoch number</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">checkpoint_epoch_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">start_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Training was interrupted. Continuing at epoch&quot;</span><span class="p">,</span> <span class="n">start_epoch</span><span class="p">)</span>
        <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># checkpoint every 100 epochs</span>
            <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">checkpoint_epoch_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">loss_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">loss_val</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
            <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">final_model_path</span><span class="p">)</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss_val</span>
            <span class="c1"># best_parameters = parameters.eval()</span>
</pre></div>
<p>To use a trained model <em>in production</em>, restoring a model is just as easy: You create a Saver node at the end of the graph, just like before, but then, when beginning the execution phase, instead of initializing the variables using the typical <tt class="docutils literal">init</tt> node, you call the <tt class="docutils literal">restore()</tt> method of the Saver object:</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./my_model_final.ckpt&quot;</span><span class="p">)</span>
    <span class="n">X_unseen</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>  <span class="c1"># some unseen (scaled) data</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_unseen</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="section" id="monitoring-with-tensorboard">
<h2>Monitoring with TensorBoard</h2>
<p>One of the biggest advantages of TensorFlow over many other frameworks is the TensorBoard. It allows you to visualize the progression of any variable in your graph.</p>
<div class="section" id="writing-session-summaries">
<h3>Writing Session Summaries</h3>
<p>To provide TensorBoard with data, you need to write TF's graph definition and some training stats (like the cost/loss) to a log directory that TensorBoard reads from. You need to use a different log directory on every run, to avoid that TensorBoard will merge the output of different runs. The solution to this here will be to include a timestamp in the log directory name.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="n">root_logdir</span> <span class="o">=</span> <span class="s2">&quot;tf_logs&quot;</span>
<span class="n">now</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">utcnow</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y%m</span><span class="si">%d</span><span class="s2">%H%M%S&quot;</span><span class="p">)</span>
<span class="n">logdir</span> <span class="o">=</span> <span class="s2">&quot;{}/run-{}/&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">root_logdir</span><span class="p">,</span> <span class="n">now</span><span class="p">)</span>
</pre></div>
<p>Next, add a <strong>summary node</strong> and attach a <strong>file writer</strong> to the node you wish to visualize on your TensorBoard; The <tt class="docutils literal">FileWriter</tt> shown below will create any missing directories for you:</p>
<div class="highlight"><pre><span></span><span class="n">mse_summary</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s1">&#39;MSE&#39;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
<span class="n">file_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">FileWriter</span><span class="p">(</span><span class="n">logdir</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span>
</pre></div>
<p>The first line creates a node in the graph that will evaluate the MSE value and write it to a TensorBoard-compatible binary log string called a <strong>summary</strong>. The second line creates a <tt class="docutils literal">FileWriter</tt> that you will use to write summaries into the log directory. The second (optional) parameter is the graph you want to visualize. Upon creation, the FileWriter creates the directory path if it does not exist, and writes the graph definition in a binary log file called an <strong>events</strong> file.</p>
<p>Next, you need to update the execution phase, to evaluate the summary node regularly during training, and you should not forget to close the writer after training:</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="p">[</span><span class="o">...</span><span class="p">]</span>
    <span class="n">update_summary</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>
        <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">fetch_batch</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">update_summary</span><span class="p">(</span><span class="n">batch_index</span><span class="p">):</span>
            <span class="n">summary_str</span> <span class="o">=</span> <span class="n">mse_summary</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
            <span class="n">step</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">*</span> <span class="n">n_batches</span> <span class="o">+</span> <span class="n">batch_index</span>
            <span class="n">file_writer</span><span class="o">.</span><span class="n">add_summary</span><span class="p">(</span><span class="n">summary_str</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">training_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">X</span><span class="p">:</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">})</span>
    <span class="p">[</span><span class="o">...</span><span class="p">]</span>

<span class="n">file_writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
<p>Finally, you now can visualize the stats you are recording by starting the TensorBoard server and pointing it at the log directory:</p>
<div class="highlight"><pre><span></span>$ tensorboard --logdir tf_logs/
</pre></div>
</div>
</div>
<div class="section" id="epilogue">
<h2>Epilogue</h2>
<p>As I already advised in the beginning, if you want to learn more, I can warmly recommend you get Aurélien Géron's fantastic book &quot;<a class="reference external" href="http://shop.oreilly.com/product/0636920052289.do">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>&quot;; The more advanced topics covered (and that would explode this blog post...) are transfer learning, distributed training, designing Recurrent Networks and Auto-encoders, and even a &quot;beginner's guide&quot; to Deep Reinforcement Learning. Yet, I hope, this tiny taste of the book's contents, spiced up with a bit of my own &quot;opinionated&quot; modifications, will provide you with  a handy quick-reference when building and using TensorFlow graphs. (And, that I don't get sued by him or O'Reilly for plagiarism! :-) Please just contact me if this post is an issue -- I have no problem taking the post down again, if it is problematic.)</p>
</div>
                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/what-are-the-best-books-for-programmers-to-get-into-data-science.html" rel="bookmark"
                           title="Permalink to What are the best books [for programmers] to get into Data Science?">What are the best books [for programmers] to get into Data Science?</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2015-08-07T00:00:00+02:00">
                Published: Fri 07 August 2015
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/machine-learning.html">Machine Learning</a>.</p>
<p>tags: <a href="http://fnl.es/tag/data-science.html">data science</a> <a href="http://fnl.es/tag/review.html">review</a> </p>
</footer><!-- /.post-info -->                <div class="section" id="introduction">
<h2>Introduction</h2>
<p>This is a question I get on a frequent basis by colleagues that are serious programmers or software developers and are planning to pick up data analytics. There are three fundamental topics in mathematics that you need to cover, assuming that you are an expert in software development and …</p></div>
                <a class="readmore" href="http://fnl.es/what-are-the-best-books-for-programmers-to-get-into-data-science.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/an-efficient-online-sequence-tagger-resource-for-gate.html" rel="bookmark"
                           title="Permalink to An efficient online sequence tagger resource for GATE">An efficient online sequence tagger resource for GATE</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2015-04-06T00:00:00+02:00">
                Published: Mon 06 April 2015
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/machine-learning.html">Machine Learning</a>.</p>
<p>tags: <a href="http://fnl.es/tag/text-mining.html">text mining</a> <a href="http://fnl.es/tag/nlp.html">nlp</a> <a href="http://fnl.es/tag/java.html">Java</a> </p>
</footer><!-- /.post-info -->                <p><strong>tl;dr</strong> for a stressed out generation:
<a class="reference external" href="https://gate.ac.uk">GATE</a>'s <a class="reference external" href="https://gate.ac.uk/sale/tao/splitch23.html#sec:parsers:taggerframework">Generic Tagger</a> framework is a CREOLE plug-in that allows you to wrap any existing <a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html">sequence tagger</a> and use it to create annotations in your pipeline, but it is a bit slow.
Therefore, I have created the <a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework">Online Tagger</a> GATE plug-in that …</p>
                <a class="readmore" href="http://fnl.es/an-efficient-online-sequence-tagger-resource-for-gate.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/segtok-a-segmentation-and-tokenization-library.html" rel="bookmark"
                           title="Permalink to segtok - a segmentation and tokenization library">segtok - a segmentation and tokenization library</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2015-01-12T00:00:00+01:00">
                Published: Mon 12 January 2015
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/machine-learning.html">Machine Learning</a>.</p>
<p>tags: <a href="http://fnl.es/tag/text-mining.html">text mining</a> <a href="http://fnl.es/tag/nlp.html">nlp</a> <a href="http://fnl.es/tag/python.html">Python</a> </p>
</footer><!-- /.post-info -->                <p><strong>tl;dr</strong>
Surprisingly, it is hard to find a good command-line tool for sentence segmentation and word tokenization that works well with European languages.
Here, I present <a class="reference external" href="https://pypi.python.org/pypi/segtok">segtok</a>, a <strong>Python 2.7 and 3</strong> package, API, and Unix command-line tool to remedy this shortcoming.</p>
<div class="section" id="text-processing-pipelines">
<h2>Text processing pipelines</h2>
<p>This is the …</p></div>
                <a class="readmore" href="http://fnl.es/segtok-a-segmentation-and-tokenization-library.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/a-review-of-sparse-sequence-taggers.html" rel="bookmark"
                           title="Permalink to A review of sparse sequence taggers">A review of sparse sequence taggers</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2014-10-02T00:00:00+02:00">
                Published: Thu 02 October 2014
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/machine-learning.html">Machine Learning</a>.</p>
<p>tags: <a href="http://fnl.es/tag/text-mining.html">text mining</a> <a href="http://fnl.es/tag/nlp.html">nlp</a> <a href="http://fnl.es/tag/probabilistic-programming.html">probabilistic programming</a> </p>
</footer><!-- /.post-info -->                <div class="section" id="introduction">
<h2>Introduction</h2>
<p><strong>tl;dr</strong>
Right now, use <a class="reference external" href="http://wapiti.limsi.fr/">Wapiti</a> <em>unless</em> you want to go beyond first-order and/or linear models, need the fastest possible training cycles, or are a Scala programmer, in which case you would be best advised to choose <a class="reference external" href="http://factorie.cs.umass.edu/">Factorie</a>.
OK, so that's that for a stressed out generation;
Read …</p></div>
                <a class="readmore" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/medline-kung-fu.html" rel="bookmark"
                           title="Permalink to MEDLINE Kung-Fu">MEDLINE Kung-Fu</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2014-09-11T00:00:00+02:00">
                Published: Thu 11 September 2014
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/programming.html">Programming</a>.</p>
<p>tags: <a href="http://fnl.es/tag/text-mining.html">text mining</a> <a href="http://fnl.es/tag/bionlp.html">bionlp</a> <a href="http://fnl.es/tag/pubmed.html">pubmed</a> </p>
</footer><!-- /.post-info -->                <p>If you are a computational linguist, data analyst, or bioinformatician working with biological text corpora (on medicine, neuroscience, molecular biology, etc.), you will rather sooner than later need access to <a class="reference external" href="http://www.nlm.nih.gov/bsd/pmresources.html">MEDLINE</a>.
Right now, the MEDLINE <a class="reference external" href="http://www.nlm.nih.gov/pubs/factsheets/dif_med_pub.html">subset</a> (&quot;baseline&quot;) of PubMed contains nearly <a class="reference external" href="http://www.nlm.nih.gov/bsd/licensee/baselinestats.html">23 million records</a>, all with titles, author names, etc …</p>
                <a class="readmore" href="http://fnl.es/medline-kung-fu.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/an-introduction-to-statistical-text-mining.html" rel="bookmark"
                           title="Permalink to An Introduction to Statistical Text Mining">An Introduction to Statistical Text Mining</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2014-07-07T00:00:00+02:00">
                Published: Mon 07 July 2014
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/machine-learning.html">Machine Learning</a>.</p>
<p>tags: <a href="http://fnl.es/tag/text-mining.html">text mining</a> <a href="http://fnl.es/tag/python.html">python</a> <a href="http://fnl.es/tag/nltk.html">nltk</a> </p>
</footer><!-- /.post-info -->                <p><em>Update</em> 2015-07-24: Please check out the <a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-from-bayes-rule-to-de">latest slides</a> for this course, which now includes a quick introduction to dependency parsing, a more terse start, and several corrections and improvements all over.</p>
<p>Last week we had a really great time at the first hands-on text mining workshop in the context of …</p>
                <a class="readmore" href="http://fnl.es/an-introduction-to-statistical-text-mining.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/getting-started-with-a-virtual-go-environment.html" rel="bookmark"
                           title="Permalink to Getting started with a "virtual" Go environment">Getting started with a &quot;virtual&quot; Go environment</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2013-11-29T00:00:00+01:00">
                Published: Fri 29 November 2013
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/programming.html">Programming</a>.</p>
<p>tags: <a href="http://fnl.es/tag/golang.html">golang</a> <a href="http://fnl.es/tag/posix.html">posix</a> </p>
</footer><!-- /.post-info -->                <p>Given how easy it is to write highly concurrent code in Go (aka &quot;<a class="reference external" href="http://golang.org">golang</a>&quot;), it is probably worth learning this language.
Personally, I believe Go is not yet mature enough for &quot;production&quot; projects other than servers maybe (and I am sure there are people who will not agree with my …</p>
                <a class="readmore" href="http://fnl.es/getting-started-with-a-virtual-go-environment.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/concurrent-nodejs.html" rel="bookmark"
                           title="Permalink to Concurrent Node.js">Concurrent Node.js</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2013-10-08T00:00:00+02:00">
                Published: Tue 08 October 2013
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/programming.html">Programming</a>.</p>
<p>tags: <a href="http://fnl.es/tag/javascript.html">javascript</a> <a href="http://fnl.es/tag/nodejs.html">node.js</a> </p>
</footer><!-- /.post-info -->                <div class="section" id="introduction">
<h2>Introduction</h2>
<p>Recently, a <a class="reference external" href="https://twitter.com/SoftActiva">colleague</a> of mine asked me to introduce the most important concepts of <a class="reference external" href="http://nodejs.org/">Node</a> programming to a flock of interested people in our <a class="reference external" href="http://www.cnio.es/es/grupos/plantillas/presentacion.asp?grupo=50004294">research group</a>.
Initially, I declined, considering the <a class="reference external" href="http://howtonode.org/">vast</a> <a class="reference external" href="http://docs.nodejitsu.com/">number</a> of <a class="reference external" href="http://www.nodebeginner.org/">tutorials</a> and <a class="reference external" href="https://duckduckgo.com/?q=node.js+book">books</a>, but then thought it might be quite an interesting challenge:
Is there …</p></div>
                <a class="readmore" href="http://fnl.es/concurrent-nodejs.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/installing-a-full-stack-python-data-analysis-environment-on-osx.html" rel="bookmark"
                           title="Permalink to Installing a full stack Python data analysis environment on OSX">Installing a full stack Python data analysis environment on OSX</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2013-02-11T00:00:00+01:00">
                Published: Mon 11 February 2013
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/programming.html">Programming</a>.</p>
<p>tags: <a href="http://fnl.es/tag/python.html">python</a> <a href="http://fnl.es/tag/apple.html">apple</a> <a href="http://fnl.es/tag/data-mining.html">data mining</a> </p>
</footer><!-- /.post-info -->                <p><strong>UPDATE</strong>: Installing the Scientific Python stack from &quot;source&quot; has become a lot simpler recently and this tutorial was updated accordingly in November 2013 to use with OSX Mavericks and, in particular, <strong>Python 3</strong>.</p>
<p>Installing a full-stack scientific data analysis environment on Mac OSX for Python 3 and making sure the …</p>
                <a class="readmore" href="http://fnl.es/installing-a-full-stack-python-data-analysis-environment-on-osx.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/rails-rspecing-controllers-with-declarative-authorization-and-authlogic.html" rel="bookmark"
                           title="Permalink to Rails: RSpec'ing controllers with declarative authorization AND AuthLogic">Rails: RSpec'ing controllers with declarative authorization AND AuthLogic</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2010-03-12T00:00:00+01:00">
                Published: Fri 12 March 2010
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/programming.html">Programming</a>.</p>
<p>tags: <a href="http://fnl.es/tag/rspec.html">rspec</a> <a href="http://fnl.es/tag/rails.html">rails</a> </p>
</footer><!-- /.post-info -->                <p>I just had a rough time figuring out how to bypass all the security
features of the Rails project I am developing to write decent controller
specs with RSpec. I am using AuthLogic as authentication module and
declarative authorization (DA) for exactly that. However, when I started
to write controller …</p>
                <a class="readmore" href="http://fnl.es/rails-rspecing-controllers-with-declarative-authorization-and-authlogic.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/mobileme-vs-sugarsync-vs-dropbox.html" rel="bookmark"
                           title="Permalink to MobileMe vs. SugarSync vs. DropBox">MobileMe vs. SugarSync vs. DropBox</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2009-05-26T00:00:00+02:00">
                Published: Tue 26 May 2009
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/miscellaneous.html">Miscellaneous</a>.</p>
<p>tags: <a href="http://fnl.es/tag/apple.html">apple</a> <a href="http://fnl.es/tag/cloud-storage.html">cloud storage</a> </p>
</footer><!-- /.post-info -->                <p>I now have tested MobileMe, SugarSync, and DropBox for quite a while to
decide which service to buy for syncing my “electronic life” between my
Macs (soon I’ll be managing two OSX Server blades, one Mini, and two
MBPs!). After this period, there is no doubt to me: I …</p>
                <a class="readmore" href="http://fnl.es/mobileme-vs-sugarsync-vs-dropbox.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/news-swines-pigs.html" rel="bookmark"
                           title="Permalink to News, Swines & Pigs">News, Swines &amp; Pigs</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2009-04-30T00:00:00+02:00">
                Published: Thu 30 April 2009
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/miscellaneous.html">Miscellaneous</a>.</p>
<p>tags: <a href="http://fnl.es/tag/politics.html">politics</a> </p>
</footer><!-- /.post-info -->                <p>Usually, I prefer to steer free from the day-to-day mainstream news, yet
even I have to accept a low level of &quot;noise&quot; if I want to know at least
something about the most significant things going on. However, currently
I get the overwhelming feeling that the whole news world is …</p>
                <a class="readmore" href="http://fnl.es/news-swines-pigs.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/why-i-love-python-30-unicode-utf-8.html" rel="bookmark"
                           title="Permalink to Why I love Python 3.0: Unicode + UTF-8">Why I love Python 3.0: Unicode + UTF-8</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2009-04-27T00:00:00+02:00">
                Published: Mon 27 April 2009
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/programming.html">Programming</a>.</p>
<p>tags: <a href="http://fnl.es/tag/unicode.html">unicode</a> <a href="http://fnl.es/tag/python.html">python</a> </p>
</footer><!-- /.post-info -->                <div class="section" id="tl-dr-summary">
<h2>tl;dr summary</h2>
<table> <tbody>
<tr> <th align="left"> <strong>Python pre-3.0</strong> </th>
     <th align="left"> <strong>Python post-3.0</strong> </th> </tr>
<tr> <td> str.encode </td> <td> bytes.translate or (new) str.encode </td> </tr>
<tr> <td> str.decode </td> <td> bytes.decode </td> </tr>
<tr> <td> unicode </td> <td> str </td> </tr>
<tr> <td> unicode.encode </td> <td> str.encode </td> </tr>
<tr> <td> unicode.decode </td> <td> *n/a* </td> </tr>
<tr> <td> str("x") == unicode("x") &nbsp;&nbsp; </td> <td> bytes("x") != str("x") </td> </tr>
</tbody> </table><p>This change in Python 3.0 might be more than useful …</p></div>
                <a class="readmore" href="http://fnl.es/why-i-love-python-30-unicode-utf-8.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://fnl.es/textmate-python-and-django-cheat-sheet.html" rel="bookmark"
                           title="Permalink to TextMate Python and Django cheat sheet">TextMate Python and Django cheat sheet</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2007-09-07T00:00:00+02:00">
                Published: Fri 07 September 2007
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://fnl.es/author/florian-leitner.html">Florian Leitner</a>
        </address>
<p>In <a href="http://fnl.es/category/programming.html">Programming</a>.</p>
<p>tags: <a href="http://fnl.es/tag/python.html">python</a> <a href="http://fnl.es/tag/django.html">django</a> </p>
</footer><!-- /.post-info -->                <p>After not finding anything appropriate, I decided to do my own reference
card (aka cheat sheet) for the pythonic and django commands you can use
in TextMate. If you want to have it, download it from <a class="reference external" href="http://www.scribd.com/doc/7759743/TextMate-PythonDjango-Cheat-Sheet">here</a>.</p>

                <a class="readmore" href="http://fnl.es/textmate-python-and-django-cheat-sheet.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="http://fnl.es/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>