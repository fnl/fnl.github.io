<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>fnl en España</title><link href="https://fnl.es/" rel="alternate"></link><link href="https://fnl.es/feeds/all.atom.xml" rel="self"></link><id>https://fnl.es/</id><updated>2021-10-04T00:00:00+02:00</updated><entry><title>Lean, generative cultures and how to probe for them</title><link href="https://fnl.es/lean-generative-cultures-and-how-to-probe-for-them.html" rel="alternate"></link><published>2021-10-04T00:00:00+02:00</published><updated>2021-10-04T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2021-10-04:/lean-generative-cultures-and-how-to-probe-for-them.html</id><summary type="html">&lt;p&gt;Performance-oriented organizations that value transparency and empower
their workforce were baptized &lt;a class="reference external" href="https://qualitysafety.bmj.com/content/13/suppl_2/ii22"&gt;generative
cultures&lt;/a&gt; by
Ron Westrum in 2004. This article will describe why
&lt;a class="reference external" href="https://www.lean.org/explore-lean/what-is-lean/"&gt;lean&lt;/a&gt;, generative
cultures might interest the reader and point out the &amp;quot;secrete sauce&amp;quot; in
creating them: transforming &lt;a class="reference external" href="https://cdn.csu.edu.au/__data/assets/pdf_file/0008/917018/Eight-Behaviors-for-Smarter-Teams-2.pdf"&gt;behavior and
communication&lt;/a&gt;.
Note that despite having &amp;quot;lean&amp;quot; in the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Performance-oriented organizations that value transparency and empower
their workforce were baptized &lt;a class="reference external" href="https://qualitysafety.bmj.com/content/13/suppl_2/ii22"&gt;generative
cultures&lt;/a&gt; by
Ron Westrum in 2004. This article will describe why
&lt;a class="reference external" href="https://www.lean.org/explore-lean/what-is-lean/"&gt;lean&lt;/a&gt;, generative
cultures might interest the reader and point out the &amp;quot;secrete sauce&amp;quot; in
creating them: transforming &lt;a class="reference external" href="https://cdn.csu.edu.au/__data/assets/pdf_file/0008/917018/Eight-Behaviors-for-Smarter-Teams-2.pdf"&gt;behavior and
communication&lt;/a&gt;.
Note that despite having &amp;quot;lean&amp;quot; in the title, the text will deliberately
ignore Lean's &amp;quot;customer value&amp;quot; aspect, focusing on the soft skills
(respect and teamwork) and &lt;strong&gt;Kaizen&lt;/strong&gt; (continuous improvement) instead.
Finally, this post concludes with a reasonable way to probe if an
organization has adopted such a culture, no matter if you are inside or
outside that org.&lt;/p&gt;
&lt;p&gt;According to Westrum, generative cultures place their utmost value on
the flow of information. One might say they value &lt;strong&gt;transparency&lt;/strong&gt; above
all else. The &lt;a class="reference external" href="https://openstax.org/books/principles-management/pages/3-3-the-industrial-revolution"&gt;industrial
revolution&lt;/a&gt;
came around by ensuring that data flows from the edges (workers/actors)
to the centers (leaders). Even Napoleon's success as a military leader
&lt;a class="reference external" href="https://core.ac.uk/download/pdf/36698436.pdf"&gt;has been attributed&lt;/a&gt;
to Napoleon creating an efficient flow of intelligence back to central
command and control.&lt;/p&gt;
&lt;p&gt;Generative cultures ensure the &lt;em&gt;reverse&lt;/em&gt; flow of information back to the
edges, too: In a self-evident example, imagine workers having access to
real-time data about the sale of each type of product made in their
factory. That would allow those workers to make &lt;strong&gt;Agile&lt;/strong&gt; production
decisions (i.e., swiftly and independently) without including a long and
tedious communication chain through company silos and hierarchies. The
&lt;a class="reference external" href="https://hbr.org/1990/11/how-i-learned-to-let-my-workers-lead"&gt;(sausage) factory
example&lt;/a&gt;
also highlights the lean, bottom-up nature of such cultures: Namely, an
organization's ability to build a workforce that can make decisions at
the edges instead of relying on centralized decision-making structures.
That requires that critical information gets to those workers who might
be taking corrective actions based on it without impediment.&lt;/p&gt;
&lt;p&gt;Beyond transparency, to cite Westrum, generative cultures exhibit high
cooperation and teamwork with shared accountability, encourage bridging
across organizational silos, follow the practice of Kaizen (continuous
improvement), and provide fertile grounds for novel ideas to thrive. It
requires an organization with high levels of trust among its people and
leaders able to coach their teams on handling disagreements and conflict
to unlock these benefits, thereby &lt;a class="reference external" href="https://itrevolution.com/agile-conversations/"&gt;transforming the organization's
culture&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One way to describe a &lt;em&gt;generative&lt;/em&gt; culture is to compare it to its
logical opposite, an &lt;em&gt;imperative&lt;/em&gt; culture.  In an imperative
environment, &lt;cite&gt;management&lt;/cite&gt; plans solutions behind closed doors that then
get pushed down to the workers to execute.  This puts workers under
tutelage, as only solutions can be discussed (sometimes even that is
discouraged), while analyzing the needs and problems is done for them -
in fact, that is taken away from them.  In generative cultures, both
management &lt;em&gt;and&lt;/em&gt; workers can propose the needs and problems that should
be discussed, while the &lt;cite&gt;workers&lt;/cite&gt; then &lt;em&gt;generate&lt;/em&gt; the solutions.  By
engaging the workers to think through the organization's issues and
asking them to design the solutions, basic human needs such as
motivation and respect are met:  Workers are challenged to come up with
the solutions, and they can feel that management trusts them to identify
and execute those solutions.&lt;/p&gt;
&lt;p&gt;Therefore, the &amp;quot;secret sauce&amp;quot; is all about building engagement and
dependability by gradually giving the workforce more decision-making
authority. Information gets &amp;quot;stuck&amp;quot; if any manager in the company's
reporting chain first has to &amp;quot;call the shots&amp;quot;: Every time that happens,
in the best case, a delay is introduced - and in the worst case,
information is lost. Ultimately, an organization's ability to make the
most optimal decisions and therefore reach the best possible &lt;em&gt;business
outcomes&lt;/em&gt; depends on surfacing the relevant data in time at those places
where a change needs to transpire (which is, most often, at the edges).&lt;/p&gt;
&lt;p&gt;Building such an enabled workforce requires a leadership team that
trusts and respects its people.
&lt;a class="reference external" href="https://www.leanblog.org/2013/02/toyota-respect-for-people-or-humanity-and-lean/"&gt;Respect&lt;/a&gt;
refers to one of the five Lean values: Having a psychologically safe
environment where leaders coach their teams in decision-making while
providing all the necessary data. And &lt;a class="reference external" href="https://www.tablegroup.com/the-five-dysfunctions-of-a-team-trust/"&gt;(Lencioni's vulnerability-based)
trust&lt;/a&gt;,
among other things, implies that the leader is willing to rely on the
team to make the right decisions instead of the leader making those
decisions unilaterally. In other words, it requires leaders to face
their ego and overcome inner fears by &lt;a class="reference external" href="https://www.youtube.com/watch?v=uOiP4mJwqE0"&gt;conceding decision-making powers
to their reports (&amp;quot;Brave New
Work&amp;quot;)&lt;/a&gt;. Furthermore, it
requires a leadership behavior that allows curiosity and collaboration
to foster. That, in turn, needs managers with a broad range of abilities
and a coaching mindset:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The ability to create a psychologically safe learning environment and
teach &lt;a class="reference external" href="http://www-personal.umich.edu/~mrother/The_Improvement_Kata.html"&gt;continual
self-improvement&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;The ability to coach people on how to approach problems in a
data-driven way to generate optimal solutions &lt;em&gt;as a team&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;Making all relevant information available while eradicating &amp;quot;the
undiscussable&amp;quot; (Anything that a team should talk about but only
get addressed in hallways, 1on1s, or among smaller groups.)&lt;/li&gt;
&lt;li&gt;Plus all the &lt;a class="reference external" href="https://rework.withgoogle.com/guides/managers-identify-what-makes-a-great-manager/steps/learn-about-googles-manager-research/"&gt;&amp;quot;standard&amp;quot;
ingredients&lt;/a&gt;
that make up a great manager, such as providing structure &amp;amp;
clarity, accountability, meaning, and impact.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the reader might agree with all of the above, it can be a long
path for an untrained person to acquire those skills. When I first
encountered these ideas, I believed that I was practicing them already,
only to discover that I was (and in some areas very much still am)
nowhere close after a more detailed introspection of my behavior.
Anyhow, the above description and links should provide the interested
reader with more than sufficient material to take that path. Instead, in
this blog post, I would like to conclude with an exciting way to probe
if you, your team, your company, or even the company you are
interviewing with is building a lean, generative culture.&lt;/p&gt;
&lt;p&gt;The test is a short set of questions that you can ask, in any context,
as a self-test, when talking to your future manager or out of general
curiosity:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Transparency&lt;/strong&gt; and information flow: People should be willing to
disagree when they see things differently and openly share how
they come to their views. Can you give some examples of how your
team navigates disagreements and makes decisions? Can you explain
how you increase teamwork among your reports?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Curiosity&lt;/strong&gt; and continuous improvement: Teams should have a mutual
learning mindset and be eager to improve continually. Can you
provide concrete examples of how your team improves itself and
learns together? What is the most exciting thing your team worked
on or researched recently, and what did you learn?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accountability&lt;/strong&gt; and dependability: Everyone on the team should be
encouraged to take full responsibility for their words, actions,
and results. Can you give examples of how you ensure that your
people can depend on each other? How do you motivate people to
participate in meetings, take on responsibilities, and grow?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Core values&lt;/strong&gt; and more Kaizen: What do you see as the
organization's most significant strengths and weaknesses, and how
are you acting on those insights?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above questions are the &amp;quot;probes&amp;quot; I have designed to keep asking
myself and track if I am improving as a leader. While I at least believe
that the remaining core lean, generative culture values (compassion,
respect, and trust) must be present if you can answer the above
questions satisfactorily. I am sure the questions are not pixel perfect,
so I would greatly welcome all constructive criticism if you see
cleverer ways to design this test.&lt;/p&gt;
</content><category term="Miscellaneous"></category><category term="management"></category></entry><entry><title>A sober perspective on Deep Learning</title><link href="https://fnl.es/a-sober-perspective-on-deep-learning.html" rel="alternate"></link><published>2018-02-17T00:00:00+01:00</published><updated>2018-02-17T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2018-02-17:/a-sober-perspective-on-deep-learning.html</id><summary type="html">&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This is a short tutorial to compare the outcome of applying Deep Learning techniques to a text classification problem, using word embeddings and a convolutional neural network (CNN), via Keras (with Theano, for simplicity - but any Keras backend will do), GloVe embeddings, and a SciKit-Learn dataset. The original tutorial is taken from very useful &lt;a href="https://blog.keras.io/index.html"&gt;blog&lt;/a&gt;</summary><content type="html">&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell &gt; div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area &gt; div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area &gt; div.highlight &gt; pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the &lt;head&gt; if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev &lt;Maniac@SoftwareManiacs.Org&gt;
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph &gt; img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell &gt; div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
&lt;/style&gt;
&lt;style type="text/css"&gt;.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */&lt;/style&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This is a short tutorial to compare the outcome of applying Deep Learning techniques to a text classification problem, using word embeddings and a convolutional neural network (CNN), via Keras (with Theano, for simplicity - but any Keras backend will do), GloVe embeddings, and a SciKit-Learn dataset. The original tutorial is taken from very useful &lt;a href="https://blog.keras.io/index.html"&gt;blog&lt;/a&gt; about "doing" Deep Learning with Keras (in Python) and you might find different details in the &lt;a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"&gt;original tutorial post&lt;/a&gt; that I will highlight. Our goal will be to beat the state-of-the-art (SOTA) with Deep Learning (spoiler alert: we will not beat it), to better understand the limits and considerations you need to take into acoount with this machine learning technique.&lt;/p&gt;
&lt;p&gt;We will work with a CNN to try and beat &lt;a href="http://clair.si.umich.edu/%7Eradev/papers/tc.pdf"&gt;the best &lt;em&gt;published&lt;/em&gt; results&lt;/a&gt; from models &lt;em&gt;other&lt;/em&gt; than neural networks on the 20 Newsgroup dataset (and will discuss Reuters-21578 a bit, too).
If you like, particularly the 20 Newsgroups corpus is the equivalent of "MINST" for computer vision, but for text mining.
However, one caveat applies: The datasets are relatively small for what Deep Learning "needs" (and this has a huge impact on the final results), and for the Reuters dataset, most of the categories indeed are far too small to apply any serious Deep Learning techniques.
Therefore, we will only look into the 20 Newsgroups dataset here.&lt;/p&gt;
&lt;p&gt;For those two corpora, the best (non-neural) baselines achieve around 90% accuracy on the 20 Newsgroups  corpus, using the official (roughly 3:2) split. Even the &lt;a href="http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"&gt;off-the-shelf SciKit-Learn setup&lt;/a&gt; achieves around 85% accuracy on this set (using a Linear SVM).
For Reuters-21578, the state-of-the-art ("ante Deep Learning") was 94% micro-averaged $F_1$ Score using the official ModApte split, but only selecting documents among the ten most frequent categories (200 or more documents), and a micro-averaged 89% $F_1$ Score is using all 90 categories.
And, obviously, it is important that the evaluation follows a single multilabel classification setting, not 10 or 90 individual binary classification problems... Most Deep Learning literature only focuses on the simpler 10-categories subset of Reuter-21578, and/or treats it as 10 distinct binary classification problems, because otherwise there are too few examples to work with. Yet the "true" SOTA baseline you should keep in mind for that set when reading a paper oon Deep Learning using it, is the 94% micro-averaged $F_1$ score on the mulit-label task with the top 10 categories; Anything else is just "cheating yourself".&lt;/p&gt;
&lt;h2 id="Installation-and-setup"&gt;Installation and setup&lt;a class="anchor-link" href="#Installation-and-setup"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We will be using &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; "on Theano" (or CNTK or TensorFlow, as you prefer [1]) to understand how to build a simple classifier with a neural network - that short-handedly beats all results you've seen so far.&lt;/p&gt;
&lt;p&gt;[1] Theano is probably well suited for teaching/learning networks, while Microsoft's CNTK is probably best suited for langauge modeling (of those three choices!) and Google's TensorFlow is certainly the best all-rounder and probably the most popular. If you already have something else than Theano (the default, AFAIK) set up with Keras, go with that.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://keras.io/#installation"&gt;Installing Keras&lt;/a&gt; is simple:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda install keras
&lt;span class="c1"&gt;# or:&lt;/span&gt;
pip3 install keras
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Similarly, &lt;a href="http://deeplearning.net/software/theano/install.html#install"&gt;installing Theano&lt;/a&gt; is easy, too, although getting it to work with your Nvidia GPU - assuming you have one in your laptop in the first place - might be more fidegty (see Theano's instructions about &lt;code&gt;libgpuarray&lt;/code&gt; if you are not using &lt;code&gt;conda&lt;/code&gt;, which does that for you via &lt;code&gt;pygpu&lt;/code&gt;) - but still a lot simpler than with most other neural network libraries (if they are not supported by &lt;code&gt;conda&lt;/code&gt;...)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda install theano &lt;span class="c1"&gt;# no Nvidia GPU&lt;/span&gt;
conda install theano pygpu &lt;span class="c1"&gt;# with Nvidia GPU support&lt;/span&gt;
&lt;span class="c1"&gt;# or:&lt;/span&gt;
pip install Theano&lt;span class="o"&gt;[&lt;/span&gt;doc&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;keras&lt;/span&gt;
&lt;span class="c1"&gt;# only to ensure its installed:&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;theano&lt;/span&gt; &lt;span class="c1"&gt;# or your favorite choice&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;Using Theano backend.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Word-embeddings"&gt;Word embeddings&lt;a class="anchor-link" href="#Word-embeddings"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Instead of properly building your own word embeddings, we will take a "shortcut" and use a precomputed set of (single) word embeddings, &lt;a href="https://nlp.stanford.edu/projects/glove/"&gt;GloVe&lt;/a&gt;, which is &lt;a href="https://nlp.stanford.edu/data/"&gt;hosted and distributed by Stanford&lt;/a&gt;.
(Yes, so our results will be sub-SOTA-par, but this is a blog post, not an attempt to beat the SOTA in a peer reviewed journal...)
We'll "pretend" that we are just trying to get a quick prototype running to see if our idea (multi-label classification of documents with convolutional networks) works.
Once we've ensured it does, feel free to "scale" it to larger and/or more specific embeddings [1] and/or more complex networks (we'll use just a 1D conv net here).&lt;/p&gt;
&lt;p&gt;[1] Generally, if you have the time and resources to build your own embeddings, you will &lt;em&gt;always&lt;/em&gt; be better off with embeddings from the most representative documents for your target domain.
This is particularly true for capturing the right embeddings of named entities and idioms that are highly specific for your specific domain, while the semantics of collocations go well beyond that of single words.&lt;/p&gt;
&lt;p&gt;If you are more of a fan of the FastText model, language-specific 300 dimensional word embeddings for a great many languages were &lt;a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"&gt;made available by Facebook's research department&lt;/a&gt; recently.&lt;/p&gt;
&lt;p&gt;For now, we'll just stick to the smallest possible/simplest set (in the hopes that our laptops can cope with the data): &lt;code&gt;glove.6B.zip&lt;/code&gt; - word embeddings created from WikiPedia. And, we will only use the 50 dimensional embeddings (again, in the hopes that this works on your local laptop; if you have a GPU, use 100 dimensional set for a [little] performance gain - while the 300d set contributes no gains over 100d). Note the download is almost 1GB ("As homework" you can experiment with larger GloVe collections [42B, 840B] to see if that helps improve the final performance.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [2]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!wget http://nlp.stanford.edu/data/glove.6B.zip&lt;/span&gt;
&lt;span class="c1"&gt;#!unzip glove.6B.zip&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [8]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="c1"&gt;# use 100 on a GPU, or to get max. performance&lt;/span&gt;
&lt;span class="n"&gt;WORD_VECTOR_FILE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'glove.6B.&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;d.txt'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [9]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;pylab&lt;/span&gt; inline --no-import-all
&lt;span class="n"&gt;embeddings_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WORD_VECTOR_FILE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;coefs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'float32'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;embeddings_index&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;coefs&lt;/span&gt;
        
    &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Found &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt; word vectors with dim=&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;.'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embeddings_index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embeddings_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Populating the interactive namespace from numpy and matplotlib
Found 400000 word vectors with dim=100.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I.e., we now have loaded 400,000 word vector representations.&lt;/p&gt;
&lt;h2 id="Corpus-setup"&gt;Corpus setup&lt;a class="anchor-link" href="#Corpus-setup"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Here, &lt;strong&gt;we shall differ from the blog post&lt;/strong&gt; in a positive sense:
We will keep using the headers, which contain the subject, and can often give us critcal hints.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [10]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fetch_20newsgroups&lt;/span&gt;

&lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_20newsgroups&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_20newsgroups&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"------------ TRAIN ------------"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;LABEL:"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
      &lt;span class="s2"&gt;"="&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s2"&gt;------------ TEST ------------"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;LABEL:"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
      &lt;span class="s2"&gt;"="&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;------------ TRAIN ------------
From: lerxst@wam.umd.edu (where's my thing)
Subject: WHAT car is this!?
Nntp-Posting-Host: rac3.wam.umd.edu
Organization: University of Maryland, College Park
Lines: 15

 I was wondering if anyone out there could enlighten me on this car I saw
the other day. It was a 2-door sports car, looked to be from the late 60s/
early 70s. It was called a Bricklin. The doors were really small. In addition,
the front bumper was separate from the rest of the body. This is 
all I know. If anyone can tellme a model name, engine specs, years
of production, where this car is made, history, or whatever info you
have on this funky looking car, please e-mail.

Thanks,
- IL
   ---- brought to you by your neighborhood Lerxst ----

LABEL: rec.autos = 7


------------ TEST ------------
From: adamsj@gtewd.mtv.gtegsc.com
Subject: Re: Homosexuality issues in Christianity
Reply-To: adamsj@gtewd.mtv.gtegsc.com
Organization: GTE Govt. Systems, Electronics Def. Div.
Lines: 18

In article &lt;may.13.02.29.39.1993.1505@geneva.rutgers.edu&gt;, revdak@netcom.com (D. Andrew Kille) writes:
&gt; Of course the whole issue is one of discernment.  It may be that Satan
&gt; is trying to convince us that we know more than God.  Or it may be that
&gt; God is trying (as God did with Peter) to teach us something we don't
&gt; know- that "God shows no partiality, but in every nation anyone who fears
&gt; him and does what is right is acceptable to him." (Acts 10:34-35).
&gt; 
&gt; revdak@netcom.com

Fine, but one of the points of this entire discussion is that "we"
(conservative, reformed christians - this could start an argument...
But isn't this idea that homosexuality is ok fairly "new" [this
century] ? Is there any support for this being a viable viewpoint
before this century? I don't know.) don't believe that homosexuality
is "acceptable to Him". So your scripture quotation doesn't work for
"us".

-jeff adams-

LABEL: soc.religion.christian = 15
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;However, the GloVe word vectors don't come with "apostrophe forms" and instead expand those contractions to full words; Here, we will do the same (removing them, thereby once more differing from the orignal Keras blog post).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [11]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;

&lt;span class="n"&gt;lexicon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bdon't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"do not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bit's\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"it is"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bi'm\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"i am"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bi've\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"i have"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bcan't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"cannot"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bdoesn't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"does not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bthat's\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"that is"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bdidn't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"did not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bi'd\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"i would"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\byou're\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"you are"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bisn't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"is not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bi'll\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"i will"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bthere's\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"there is"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bwon't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"will not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bwoudn't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"would not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bhe's\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"he is"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bthey're\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"they are"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bwe're\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"we are"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\blet's\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"let us"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bhaven't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"have not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bwhat's\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"what is"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\baren't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"are not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bwasn't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"was not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;"\bwouldn't\b"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"would not"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fix_apostrophes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replacement&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;lexicon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;replacement&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;

&lt;span class="n"&gt;text_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fix_apostrophes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;text_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fix_apostrophes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Data-preparation"&gt;Data preparation&lt;a class="anchor-link" href="#Data-preparation"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Keras comes with its own &lt;a href="https://keras.io/preprocessing/text/"&gt;test preprocessing facilities&lt;/a&gt; (very much like &lt;a href="https://radimrehurek.com/gensim/index.html"&gt;Gensim&lt;/a&gt;'s, by the way, but not quite as powerful).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [12]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.sequence&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Limit extraction to the words found in the &lt;strong&gt;training set&lt;/strong&gt; (only [2]!), selecting the &lt;code&gt;NUM_UNIQ_WORDS&lt;/code&gt; most frequent tokens as feature &lt;em&gt;candidates&lt;/em&gt; (see padding below) only; It turns out, we can live with less than what the blog post uses and still get nearly the "same" results, and, as already descibed, we will remove the single quote apostrophe character (&lt;code&gt;'&lt;/code&gt;):&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [13]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;NUM_UNIQ_WORDS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;num_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NUM_UNIQ_WORDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# use True if you don't fix_apostrophes&lt;/span&gt;
    &lt;span class="c1"&gt;# Keras' default filters don't remove the single quote&lt;/span&gt;
    &lt;span class="c1"&gt;# apostrophe (') - filter it, as GloVe doesn't know it &lt;/span&gt;
    &lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'!"&lt;/span&gt;&lt;span class="se"&gt;\'&lt;/span&gt;&lt;span class="s1"&gt;#$%&amp;()*+,-./:;&lt;=&gt;?@[&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s1"&gt;]^_`{|}~&lt;/span&gt;&lt;span class="se"&gt;\t\n&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;[2] About that above remark regarding "only", and the next step:
The original tutorial makes a rather typical mistake - it fits the word extraction on the test data, too.
Therefore, the blog post's preprocessing takes its own test data into account.&lt;/p&gt;
&lt;p&gt;The test data is only there to &lt;em&gt;apply&lt;/em&gt; and &lt;em&gt;evaluate&lt;/em&gt; your model, not to develop it:
"In real life", you don't actually get a chance to tune your setup against the test data.
Therefore, such errors lead to overley optimistic evaluation results of classifiers and machine learning models (and possibly "irreproducible results" for your fellow researchers).
As a sad word of warning, a far too big proportion of peer-reviewed research contains such trivial, but mission-critical errors, and for "papers" on arXiv and similar sites, the only right assumption is that the evaluation results presented are probably wrong, unless you can prove (yourself) otherwise.&lt;/p&gt;
&lt;p&gt;In any case, you should &lt;strong&gt;never "fit" or "tune"&lt;/strong&gt; anything in your (preprocessing or not) pipeline &lt;strong&gt;using test data&lt;/strong&gt;, as you are guaranteed to get an overly optimistic result (that will not hold against truly "unseen" data, because you now have &lt;em&gt;overfitted&lt;/em&gt; your model).
At least if you are building a real-life, "production" classifier, this single advice will be probably the most imporant thing you need to keep in mind.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [14]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# only fit on training data!&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_on_texts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Found &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt; unique tokens.'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;word_index&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# so that texts_to_sequences will only be using&lt;/span&gt;
&lt;span class="c1"&gt;# the NUM_UNIQ_WORDS most frequent *training* words!&lt;/span&gt;

&lt;span class="c1"&gt;# generate "word index" vectors from both train and test&lt;/span&gt;
&lt;span class="c1"&gt;# (using only the NUM_UNIQ_WORDS most frequent ones)&lt;/span&gt;
&lt;span class="n"&gt;seq_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;texts_to_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;seq_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;texts_to_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Found 126595 unique tokens.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Note that our sequences are now integers, where each integer is an index (from &lt;code&gt;tokenizer.word_index&lt;/code&gt;), in the order in which the tokens appeared in the document, and only for the "selected" (&lt;code&gt;NUM_UNIQ_WORDS&lt;/code&gt;) tokens.&lt;/p&gt;
&lt;p&gt;Next, we chop our sequences to equally sized vectors of &lt;code&gt;MAX_SEQ_LEN&lt;/code&gt;, thereby generating the actual input "document vector" for our model.
Unlike the blog post, though, we will take the &lt;em&gt;first&lt;/em&gt; &lt;code&gt;MAX_SEQ_LEN&lt;/code&gt; words (by setting &lt;code&gt;truncating='post'&lt;/code&gt;), not the last.
That is, we will be using at most the first &lt;code&gt;MAX_SEQ_LEN&lt;/code&gt; words of each document, and each element in the vector will be an index for that word at the given position:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [15]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.sequence&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;to_categorical&lt;/span&gt;

&lt;span class="n"&gt;MAX_SEQ_LEN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="n"&gt;data_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seq_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQ_LEN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;truncating&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'post'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seq_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQ_LEN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;truncating&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'post'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;labels_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;to_categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;labels_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;to_categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Size of training set:'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Shape of training data tensor:'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Shape of taraining label tensor:'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;Size of test set:'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Shape of test data tensor:'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Shape of test label tensor:'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Size of training set: 11314
Shape of training data tensor: (11314, 1000)
Shape of taraining label tensor: (11314, 20)

Size of test set: 7532
Shape of test data tensor: (7532, 1000)
Shape of test label tensor: (7532, 20)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;We have 11,311 training examples/documents for training, characterized as a 1000-dimensional word &lt;em&gt;count&lt;/em&gt; vecotor, and a 20-dimensional label vector (20 Newsgroups...) for each example/document.
And we have 7532 test examples/documents for evaluating our approach.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note that here, too, we significantly deviate from the blog post.&lt;/strong&gt; Instead of using the official ~3:~2 split (for each 3 training articles, you leave aside 2 test articles, roughly) on the 20 Newsgroups corpus, the post uses an easier 4:1 random split.
Between the preprocessing issues and the non-standard split, these two issues alone explain why the blog post achieves such a surprisingly good results with such a simple architecture, with way higher performance scores than anything previously seen in the research literature.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [16]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s2"&gt;"..."&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;[0 0 0 0 0 0 0 0 0 0] ... [ 113  186  203 1438 1327    2   14   37   58 7828]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;What you see above are word indexes - and/or leading zeros, if the document didn't contain enough words.&lt;/p&gt;
&lt;p&gt;Finally, we rename the dataset to follow the same nomencalture as used in the blog post (instead of using the random 1:4 split).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [17]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_train&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels_train&lt;/span&gt;
&lt;span class="n"&gt;x_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_test&lt;/span&gt;
&lt;span class="n"&gt;y_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels_test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Building-the-model"&gt;Building the model&lt;a class="anchor-link" href="#Building-the-model"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This is probably the "technically" most interesting part - you will see just how incredibly easy it is to transform our word count vectors into proper word embedding vectors and plug that into a neural network with Keras.
This is really the part where Keras shines - unlike "low-level" APIs provided directly by the framework, Keras makes building standard (and not &lt;em&gt;so&lt;/em&gt; standard...) networks really easy.&lt;/p&gt;
&lt;p&gt;First, we generate the weight matrix for the connections between the input (the padded "document vector" sequences) and the embedding layer. Those weights therefore will be the  GloVe word embedding vectors, one for each of the &lt;code&gt;NUM_UNIQ_WORDS&lt;/code&gt; possible words we have.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [18]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;num_uniq_input_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NUM_UNIQ_WORDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;word_index&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;embedding_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_uniq_input_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;word_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;NUM_UNIQ_WORDS&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;
        
    &lt;span class="n"&gt;embedding_vector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embeddings_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;embedding_vector&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_vector&lt;/span&gt;
    &lt;span class="c1"&gt;#else:&lt;/span&gt;
    &lt;span class="c1"&gt;#    # words not found in the index use all-zero vectors&lt;/span&gt;
    &lt;span class="c1"&gt;#    print("not in index:", word)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Next, we need a bunch of "components" used to build our neural network:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [31]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Conv1D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;MaxPooling1D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The &lt;a href="https://keras.io/layers/embeddings/"&gt;embedding layer&lt;/a&gt;; Note that the tutorial set &lt;code&gt;trainable&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;, to avoid that the embeddings get change; However, there is no conceivable reason to do that, and in fact, performance suffers if held constant. So this layer will expand our one-dimensional &lt;code&gt;MAX_SEQ_LEN&lt;/code&gt; "document word index vectors" into &lt;code&gt;MAX_SEQ_LEN&lt;/code&gt; times &lt;code&gt;EMBEDDING_DIM&lt;/code&gt; matrices, replacing the index values with the appropriate GloVe word embedding vector, hence it is a a simple "vector lookup" this layer is doing.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [49]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;embedding_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;num_uniq_input_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQ_LEN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# no need to keep embeddings fixed (as in the blog post)!&lt;/span&gt;
&lt;span class="c1"&gt;# use True if you have a GPU, False if not or to get max. performance&lt;/span&gt;
&lt;span class="c1"&gt;# note that with False, your final accuracy will be 5-10% lower&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Plugging our input tensor shape ("layer") and the embeddings lookup layer together:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [41]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sequence_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQ_LEN&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int32'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;embedded_sequences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sequence_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;So that's it - at this point, you've seen how easy it is to "transform" a collection of words in a document into a semantically meaningful tensor ready for Deep Learning.&lt;/p&gt;
&lt;p&gt;Next, we shall set up a conv net with three 5 word-window-sized, 1-dimensional (documents are "linear") convolutions, each followed by a max-pooling regularization (with a factor of 2, 5, and 35 max-pooling of our words).
Why? Because you are an expert, or read tons of literature to find the "best" architecture, or (here) are simply following a blog post...
In essence, these max-pooled convolutions "compress" the &lt;code&gt;MAX_SEQ_LEN&lt;/code&gt; document vector into one one single number (times the &lt;code&gt;EMBEDDING_DIM&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The WildML blog has excellent posts explaining the &lt;a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"&gt;basics of text classiciation with conv nets&lt;/a&gt; and an example &lt;a href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"&gt;conv net for text classification&lt;/a&gt;. Note that the posts assumes you have a 2-dimenstional word-sentence document matrix as input, while we are using a 1-dimensional document vector (and hence use &lt;code&gt;Conv1D&lt;/code&gt;, not &lt;code&gt;Conv2D&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;With that in mind, we make a few minor tweaks to the blog post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of using fixed, 128 dimensional outputs, we stick with our embedding dimension.&lt;/li&gt;
&lt;li&gt;We remove the final ReLU layer and add a Dropout layer to get stronger regularization (which will allows us to keep training for more epochs, getting to a higher accuracy - but also training longer...).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And after the convolutions, we "roll out" (aka. flatten) the convolutions into a single-dimensional "document vector".&lt;/p&gt;
&lt;p&gt;(Tip: for better -but far more complex- architectures look at the Conclusions...)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [50]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedded_sequences&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;layer&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="n"&gt;Conv1D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;MaxPooling1D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;Conv1D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;MaxPooling1D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;Conv1D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# very critical to tune this hyper-parameter! (.25 - .5)&lt;/span&gt;
    &lt;span class="n"&gt;MaxPooling1D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;"input:"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s2"&gt;"- output:"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;conv1d_13 input: (None, 1000, 100) - output: (None, 996, 100)
max_pooling1d_13 input: (None, 996, 100) - output: (None, 199, 100)
conv1d_14 input: (None, 199, 100) - output: (None, 195, 100)
max_pooling1d_14 input: (None, 195, 100) - output: (None, 39, 100)
conv1d_15 input: (None, 39, 100) - output: (None, 35, 100)
dropout_5 input: (None, 35, 100) - output: (None, 35, 100)
max_pooling1d_15 input: (None, 35, 100) - output: (None, 1, 100)
flatten_5 input: (None, 1, 100) - output: (None, 100)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Note that to get to the &lt;code&gt;1 x EMBEDDING_DIM&lt;/code&gt; final size, the &lt;code&gt;MAX_SEQ_LEN&lt;/code&gt; needs to be &lt;code&gt;1000&lt;/code&gt; (so you &lt;em&gt;might&lt;/em&gt; need/want to fiddle with the parameters of the max-pooled convolutions if you change &lt;code&gt;MAX_SEQ_LEN&lt;/code&gt;). For &lt;code&gt;MAX_SEQ_LEN = 1000&lt;/code&gt;, the final output tensor from the convolusions is &lt;code&gt;1 x EMBEDDING_DIM&lt;/code&gt; due to:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [43]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# for conv layers, subtract kernel size minus one to get the output size&lt;/span&gt;
&lt;span class="c1"&gt;# for max pool layers, divide by the pool size&lt;/span&gt;
&lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;35&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[43]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;1&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;On the final output - a vector with &lt;code&gt;EMBEDDING_DIM&lt;/code&gt; numbers - we apply a softmax transformation down to the number of category lables, thereby giving us the propabilities for each category:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [44]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_cats&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_cats&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'softmax'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Finally, we place the layers into a proper &lt;a href="https://keras.io/models/model/"&gt;Keras model&lt;/a&gt;, using multi-class cross-entropy training handled by an &lt;a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop"&gt;RMS-prop gradient descent optimizer&lt;/a&gt;. Here, typically, I get asked: Why am I not using Adam? Because of Occams razor: Using Adam instead of RMS-prop makes hardly any difference on this example. And we ask Keras to report the current (training and validation) accuracy at each epoch.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [45]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sequence_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rmsprop'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'acc'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_4 (InputLayer)         (None, 1000)              0         
_________________________________________________________________
embedding_3 (Embedding)      (None, 1000, 100)         1000000   
_________________________________________________________________
conv1d_10 (Conv1D)           (None, 996, 100)          50100     
_________________________________________________________________
max_pooling1d_10 (MaxPooling (None, 199, 100)          0         
_________________________________________________________________
conv1d_11 (Conv1D)           (None, 195, 100)          50100     
_________________________________________________________________
max_pooling1d_11 (MaxPooling (None, 39, 100)           0         
_________________________________________________________________
conv1d_12 (Conv1D)           (None, 35, 100)           50100     
_________________________________________________________________
dropout_4 (Dropout)          (None, 35, 100)           0         
_________________________________________________________________
max_pooling1d_12 (MaxPooling (None, 1, 100)            0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 20)                2020      
=================================================================
Total params: 1,152,320
Trainable params: 1,152,320
Non-trainable params: 0
_________________________________________________________________
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Model-training"&gt;Model training&lt;a class="anchor-link" href="#Model-training"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Now, we train... Note that we here actually commit another crime: We evaluate the model on the test data! Instead, if done properly, you should be evaluating on a subset of the &lt;em&gt;training&lt;/em&gt; data, and only once your entire model is build, evaluate it on the official test data.
But that would leave our model with even less training data to work with...
At the end of the day, we can probably assume that no researcher gets stuff published without that "cheat" [1], we will just do the same: Evaluate training progress directly against (aka. "by overfitting the model on") the test data.&lt;/p&gt;
&lt;p&gt;[1] And that explains why community evaluations exist: To tell you the "real truth"! Because only then nobody gets access to the test data before the final evaluation. That is, community evaluations are like (IMO: far) more serious versions of the now popular Kaggle tasks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: This step can take &lt;em&gt;a &lt;strong&gt;very long&lt;/strong&gt; time&lt;/em&gt; unless you have a (or more...) GPU[s] (just see how long you wait for the next Epoch and multiply by n. epochs to estimate the overall runtime). Using 100 (or 300) dimensional embeddings and training them makes the epochs take much longer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [46]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%%time&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 11314 samples, validate on 7532 samples
Epoch 1/20
11314/11314 [==============================] - 175s - loss: 2.6601 - acc: 0.1409 - val_loss: 2.3280 - val_acc: 0.2411
Epoch 2/20
11314/11314 [==============================] - 177s - loss: 1.9916 - acc: 0.3037 - val_loss: 1.8654 - val_acc: 0.3828
Epoch 3/20
11314/11314 [==============================] - 181s - loss: 1.5137 - acc: 0.4449 - val_loss: 1.4921 - val_acc: 0.5133
Epoch 4/20
11314/11314 [==============================] - 182s - loss: 1.1167 - acc: 0.6070 - val_loss: 1.3270 - val_acc: 0.5593
Epoch 5/20
11314/11314 [==============================] - 180s - loss: 0.8647 - acc: 0.7036 - val_loss: 1.1327 - val_acc: 0.6518
Epoch 6/20
11314/11314 [==============================] - 180s - loss: 0.6720 - acc: 0.7757 - val_loss: 1.0200 - val_acc: 0.6887
Epoch 7/20
11314/11314 [==============================] - 181s - loss: 0.5374 - acc: 0.8269 - val_loss: 0.8951 - val_acc: 0.7260
Epoch 8/20
11314/11314 [==============================] - 181s - loss: 0.4317 - acc: 0.8632 - val_loss: 0.8843 - val_acc: 0.7339
Epoch 9/20
11314/11314 [==============================] - 185s - loss: 0.3440 - acc: 0.8906 - val_loss: 0.8010 - val_acc: 0.7600
Epoch 10/20
11314/11314 [==============================] - 184s - loss: 0.2688 - acc: 0.9156 - val_loss: 0.7563 - val_acc: 0.7666
Epoch 11/20
11314/11314 [==============================] - 178s - loss: 0.2184 - acc: 0.9334 - val_loss: 0.7849 - val_acc: 0.7689
Epoch 12/20
11314/11314 [==============================] - 180s - loss: 0.1740 - acc: 0.9473 - val_loss: 0.8139 - val_acc: 0.7604
Epoch 13/20
11314/11314 [==============================] - 180s - loss: 0.1367 - acc: 0.9570 - val_loss: 0.9951 - val_acc: 0.7179
Epoch 14/20
11314/11314 [==============================] - 182s - loss: 0.1044 - acc: 0.9683 - val_loss: 0.7785 - val_acc: 0.7900
Epoch 15/20
11314/11314 [==============================] - 180s - loss: 0.0877 - acc: 0.9725 - val_loss: 0.8641 - val_acc: 0.7828
Epoch 16/20
11314/11314 [==============================] - 184s - loss: 0.0769 - acc: 0.9775 - val_loss: 0.9341 - val_acc: 0.7637
Epoch 17/20
11314/11314 [==============================] - 188s - loss: 0.0551 - acc: 0.9847 - val_loss: 0.8329 - val_acc: 0.7963
Epoch 18/20
11314/11314 [==============================] - 183s - loss: 0.0521 - acc: 0.9860 - val_loss: 0.8523 - val_acc: 0.7997
Epoch 19/20
11314/11314 [==============================] - 178s - loss: 0.0590 - acc: 0.9861 - val_loss: 1.0596 - val_acc: 0.7645
Epoch 20/20
11314/11314 [==============================] - 183s - loss: 0.0479 - acc: 0.9883 - val_loss: 0.9919 - val_acc: 0.7821
CPU times: user 1h 32min 3s, sys: 6min 59s, total: 1h 39min 2s
Wall time: 1h 43s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[46]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&lt;keras.callbacks.history at 0x12f748b70&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Geat! So we've built a neural network that learns to classify documents.
Yet, as we can see, it takes &lt;em&gt;ages&lt;/em&gt; to train (and even with a GPU: a lot more time) compared to all former models. Worse, it does not achieve the accuracy of the best &lt;a href="http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"&gt;off-the shelf models from SciKit-Learn&lt;/a&gt;. The above model can be made to achieve around 80% max. accuracy if you can use 300- or 100-dimensional word-embeddings and train the model long enough (15-20 epochs). With 50d embeddings and no embedding layer training, you need to run for about 20-30 epochs to converge on around 67% accuracy. So these results are a long shot from the 90% SOTA accuracy that is possible on this dataset and even the 85% we can achieve with the very ad-hoc "blitz-classification-experiment" from SciKit-Learn's own tutorial.&lt;/p&gt;
&lt;h2 id="Conclusion"&gt;Conclusion&lt;a class="anchor-link" href="#Conclusion"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Overall, I wrote this "tutorial" mostly to demonstrate that you should probably focus on simple things first, before you dive head-first into Deep Learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn your own embeddings (ideally from a text collection matching your target domain), and make sure to add those mission-critical collocations that Mikolov points out in his famous &lt;a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases"&gt;word2vec NIPS paper&lt;/a&gt; (pro-tip: all of which is trivial when working with &lt;a href="https://radimrehurek.com/gensim/index.html"&gt;Gensim&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;It is cool that we now can replace old-school TF-IDF vectors with modern-day neural word embeddings; But do use them in an "old school" ML model first, because its simple and fast; If for nothing else than to make sure you need or even can expect more performance from a Deep Learning classifier at an &lt;em&gt;econmically viable&lt;/em&gt; expense.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And so, the challenge remains: How to actually beat the state-of-the-art in text classification with Deep Learning?
At the very least: "Its tricky"!
Very &lt;a href="https://link.springer.com/chapter/10.1007/978-3-662-44851-9_28"&gt;well-designed conv nets&lt;/a&gt;, and rather &lt;a href="https://link.springer.com/article/10.1007/s00521-016-2401-x"&gt;recent research on belief networks&lt;/a&gt; only in the most recent years managed to achieve the same ballpark results as the state-of-the-art results with "old school" ML models. But most of even the &lt;a href="https://arxiv.org/pdf/1601.02733.pdf"&gt;current Deep Learning research&lt;/a&gt; does not actually beat those "old" models on these two datasets.
(Which is not to say that stuff like VAEs are very cool though - and probalby would unfold their "full beauty" if you had a much larger dataset - see next.)
And probably by using LSTMs, GRU-RNNs, and (IMO: in particular) CharCNNs, to train sequence models might get you even beyond the state-of-the-art - if you have the resources and the data to even think of that, and an extensive amount of time to develop your specialized classifier/system.
(And the resources to run the inference on that mega-model in production, too, by the way...)&lt;/p&gt;
&lt;p&gt;That is to say, yes, Deep Learning can claim it beats standard ML methods on this task (text classification), but the effort to do so is highly disproptionate if compared to "traditional" ML methods: You need very large datasets, designing the model is incredibly complex and time consuming, and training and using the setup is several orders of magnitude more costly.&lt;/p&gt;
&lt;h2 id="Take-home-message"&gt;Take-home message&lt;a class="anchor-link" href="#Take-home-message"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In my opinion, the Deep Learning literature is &lt;em&gt;littered&lt;/em&gt; with evaluation results that claim to beat all former state-of-the-art, but indeed are quite frequently not much better (or ex-aequo, and often even worse). However, computer vision, machine translation, and dependency parsing being the now famous cases where Deep Learning indeed has "pushed the envelope" by a substantial margin &lt;em&gt;on the &lt;strong&gt;same&lt;/strong&gt;, &lt;strong&gt;public&lt;/strong&gt; (and often, small) community datasets&lt;/em&gt; for evaluating the approach and comparing it to existing methods. And nearly no paper at all discusses how much more resources go into setting up, developing, training, and using Deep Learning models as opposed to traditional Machine Learning.&lt;/p&gt;
&lt;p&gt;That being said, many other applications (apart from CV, MT, and DP) can profit from Deep Learning for the following reason:
&lt;em&gt;Iff&lt;/em&gt; you have much more training data (thousands, or even millions of examples per label), then, because Deep Learning can easily be scaled to work on such gigantic datasets, it indeed beats other methods (Support Vector Machines, Random Forrests, Nearest Neighbours, Gradient Boosting, etc.).&lt;/p&gt;
&lt;p&gt;At the end of the day, I think Deep Learning is a &lt;em&gt;very exciting&lt;/em&gt; technology you should learn to master, but you should take much of it with a &lt;em&gt;very large&lt;/em&gt; grain of salt due to how much time and money you will need to invest.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;
</content><category term="Machine Learning"></category><category term="keras"></category><category term="scikit-learn"></category><category term="deep learning"></category></entry><entry><title>A quick reference for working with TensorFlow</title><link href="https://fnl.es/a-quick-reference-for-working-with-tensorflow.html" rel="alternate"></link><published>2018-02-15T00:00:00+01:00</published><updated>2018-02-15T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2018-02-15:/a-quick-reference-for-working-with-tensorflow.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Given the last few years of hype around Deep Learning, knowing one of those frameworks is probably no longer an option, at least if you are a professional Machine Learning engineer. Personally, I always favor free, open source solutions, so Apache MXNet would be the natural fit. However, I …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Given the last few years of hype around Deep Learning, knowing one of those frameworks is probably no longer an option, at least if you are a professional Machine Learning engineer. Personally, I always favor free, open source solutions, so Apache MXNet would be the natural fit. However, I must admit that for research &amp;amp; development, Facebook's PyTorch is probably the nicer API, and for operations &amp;amp; production, Google's TensorFlow is beyond doubt the best fit right now. And, as I've mostly moved out of academia/research lately, and am pretty dedicated to consulting by now (and loving it!), I am mostly interested in a tooling that I can put to good use in my day-to-day work. Hence, I have been - not without remorse - focusing on TensorFlow and want to share my personal notes with you, to use as a quick reference when setting up a new tensor graph.&lt;/p&gt;
&lt;p&gt;I mostly learned how to use TensorFlow from Aurélien Géron's book &amp;quot;&lt;a class="reference external" href="http://shop.oreilly.com/product/0636920052289.do"&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow&lt;/a&gt;&amp;quot;; So if you are familiar with it and my notes below remind you of it, that is not by accident: These are my modified excerpts, all taken from that book. In fact, if you really are interested in using TensorFlow, I cannot emphasize enough how much I recommend reading this book. And, if you are not familiar with the SciKit-Learn world, or simply want to learn how you can combine these two essential Machine Learning frameworks into &amp;quot;one ring to rule them all&amp;quot;, you need to read this book. Full-stop. In fact, if you are a serious Machine Learning practitioner, you either should have read it, or at least make sure you know the techniques presented in it already. Beyond just giving you tips, the book is chock-a-block full of up-to-date examples and highly relevant exercises. Aurélien even recently &lt;a class="reference external" href="https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb"&gt;released more example code&lt;/a&gt;, to produce G. E. Hinton's Capsule Network architecture (using dynamic routing) with TensorFlow, and generally ensures the practical material is very much up to date with the latest TensorFlow releases. (Disclaimer: I am in no way affiliated with Aurélien, O'Reilly, or would otherwise benefit from sales of this book!)&lt;/p&gt;
&lt;p&gt;That being said, let's dive right in!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="setup"&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;div class="section" id="miniconda"&gt;
&lt;h3&gt;Miniconda&lt;/h3&gt;
&lt;p&gt;I strongly recommend using the &lt;a class="reference external" href="https://www.anaconda.com/download/"&gt;Anaconda&lt;/a&gt; distribution to set up TensorFlow, and Python in general. And, for the more expert users, do go directly to &lt;a class="reference external" href="https://conda.io/miniconda.html"&gt;miniconda&lt;/a&gt; (&amp;quot;don't go over Anaconda, and don't collect 200MB of (irrelevant) bytes&amp;quot;). The packages you want to install (and that will also fetch all other, relevant dependencies, like Jupyter, SciPy, or NumPy) are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda install -c conda-forge tensorflow
conda install -c conda-forge scikit-learn
conda install -c conda-forge matplotlib
conda install -c conda-forge jupyter_contrib_nbextensions
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="graph-design"&gt;
&lt;h2&gt;Graph Design&lt;/h2&gt;
&lt;div class="section" id="back-propagation-basics"&gt;
&lt;h3&gt;Back-propagation Basics&lt;/h3&gt;
&lt;p&gt;Input is feed into TensorFlow's (static) computational graphs via &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/placeholder"&gt;tf.placeholder&lt;/a&gt; (&lt;strong&gt;placeholder&lt;/strong&gt;) nodes. Typically, those placeholders will be accepting a tensor &lt;tt class="docutils literal"&gt;X&lt;/tt&gt; and some array or matrix of labels &lt;tt class="docutils literal"&gt;y&lt;/tt&gt;. Input is part of the &lt;a class="reference external" href="https://www.tensorflow.org/api_guides/python/io_ops"&gt;io_ops&lt;/a&gt; package; Input values for placeholders must be provided (see &amp;quot;Feeding the Graph&amp;quot;) during graph evaluation, or cause exceptions if left unset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weights&lt;/strong&gt; &lt;tt class="docutils literal"&gt;W&lt;/tt&gt; and &lt;strong&gt;bias&lt;/strong&gt; &lt;tt class="docutils literal"&gt;B&lt;/tt&gt; tensors are represented by &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Variable"&gt;tf.Variable&lt;/a&gt; nodes, via the &lt;a class="reference external" href="https://www.tensorflow.org/api_guides/python/state_ops"&gt;state_ops&lt;/a&gt; package; &lt;strong&gt;Variables&lt;/strong&gt; are stateful, that is, they maintain their values across session runs.
&lt;strong&gt;Operations&lt;/strong&gt; are defined by either applying standard Python operators (+, -, /, *) to nodes, or by using the &lt;tt class="docutils literal"&gt;tf.add&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;tf.matmul&lt;/tt&gt;, etc. functions from the TF &lt;a class="reference external" href="https://www.tensorflow.org/api_guides/python/math_ops"&gt;math_ops&lt;/a&gt; packages.&lt;/p&gt;
&lt;p&gt;The final operation typically evaluates the error or cost between the predicted &lt;tt class="docutils literal"&gt;y_hat&lt;/tt&gt;, modeled as a state node, and [minus] the true &lt;tt class="docutils literal"&gt;y&lt;/tt&gt;, modeled as an io node, as shown above.
Common loss functions for this task are found in TensorFlow's &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/losses"&gt;losses&lt;/a&gt; package.
The outcome of this operation (by following Aurélien nomenclature, the &lt;tt class="docutils literal"&gt;cost_op&lt;/tt&gt; node) is the one you typically want to plot on your TensorBoard (more on that at the end of this post).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# n variables + 1 constant bias input&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# ... graph setup with tf.Variables ...&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="c1"&gt;# some last step...&lt;/span&gt;
&lt;span class="n"&gt;cost_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The final touch is to create and append the optimizer (e.g, &lt;tt class="docutils literal"&gt;tf.train.GradientDescentOptimizer&lt;/tt&gt;):
That creates the &lt;tt class="docutils literal"&gt;training_op&lt;/tt&gt; (for minimizing the cost/error), which typically will be the node that gets sent to evaluations of a TensorFlow graph via a &lt;strong&gt;Session&lt;/strong&gt; (&lt;tt class="docutils literal"&gt;sess.run&lt;/tt&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;training_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="placeholder-nodes"&gt;
&lt;h3&gt;Placeholder Nodes&lt;/h3&gt;
&lt;p&gt;As discussed already, to supply data to your TF graph you designed, you insert &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/placeholder"&gt;tf.placeholder&lt;/a&gt; nodes in the graph (e.g., in the bottom-most layer of your net). Placeholders don’t perform any computation, they just output any data you tell them to, during the graph evaluation (&amp;quot;execution&amp;quot;) phase. Optionally, you can also specify the node's &lt;tt class="docutils literal"&gt;shape&lt;/tt&gt; , if you want to enforce it: And, if you furthermore specify &lt;tt class="docutils literal"&gt;None&lt;/tt&gt; for any tensor dimension, that dimension will adapt to any size (according to the next node's input).&lt;/p&gt;
&lt;p&gt;In the example shown below, the placeholder &lt;tt class="docutils literal"&gt;A&lt;/tt&gt; must be of rank 2 (i.e., two-dimensional), and the tensor must have three columns, but it can have any number of rows (which typically will be the current batch' examples).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="reassignable-variables"&gt;
&lt;h3&gt;Reassignable Variables&lt;/h3&gt;
&lt;p&gt;Note that it is possible to feed values into variables, too, not just to placeholders, even if it is a tad unusual.
To set a variable to some value during graph evaluation, use the &lt;a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/assign"&gt;tf.assign&lt;/a&gt; state operator:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# sample a random variable:&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;minval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# or feed a variable:&lt;/span&gt;
&lt;span class="n"&gt;x_new_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x_assign&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_new_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# now, you can feed new values into X:&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;x_assign&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x_new_val&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="c1"&gt;# always 0.5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizing-the-graph"&gt;
&lt;h3&gt;Optimizing the Graph&lt;/h3&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/gradients"&gt;tf.gradients&lt;/a&gt; function takes a cost operator (e.g., to calculate the MSE) and a list of variables to optimize, and creates a list of ops (one per variable) to compute the gradients of each op with regard to each variable, returning the desired list of gradients (aka. performa &lt;em&gt;reverse-mode autodiff&lt;/em&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var1grad&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var2grad&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, TensorFlow provides a good number of &lt;a class="reference external" href="https://www.tensorflow.org/api_guides/python/train#Optimizers"&gt;optimizers&lt;/a&gt; right out of the box, for example, the Adam optimizer, saving you the need to explicitly calculate gradients or update variables yourself:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;training_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="adding-regularization"&gt;
&lt;h3&gt;Adding Regularization&lt;/h3&gt;
&lt;p&gt;Regularization (typically, L1 or L2) prevents overfitting and therefore allows you to train your model for more epochs. Simply add the appropriate operations to your graph, to get to a regularized cost (or &lt;tt class="docutils literal"&gt;loss&lt;/tt&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# construct the neural network&lt;/span&gt;
&lt;span class="n"&gt;base_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xentropy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;avg_xentropy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;reg_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# L1&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reg_losses&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, if you have many layers, this approach quickly becomes inconvenient. Instead,
most TensorFlow functions in the &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers"&gt;tf.contrib.layers&lt;/a&gt; package that create variables accept a &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;..._regularizer&lt;/span&gt;&lt;/tt&gt; argument for their weights and biases.
Those arguments need to be functions that takes the weights as their argument, and return the regularization losses of that layer.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;weights_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l1_regularizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;hidden1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hidden1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;hidden2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hidden2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;out&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;TensorFlow automatically adds these nodes to a special collection containing all the regularization losses.
Then, when calculating the final loss, you add them up to find your overall, final loss:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;reg_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_collection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GraphKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;REGULARIZATION_LOSSES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_n&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;base_loss&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;reg_losses&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Max-norm regularization&lt;/strong&gt; (clipping the L2-norm at some threshold) has become quite popular, yet TensorFlow does not provide an off-the-shelf max-norm regularizer. The following code creates a node &lt;tt class="docutils literal"&gt;clip_weights&lt;/tt&gt; that will clip your &lt;tt class="docutils literal"&gt;weights&lt;/tt&gt; along their second axis ( &lt;tt class="docutils literal"&gt;axes=1&lt;/tt&gt; ), so that every resulting row vector will have a max-norm of 1.0:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="n"&gt;clipped_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip_by_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clip_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clip_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clipped_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The issue then becomes accessing the weights of a &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers"&gt;tf.contrib.layers&lt;/a&gt; module; A better solution therefore is to create a function equivalent to the &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/l1_regularizer"&gt;l1_regularizer&lt;/a&gt; found in the layers module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;max_norm_regularizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;max_norm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;max_norm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;max_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;clipped&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip_by_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clip_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;clip_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clipped&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_to_collection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clip_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;  &lt;span class="c1"&gt;# there is no regularization loss term&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;max_norm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, this regularization function can be used as an argument like any other regularizer would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hidden1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hidden1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                          &lt;span class="n"&gt;weights_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;max_norm_regularizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And to actually clip weights using max-norm during session evaluation, finally, add this to your execution phase:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;clip_all_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_collection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;max_norm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# note we used &amp;quot;max_norm&amp;quot; above&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batches&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clip_all_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="scopes-modules-and-shared-variables"&gt;
&lt;h3&gt;Scopes, Modules, and Shared Variables&lt;/h3&gt;
&lt;p&gt;Variables and nodes created within a named scope are prefixed with the scopes' name, and such scopes are collapsed into single nodes on the TensorBoard:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;mse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mse&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;E.g., to define a ReLU within a named scope (just in case: normally, you would use &lt;tt class="docutils literal"&gt;tf.nn.relu&lt;/tt&gt; ):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;w_shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_shape&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w_shape&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;weights&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bias&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;z&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you want to share a variable between various components of your graph, (e.g., thresholds, biases, etc.), TensorFlow provides the &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_variable"&gt;tf.get_variable&lt;/a&gt; function, that creates a shared variable if it does not exist, or reuses it, if it does. The desired behavior (creating or reusing) is controlled by an attribute of the current &lt;a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/variable_scope"&gt;tf.variable_scope&lt;/a&gt;, &lt;tt class="docutils literal"&gt;reuse&lt;/tt&gt; . Note that &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_variable"&gt;tf.get_variable&lt;/a&gt; raises an exception if &lt;tt class="docutils literal"&gt;reuse&lt;/tt&gt; is &lt;tt class="docutils literal"&gt;False&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;scope.reuse_variables()&lt;/tt&gt; has not been set.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# as attribute:&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reuse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;threshold&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# as function:&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reuse_variables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;threshold&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="xavier-and-he-initialization-of-model-weights"&gt;
&lt;h3&gt;Xavier and He Initialization of Model Weights&lt;/h3&gt;
&lt;p&gt;You need to use an initializer to avoid the uniform initialization of weights to all the same values.&lt;/p&gt;
&lt;p&gt;By default, TensorFlow's layers are initialized using Xavier initialization: The fully connected layers in &lt;tt class="docutils literal"&gt;tf.contrib.layers&lt;/tt&gt; use Xavier initialization, with a normal dist. using µ = &lt;tt class="docutils literal"&gt;0&lt;/tt&gt;, sigma = &lt;tt class="docutils literal"&gt;sqrt[ 2 / (n_in + n_out) ]&lt;/tt&gt; or a uniform dist. using &lt;tt class="docutils literal"&gt;+/- sqrt[ 6 / (n_in + n_out) ]&lt;/tt&gt; , where the &lt;tt class="docutils literal"&gt;n&lt;/tt&gt;'s are the sizes of the input/output connections.&lt;/p&gt;
&lt;p&gt;To use He initialization instead (which is mostly a matter of preference, but has been made popular with ResNet), you can use variance scaling initialization:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tf.contrib.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;variance_scaling_initializer&lt;/span&gt;
&lt;span class="n"&gt;initializer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variance_scaling_initializer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;FAN_AVG&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hidden1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights_initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;h1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="implementing-a-learning-rate-scheduler"&gt;
&lt;h3&gt;Implementing a Learning Rate Scheduler&lt;/h3&gt;
&lt;p&gt;Normally, it is not necessary to add a learning rate scheduler, because the AdaGrad, RMSProp, and Adam optimizers automatically reduce the learning rate for you during training. Yet, implementing a learning rate scheduler is fairly straightforward with TensorFlow; Typically, exponential decay is recommended, because it is easy to tune and will converge (slightly) faster than the optimal solution. Here, we adapt Momentum to use a dynamic learning rate: Note how the decay depends on the current global step that is set by the optimizer's minimization function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;initial_learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;span class="n"&gt;decay_steps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="n"&gt;decay_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;global_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exponential_decay&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial_learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                           &lt;span class="n"&gt;decay_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;decay_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MomentumOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;training_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="interactive-sessions"&gt;
&lt;h2&gt;Interactive Sessions&lt;/h2&gt;
&lt;p&gt;Before we get to the actual evaluation of TF graphs with sessions, let me add in a few tips that come in handy when working in interactive Python sessions.&lt;/p&gt;
&lt;div class="section" id="resetting-the-default-graph"&gt;
&lt;h3&gt;Resetting the Default Graph&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_default_graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In Jupyter (or if using TF in a Python shell), it is common to run the same commands more than once while you are experimenting. As a result, you may end up with a &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_default_graph"&gt;default graph&lt;/a&gt; containing many duplicate nodes. One solution is to restart the Jupyter kernel (or the Python shell), but a more convenient solution is to just reset the default graph by running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_default_graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In single-process TensorFlow, multiple sessions do not share any state, even if they reuse the same graph (and each session gets its own copy of every variable). Beware though, that in distributed TensorFlow, variable state is stored on the servers, not in the sessions, so multiple sessions can &lt;em&gt;share&lt;/em&gt; the same &lt;em&gt;variables&lt;/em&gt; (actually, that is a good, desired thing, obviously).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-tensorflow-s-interactive-sessions"&gt;
&lt;h3&gt;Using TensorFlow's [Interactive] Sessions&lt;/h3&gt;
&lt;p&gt;Use &lt;tt class="docutils literal"&gt;InteractiveSession&lt;/tt&gt; in notebooks to automatically set a default session, relieving you from the need of a &lt;tt class="docutils literal"&gt;with&lt;/tt&gt; block for the evaluation/execution phase. But do remember to close the session manually when you are done with it!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InteractiveSession&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# do graph setup&lt;/span&gt;
&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# do evaluation&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When running TensorFlow &lt;strong&gt;locally&lt;/strong&gt;, the sessions manage your variable values. So if you create a graph, then start two threads, and open a local session in either thread, both will use the same graph, yet each session will have its &lt;em&gt;own&lt;/em&gt; copy of the variables.&lt;/p&gt;
&lt;p&gt;However, in &lt;strong&gt;distributed&lt;/strong&gt; TensorFlow sessions, variable values are stored in containers managed by the TF cluster (see &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_variable"&gt;tf.get_variable&lt;/a&gt;). So if both sessions connect to the same cluster and use the same container, then they will share the same variable value for w.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="model-evaluation"&gt;
&lt;h2&gt;Model Evaluation&lt;/h2&gt;
&lt;div class="section" id="scaling-variables"&gt;
&lt;h3&gt;Scaling Variables&lt;/h3&gt;
&lt;p&gt;When using a Gradient Descent method, remember that it is important to &lt;strong&gt;normalize&lt;/strong&gt; the input feature vectors, or else training may progress much slower.
You can do this using TensorFlow, NumPy, Scikit-Learn’s StandardScaler, or any other solution you prefer. In fact, with NumPy arrays, this is pretty straightforward:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;scaled&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="running-the-graph"&gt;
&lt;h3&gt;Running the Graph&lt;/h3&gt;
&lt;p&gt;Once the graph has been designed (incl. a &lt;tt class="docutils literal"&gt;training_op&lt;/tt&gt; node) and the initializer (an &lt;tt class="docutils literal"&gt;init&lt;/tt&gt; node) has been set up (see Graph Design), a typical snippet for the (batched) executing of the training phase is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;iteration&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_examples&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="feeding-the-graph-with-data"&gt;
&lt;h3&gt;Feeding the Graph with Data&lt;/h3&gt;
&lt;p&gt;When you evaluate the graph, you pass a feed dictionary (&lt;tt class="docutils literal"&gt;feed_dict&lt;/tt&gt;) to the target (&amp;quot;output&amp;quot;) node's &lt;tt class="docutils literal"&gt;eval()&lt;/tt&gt; method. And you specify the value of the placeholder (input) node by using the node itself as the key of the feed dictionary.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# more graph setup, down to the training_op&lt;/span&gt;
&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;  &lt;span class="c1"&gt;# &amp;lt;- feeding&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that you can feed data into &lt;em&gt;any&lt;/em&gt; kind of node, not just placeholders. Note that when using other nodes, TensorFlow will not evaluate their operations; If fed to, TF uses the values you feed to that node, only (see Reassignable Variables in Graph Design for more info).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="mini-batching-with-tensorflow"&gt;
&lt;h3&gt;Mini-batching with TensorFlow&lt;/h3&gt;
&lt;p&gt;Instead of feeding all data at once, you typically will mini-batch your data as follows:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Create a session (&lt;tt class="docutils literal"&gt;with tf.Session() as sess&lt;/tt&gt;)&lt;/li&gt;
&lt;li&gt;Run the variable initializer (&lt;tt class="docutils literal"&gt;sess.run(init)&lt;/tt&gt;)&lt;/li&gt;
&lt;li&gt;Loop over the epochs and batches, feeding each mini-batch to the session (&lt;tt class="docutils literal"&gt;sess.run(training_op, &lt;span class="pre"&gt;feed_dict={X:&lt;/span&gt; X_batch, y: y_batch})&lt;/tt&gt;)&lt;/li&gt;
&lt;li&gt;Optionally: Write a summary every n mini-batches, to visualize the progress on your TensorBoard.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# somehow fetch data and labels (numpy arrays) to feed...&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="saving-and-restoring-models"&gt;
&lt;h3&gt;Saving and Restoring Models&lt;/h3&gt;
&lt;p&gt;Create a Saver node at the end of the construction phase (after all variable nodes are created); Then, during the execution phase, call the node's &lt;tt class="docutils literal"&gt;save()&lt;/tt&gt; method whenever you want to save the model, passing it the session and path of the &lt;strong&gt;checkpoint&lt;/strong&gt; file to create:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;saver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Saver&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;checkpoint_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/tmp/my_classifier.tfckpt&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;checkpoint_epoch_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;.tfepoch&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;final_model_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;./my_classifier.tfmodel&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;best_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;infty&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isfile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoint_epoch_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# if the checkpoint file exists, restore the model and load the epoch number&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoint_epoch_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;start_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Training was interrupted. Continuing at epoch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start_epoch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;start_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start_epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# checkpoint every 100 epochs&lt;/span&gt;
            &lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoint_epoch_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;wb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="n"&gt;loss_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;loss_val&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;best_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;final_model_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;best_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss_val&lt;/span&gt;
            &lt;span class="c1"&gt;# best_parameters = parameters.eval()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To use a trained model &lt;em&gt;in production&lt;/em&gt;, restoring a model is just as easy: You create a Saver node at the end of the graph, just like before, but then, when beginning the execution phase, instead of initializing the variables using the typical &lt;tt class="docutils literal"&gt;init&lt;/tt&gt; node, you call the &lt;tt class="docutils literal"&gt;restore()&lt;/tt&gt; method of the Saver object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;./my_model_final.ckpt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;X_unseen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# some unseen (scaled) data&lt;/span&gt;
    &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_unseen&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="monitoring-with-tensorboard"&gt;
&lt;h2&gt;Monitoring with TensorBoard&lt;/h2&gt;
&lt;p&gt;One of the biggest advantages of TensorFlow over many other frameworks is the TensorBoard. It allows you to visualize the progression of any variable in your graph.&lt;/p&gt;
&lt;div class="section" id="writing-session-summaries"&gt;
&lt;h3&gt;Writing Session Summaries&lt;/h3&gt;
&lt;p&gt;To provide TensorBoard with data, you need to write TF's graph definition and some training stats (like the cost/loss) to a log directory that TensorBoard reads from. You need to use a different log directory on every run, to avoid that TensorBoard will merge the output of different runs. The solution to this here will be to include a timestamp in the log directory name.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="n"&gt;root_logdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tf_logs&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utcnow&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;%Y%m&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;%H%M%S&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;logdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;/run-&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;/&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root_logdir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, add a &lt;strong&gt;summary node&lt;/strong&gt; and attach a &lt;strong&gt;file writer&lt;/strong&gt; to the node you wish to visualize on your TensorBoard; The &lt;tt class="docutils literal"&gt;FileWriter&lt;/tt&gt; shown below will create any missing directories for you:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;mse_summary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MSE&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;file_writer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FileWriter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logdir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_default_graph&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The first line creates a node in the graph that will evaluate the MSE value and write it to a TensorBoard-compatible binary log string called a &lt;strong&gt;summary&lt;/strong&gt;. The second line creates a &lt;tt class="docutils literal"&gt;FileWriter&lt;/tt&gt; that you will use to write summaries into the log directory. The second (optional) parameter is the graph you want to visualize. Upon creation, the FileWriter creates the directory path if it does not exist, and writes the graph definition in a binary log file called an &lt;strong&gt;events&lt;/strong&gt; file.&lt;/p&gt;
&lt;p&gt;Next, you need to update the execution phase, to evaluate the summary node regularly during training, and you should not forget to close the writer after training:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;update_summary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;update_summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;summary_str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mse_summary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt;
            &lt;span class="n"&gt;file_writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;summary_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;file_writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, you now can visualize the stats you are recording by starting the TensorBoard server and pointing it at the log directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tensorboard --logdir tf_logs/
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="epilogue"&gt;
&lt;h2&gt;Epilogue&lt;/h2&gt;
&lt;p&gt;As I already advised in the beginning, if you want to learn more, I can warmly recommend you get Aurélien Géron's fantastic book &amp;quot;&lt;a class="reference external" href="http://shop.oreilly.com/product/0636920052289.do"&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow&lt;/a&gt;&amp;quot;; The more advanced topics covered (and that would explode this blog post...) are transfer learning, distributed training, designing Recurrent Networks and Auto-encoders, and even a &amp;quot;beginner's guide&amp;quot; to Deep Reinforcement Learning. Yet, I hope, this tiny taste of the book's contents, spiced up with a bit of my own &amp;quot;opinionated&amp;quot; modifications, will provide you with  a handy quick-reference when building and using TensorFlow graphs. (And, that I don't get sued by him or O'Reilly for plagiarism! :-) Please just contact me if this post is an issue -- I have no problem taking the post down again, if it is problematic.)&lt;/p&gt;
&lt;/div&gt;
</content><category term="Machine Learning"></category><category term="tensorflow"></category><category term="deep learning"></category></entry><entry><title>What are the best books [for programmers] to get into Data Science?</title><link href="https://fnl.es/what-are-the-best-books-for-programmers-to-get-into-data-science.html" rel="alternate"></link><published>2015-08-07T00:00:00+02:00</published><updated>2015-08-07T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2015-08-07:/what-are-the-best-books-for-programmers-to-get-into-data-science.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is a question I get on a frequent basis by colleagues that are serious programmers or software developers and are planning to pick up data analytics. There are three fundamental topics in mathematics that you need to cover, assuming that you are an expert in software development and …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is a question I get on a frequent basis by colleagues that are serious programmers or software developers and are planning to pick up data analytics. There are three fundamental topics in mathematics that you need to cover, assuming that you are an expert in software development and informatics already: statistics, linear algebra, and machine learning. Therefore, I will recommend one book associated to each of these topics. Even I continue to consult those books as reference material.&lt;/p&gt;
&lt;p&gt;There might be a fourth foundation worth mentioning: calculus. I don't really have a recommendation for that, as it never has been essential to my work. As long as you can remember how to do differentiation and integration and solve an ODE, you are pretty much saddled. In other words, fresh high-school knowledge of calculus has always been good enough for me. And there is no need to make any proofs, unless you want to develop your own machine learning methods…&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-r-and-python-environments"&gt;
&lt;h2&gt;The R and Python Environments&lt;/h2&gt;
&lt;p&gt;Before presenting the books, let me recommend one tool that you should pick up when doing the books' practical exercises: &lt;a class="reference external" href="https://www.r-project.org/"&gt;R&lt;/a&gt;. R is the de facto tool for statistics and data analytics today and, as I would judge, the &lt;a class="reference external" href="https://www.rstudio.com/"&gt;R Studio environment&lt;/a&gt; has put S, SPSS, SAS, and a number of other commercial tools into their digital grave. Even big pharma, healthcare, media, banks and insurances are beginning to embrace R, &lt;a class="reference external" href="https://data-flair.training/blogs/r-applications/"&gt;apparently&lt;/a&gt;. R provides a huge ecosystem of packages that lets you quickly explore and test models, inspect your data, develop theories, and visualize results. Any of these points can be extremely tedious with other tools, but a few free and commercial statistics, math, and/or machine learning environments (e.g., MatLab or Octave) might be viable alternatives if you already know them. And &lt;em&gt;Python&lt;/em&gt; is extremely well suited (maybe even better than R) if you are interested in language processing, computer vision, neural networks, or generally strive for a strong machine learning focus. Though, I consider R the undoubtedly better choice for an advanced statistics &amp;amp; data science. In any case, I recommend working through the exercises provided by the books with your preferred interactive environment, be it R or Python - or both!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="statistics"&gt;
&lt;h2&gt;Statistics&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.statisticalsleuth.com/"&gt;The Statistical Sleuth&lt;/a&gt; by Fred Ramsey and Daniel W. Schafer.
While there might be “simpler” books around, this one has the most compact and best overview of applied statistics that I have read to date (t and F distributions, ANOVA and multivariate &amp;amp; logistic regression, etc.). It contains exercises that are very good at teaching you how to apply and think with statistics. If you can only read one book or only need to work the statistics angle, then this is “the” book. Most of the machine learning stuff can be understood easily after digesting this book, and I particularly like the very applied approach that Ramsey &amp;amp; Schafer take. Be warned, though, that if you have absolutely zero idea what statistics is about, e.g., if you cannot describe what variance is, this book will probably be a bit too intense and you might want to read something more elementary first.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="linear-algebra"&gt;
&lt;h2&gt;Linear Algebra&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://math.mit.edu/~gs/linearalgebra/"&gt;Introduction to Linear Algebra&lt;/a&gt; by Gibert Strang.
Contrary to my statistics recommendation, on this topic there certainly are more in-depth introductory books around, but this one covers all I ever needed. It takes you from the dot and outer product over eigenspaces to Singular Value Decomposition. And, the author is exceptionally gifted at teaching algebraic thinking (The book can be &amp;quot;watched&amp;quot; for free in form of &lt;a class="reference external" href="http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/"&gt;videos&lt;/a&gt; of his classes.) As a bonus, the final chapters explain how this material ties in with advanced data science techniques and machine learning models.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="machine-learning"&gt;
&lt;h2&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;An Introduction to Statistical Learning&lt;/a&gt; by James, Witten, Hastie, and Tibshirani.
While quite a bit slower moving and less detailed than the first Hastie &amp;amp; Tibshirani book (The Elements of Statistical Learning), I think it is the better start, and it has a ton of exercises using R. Especially the first few chapters contain discussions about basic aspects that are critical to this line of work, like the bias-variance tradeoff issue to keep in mind when developing your machine learning models. This is an excellent read if the Sleuth above is no longer a problem, as it does assume a solid background on basic statistics, distributions &amp;amp; regression techniques.&lt;/p&gt;
&lt;p&gt;UPDATE 2021:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/ageron/handson-ml2"&gt;Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition&lt;/a&gt;, by Aurelien Gerion.
This book has, for me, taken the crown of the best ML-focused introduction to Data Science. First, because the practical examples are in Python and use SciKit-Learn and TensorFlow, arguably the most important language, library and framework for ML today, respectively. Second, the extremely well thought out exercises and practical examples are an excellent source of &amp;quot;learning the hard way&amp;quot;. I have never seen a better practical course than these examples. Just like the ISL book (above), it has an excellent introduction on how to actually &amp;quot;do&amp;quot; machine learning. The biggest difference is a much stronger focus on deep learning than the ISL book. That said, if you want to learn more than just the basics and a bit of deep learning, you probably anyway will have to stick your nose into &lt;a class="reference external" href="https://probml.github.io/pml-book/book0.html"&gt;Machine Learning - A Probabilistic Perspective&lt;/a&gt;, by Kevin P. Murphy, or the upcoming &lt;a class="reference external" href="https://probml.github.io/pml-book/book1.html"&gt;Probabilistic Machine Learning: An Introduction&lt;/a&gt; and the &lt;a class="reference external" href="https://probml.github.io/pml-book/book2.html"&gt;Advanced Topics&lt;/a&gt; by the same author.&lt;/p&gt;
&lt;p&gt;END UPDATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;After studying these three books, you will be ready to extend your knowledge with domain-specific techniques. That should be geared at what kind of data you want to process: Text mining, time series, bioinformatics, signal processing, etc. and what specific models you want to apply: Neural networks, graphical models, kernel machines, structured equations, etc. In other words, the above three books will not convert you into a data science expert, but they will provide you with an extremely solid foundation so that you can effortlessly study any advanced topics and techniques that will.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Machine Learning"></category><category term="data science"></category><category term="review"></category></entry><entry><title>An efficient online sequence tagger resource for GATE</title><link href="https://fnl.es/an-efficient-online-sequence-tagger-resource-for-gate.html" rel="alternate"></link><published>2015-04-06T00:00:00+02:00</published><updated>2015-04-06T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2015-04-06:/an-efficient-online-sequence-tagger-resource-for-gate.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; for a stressed out generation:
&lt;a class="reference external" href="https://gate.ac.uk"&gt;GATE&lt;/a&gt;'s &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch23.html#sec:parsers:taggerframework"&gt;Generic Tagger&lt;/a&gt; framework is a CREOLE plug-in that allows you to wrap any existing &lt;a class="reference external" href="https://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;sequence tagger&lt;/a&gt; and use it to create annotations in your pipeline, but it is a bit slow.
Therefore, I have created the &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework"&gt;Online Tagger&lt;/a&gt; GATE plug-in that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; for a stressed out generation:
&lt;a class="reference external" href="https://gate.ac.uk"&gt;GATE&lt;/a&gt;'s &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch23.html#sec:parsers:taggerframework"&gt;Generic Tagger&lt;/a&gt; framework is a CREOLE plug-in that allows you to wrap any existing &lt;a class="reference external" href="https://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;sequence tagger&lt;/a&gt; and use it to create annotations in your pipeline, but it is a bit slow.
Therefore, I have created the &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework"&gt;Online Tagger&lt;/a&gt; GATE plug-in that works similarly to the Generic Tagger framework, but does not do any disk I/O for inter-process communication or launch more than one &amp;quot;singleton&amp;quot; sub-process per Processing Resource instance.
This version can in some cases be several &lt;em&gt;orders&lt;/em&gt; of magnitude faster than the built-in framework.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://gate.ac.uk"&gt;GATE&lt;/a&gt; (General Architecture for Text Engineering) has crystallized itself as my preferred tool for teaching text mining and information extraction.
While anybody might argue that there are leaner and faster frameworks around, it has one pretty outstanding, unique quality: it is (mostly) GUI-based.&lt;/p&gt;
&lt;p&gt;During any text mining course I teach, the most frequent question I get is what text mining software is around that can be used &lt;em&gt;without any a priori programming skills&lt;/em&gt;.
In other words, most of my audience is looking for a &amp;quot;graphical&amp;quot; text mining environment that can be used without first having to learn how to program.
For example, to use NLTK, LingPipe, OpenNLP, StandfordNLP, UIMA, etc., you will first have to learn how to program in the chosen framework's API language.
Therefore, the only entirely true answer is that only commercial tools can offer a &lt;strong&gt;pure&lt;/strong&gt; &amp;quot;graphical user interface&amp;quot; and require no programming experience.&lt;/p&gt;
&lt;p&gt;However, GATE can be used &lt;em&gt;mostly&lt;/em&gt; without having to write code - with the exception of its &amp;quot;JAPE-glue&amp;quot;.
&lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch8.html#chap:jape"&gt;JAPE&lt;/a&gt; stands for &amp;quot;Java Annotation Patterns Engine&amp;quot; and is GATE's solution to make data inter-operable between different text mining resources that commonly have different I/O requirements.
Furthermore, JAPE can be used to design entire rule-based annotation resources of their own right.
However, JAPE &amp;quot;grammars&amp;quot; consist of rules where the left-hand side of the grammatical rule matches (existing) GATE annotations using a (clear and simple) syntax, while the right-hand side of those rules can contain Java code that will somehow modify those annotations.
Therefore, GATE rids you from the need of writing code of your own, except for (small) blocks of (simple) code for the right-hand sides of JAPE's rules.
Luckily, GATE's extensive documentation provides lots of &lt;a class="reference external" href="https://gate.ac.uk/wiki/jape-repository/"&gt;examples&lt;/a&gt; to start with for the novice.&lt;/p&gt;
&lt;p&gt;Overall, this makes GATE the only free open source text mining software that provides a graphical interface &lt;em&gt;and&lt;/em&gt; requires (nearly) no programming skills to use it.
As I stated initially, this fact alone makes it the best fit for my typical tutorial audiences, because most of them are neither computer scientists nor do they (want to) know how to code.&lt;/p&gt;
&lt;p&gt;As mentioned in the introduction, &amp;quot;out-of-the-box&amp;quot; GATE isn't always the fastest solution.
However, due to its open source nature that only means that if you need to go faster, you always can replace any slow pieces with whatever you consider a better fit (if you know how to program, that is...)
For example, the &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch23.html#sec:parsers:taggerframework"&gt;Generic Tagger&lt;/a&gt; framework is a &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch4.html#x7-690004"&gt;CREOLE&lt;/a&gt; &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch3.html#x6-540003.7"&gt;Processing Resource&lt;/a&gt; that allows you to take any existing &lt;a class="reference external" href="https://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;sequence tagger&lt;/a&gt; and use it to create annotations for a GATE pipeline.
This is pretty nifty, because you can use whatever Part-of-Speech tagger or Named Entity Recognition system you like.
You can even use a generic sequence tagger, train your own model, and integrate it in your text mining and information extraction pipelines, all without having to learn how to program first.&lt;/p&gt;
&lt;p&gt;However, precisely due to the highly generic nature of the Generic Tagger framework, it is not very efficient.
To create GATE annotations with it, this tagger &amp;quot;wrapper&amp;quot; operates as follows &lt;strong&gt;on each input&lt;/strong&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A file with the input text for the tagger is written to disk.&lt;/li&gt;
&lt;li&gt;A new tagger sub-process is launched by the wrapper, reading the input from the file.&lt;/li&gt;
&lt;li&gt;The tagger's results are written back to disk.&lt;/li&gt;
&lt;li&gt;The wrapper resource reads the result file, generates the annotations, and deletes the two temporary files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I have highlighted &amp;quot;on each input&amp;quot;, because this loop might be run for each processed document, for each sentence in each document, or, even worse, for each and every token in your documents.
If you have already thought that doing this for each document is pretty bad, doing that loop for each token grinds your pipeline to a standstill.
Second, if you are a programmer, the expression &amp;quot;a new sub-process is launched&amp;quot; in the second point should be alarming to you.
If the tagger uses some large resources, like a dictionary (which they quite frequently do), starting up a new tagger process can be extremely expensive.
In general, of all concurrent programming concepts, launching a new &lt;em&gt;process&lt;/em&gt; is the probably most expensive resource you can create &amp;quot;within&amp;quot; a program and should be done as sparingly as possible.
The reason the plug-in is designed in this peculiar way isn't because the framework was written by inexperienced programmers, however.
It is that way because due to this design, it truly generic: Most I/O formats and tagger can be handled with this wrapper.&lt;/p&gt;
&lt;p&gt;However, while the Generic Tagger is pretty cool to have on board so you can to try out &amp;quot;foreign&amp;quot; sequence taggers, the way it is implemented makes it rather useless for a &amp;quot;real&amp;quot; pipeline, i.e., beyond experimentation.
For example, just tagging all gene mentions in a few thousand &lt;a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed"&gt;PubMed&lt;/a&gt; sentences with this wrapper takes &lt;em&gt;days&lt;/em&gt;.
But PubMed has over 24 million abstracts and (I think to recall) roughly around 100 million sentences, so go figure...&lt;/p&gt;
&lt;p&gt;Therefore, I am releasing my own CREOLE processing resource that works similarly to the Generic Tagger, but does not do any disk I/O for inter-process communication or launch more than one &amp;quot;singleton&amp;quot; process for the entire pipeline you are designing.
However, this puts some restrictions on the kinds of taggers you can use:&lt;/p&gt;
&lt;p&gt;1. The tagger must support a &lt;strong&gt;streaming I/O&lt;/strong&gt; model.
That is, the tagger must be able to read from some &amp;quot;input stream&amp;quot;, such as UNIX' &lt;cite&gt;STDIN&lt;/cite&gt;, and write to some &amp;quot;output stream&amp;quot;, commonly UNIX' &lt;cite&gt;STDOUT&lt;/cite&gt;.
Another way of putting this is that your tagger should be able to handle UNIX' piped command syntax, something like this: &lt;tt class="docutils literal"&gt;cat plain_text.txt | some_tagger &amp;gt; tagged_text.txt&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;2. The tagger must work with POSIX' classical line-based interface.
That is, the tagger must take one continuous block of text as &lt;em&gt;input&lt;/em&gt;, &lt;strong&gt;terminated with a newline&lt;/strong&gt; character.
For example, it should take one token, sentences or block of text as input (not containing any newlines), and, once it receives a newline character, start tagging that input.&lt;/p&gt;
&lt;p&gt;3. The tagger must produce &lt;strong&gt;one annotation per line&lt;/strong&gt; as &lt;em&gt;output&lt;/em&gt;, and those annotations must be &lt;strong&gt;in the same order&lt;/strong&gt; as the (input) text spans which they annotate.
Those annotations commonly are expected to be in the OTPL (one token per line) format.
For example, the output line &lt;tt class="docutils literal"&gt;Nouns noun NN &lt;span class="pre"&gt;B-NP&lt;/span&gt; O&lt;/tt&gt; might annotate the token &amp;quot;Noun&amp;quot; (verbatim, as found in the input text) with the lemma &amp;quot;noun&amp;quot;, the PoS-tag &amp;quot;NN&amp;quot;, the BIO-chunk &amp;quot;B-NP&amp;quot; and the BIO-NER-tag &amp;quot;O&amp;quot; (&amp;quot;outside&amp;quot; any entity mention).&lt;/p&gt;
&lt;p&gt;If you have a tagger that follows those requirements (it turns out, most sequence taggers I know of work precisely like this), you can instead use my &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework"&gt;Online Tagger&lt;/a&gt; framework.
What it does differently to the Generic Tagger is the following:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;On Processing Resource &lt;strong&gt;initialization&lt;/strong&gt;, you have to specify the location of the tagger and GATE launches the tagger with the supplies parameters (directory where to run the tagger and any arguments, such as dictionaries to load or command-line flags to set).&lt;/li&gt;
&lt;li&gt;The Processing Resource &lt;strong&gt;configuration&lt;/strong&gt; is nearly the same, but some of the defaults have been adapted to better reflect the nature of the on-line processing model.&lt;/li&gt;
&lt;li&gt;Once your pipeline is &lt;strong&gt;running&lt;/strong&gt;, the text is piped into the tagger sub-process and results are read from the output stream, while intermediary files are no longer created.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please clone the tagger from GitHub (&lt;tt class="docutils literal"&gt;git clone &lt;span class="pre"&gt;https://github.com/fnl/OnlineTaggerFramework&lt;/span&gt;&lt;/tt&gt;) into your local CREOLE &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch3.html#x6-530003.6"&gt;user plugin directory&lt;/a&gt;.
Then you can load my plug-in from the CREOLE Plugin Manager and once you instantiate a new &lt;tt class="docutils literal"&gt;GenericOnlineTagger&lt;/tt&gt; Processing Resource, you will be asked to supply the initial configuration data to launch the tagger in its own sub-process (tagger binary path, directory to run in [if any], runtime flags and arguments [if any]).&lt;/p&gt;
&lt;p&gt;If you run into any issue using this plug-in, please consider filing a &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework/issues"&gt;bug report&lt;/a&gt; on GitHub so I can fix the problem for everybody using it.
I hope the this plug-in will make you enjoy the new-found efficiency when integrating sequence taggers into your GATE pipelines!&lt;/p&gt;
</content><category term="Machine Learning"></category><category term="text mining"></category><category term="nlp"></category><category term="Java"></category></entry><entry><title>segtok - a segmentation and tokenization library</title><link href="https://fnl.es/segtok-a-segmentation-and-tokenization-library.html" rel="alternate"></link><published>2015-01-12T00:00:00+01:00</published><updated>2015-01-12T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2015-01-12:/segtok-a-segmentation-and-tokenization-library.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Surprisingly, it is hard to find a good command-line tool for sentence segmentation and word tokenization that works well with European languages.
Here, I present &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;, a &lt;strong&gt;Python 2.7 and 3&lt;/strong&gt; package, API, and Unix command-line tool to remedy this shortcoming.&lt;/p&gt;
&lt;div class="section" id="text-processing-pipelines"&gt;
&lt;h2&gt;Text processing pipelines&lt;/h2&gt;
&lt;p&gt;This is the …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Surprisingly, it is hard to find a good command-line tool for sentence segmentation and word tokenization that works well with European languages.
Here, I present &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;, a &lt;strong&gt;Python 2.7 and 3&lt;/strong&gt; package, API, and Unix command-line tool to remedy this shortcoming.&lt;/p&gt;
&lt;div class="section" id="text-processing-pipelines"&gt;
&lt;h2&gt;Text processing pipelines&lt;/h2&gt;
&lt;p&gt;This is the second in a series of posts that will present lean and elegant text processing tools to take you across the void from your document collection to the linguistic processing tools (&lt;em&gt;and back&lt;/em&gt;, but more on that in the future).
In the &lt;a class="reference external" href="https://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;last post&lt;/a&gt;, I discussed sequence taggers, as they form the entry point to natural language processing (NLP).
Today I am presenting existing tools and a little library of mine for pre-processing of Germanic and Romance language text (essentially, because I am not knowledgeable of any others...).
Text processing pipelines roughly consist of the following three steps:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Document extraction&lt;/li&gt;
&lt;li&gt;Text pre-processing&lt;/li&gt;
&lt;li&gt;Language processing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are many good tools around for document extraction, in particular Apache &lt;a class="reference external" href="http://tika.apache.org/"&gt;Tika&lt;/a&gt; is a great software library I highly recommend for this task.
(As a matter of fact, if you want to improve your programming skills and see some real-life, clean implementations of nearly all Java/GoF software patterns, have a look at the innards of Tika.)
If you want to extract data from PDFs in particular, you should be looking for your preferred tool in the huge collection of software that is based on the excellent &lt;a class="reference external" href="http://www.foolabs.com/xpdf/"&gt;xpdf&lt;/a&gt; library or even OCR libraries like &lt;a class="reference external" href="https://code.google.com/p/tesseract-ocr/"&gt;Tesseract&lt;/a&gt;.
As for language processing, there are several tools for chunking and tagging with dynamic graphical models that you can choose from, as outlined in an &lt;a class="reference external" href="https://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;earlier post&lt;/a&gt; of mine, and for uncovering more involved semantic relationships, dependency parsers like &lt;a class="reference external" href="https://github.com/syllog1sm/redshift"&gt;RedShift&lt;/a&gt; are available.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="overview-and-motivation"&gt;
&lt;h2&gt;Overview and motivation&lt;/h2&gt;
&lt;p&gt;However, for the intermediate pre-processing, short from cooking up your own solution, the currently best solution is to use one of the large natural language processing (NLP) frameworks.
There are a few sentence segmentation and tokenization libraries around, particularly in Perl, but they do not have the desired properties to handle more complex cases or are long &lt;a class="reference external" href="http://mailman.uib.no/public/corpora/2007-October/005429.html"&gt;forgotten&lt;/a&gt;.
The only more recent, statistics-based tool I could find is &lt;a class="reference external" href="https://code.google.com/p/splitta/"&gt;splitta&lt;/a&gt;, but development seems to have died off yet again.
If you check out its Issues page, nobody seems to be fixing the problems, it does not work with Python 3, and its command-line implementation is not really ready for text processing with Unix.
Another example is &lt;a class="reference external" href="https://www.tm-town.com/natural-language-processing"&gt;Pragmatic Segmenter&lt;/a&gt;, a rule-based segmenter written in Ruby.
But if you feed it examples with abbreviations or other occurrences of problematic issues I will discuss in this post, you will see that it performs worse than even the statistical approaches provided by the frameworks discussed next.&lt;/p&gt;
&lt;p&gt;This leaves you with &lt;a class="reference external" href="http://www.nltk.org"&gt;NLTK&lt;/a&gt;'s &lt;a class="reference external" href="http://www.nltk.org/_modules/nltk/tokenize/punkt.html"&gt;PunktTokenizer&lt;/a&gt;, &lt;a class="reference external" href="http://alias-i.com/lingpipe/"&gt;LingPipe&lt;/a&gt;'s &lt;a class="reference external" href="http://alias-i.com/lingpipe/demos/tutorial/sentences/read-me.html"&gt;SentenceModel&lt;/a&gt;, &lt;a class="reference external" href="http://opennlp.apache.org/"&gt;OpenNLP&lt;/a&gt;'s &lt;a class="reference external" href="http://opennlp.sourceforge.net/api/opennlp/tools/sentdetect/SentenceDetectorME.html"&gt;SentenceDetectorME&lt;/a&gt;, and quite a few more frameworks that have APIs to bridge that gap.
(Again, if you enjoy looking at Java [Kingdom of Nouns...] source code, check out LingPipe - while commercial, it is really well designed.)
However, such a heavy-handed approach is to break a butterfly with a wheel, and in my personal opinion/experience, none of them are doing a particularly good job at segmenting, either.
If you don't believe me (never do!), just compare their performance/results against the rule-based library I am about to present here - you will see that these statistical segmenters produce a significant number of false positives on  orthographically (mostly) correct texts.
Their strength compared to this library here, however, is language independence: the library discussed here (so far?) only works with Indo-European languages, while the mentioned segmenters from the above frameworks can be trained on nearly any language and domain.
Particularly, the unsupervised PunktTokenizer only needs sufficient text and the presence of a terminal marker to learn the segmentation rules on its own.
Similarly, if you want to parse noisy text with bad spelling (Twitter and other &amp;quot;social&amp;quot; media sources), you might be best advised to use those frameworks.
So while I do think these libraries are all great as a whole - and I would recommend any one of them - it is mildly annoying that you have to learn a framework if all you want to do is common text pre-processing.&lt;/p&gt;
&lt;p&gt;If you are analyzing corporate documents, patents, news articles, scientific texts, technical manuals, web pages, etc., that tend to have good orthography these statistical tools are not quite up to it and introduce far too many splits, at least for my taste.
This then affects your downstream language processing, because the errors made by the pre-processing will be propagated and, in the worst case, even amplified.
Therefore, text processing pipelines commonly end up doing both (2) and (3) using one such framework, but years of experience have shown me that you soon will be wanting to explore methods beyond whatever particular framework you chose offers.
That then can mean that you have to re-conceptualize all of (2) to add a different tool or even need to move to a whole new framework.
If the performance of the newly integrated framework then isn't that stellar either, it becomes disputable if it even was worth the effort.
Last but not least, this framework-based software development approach violates one of the most fundamental Unix philosophies:&lt;/p&gt;
&lt;blockquote&gt;
“A  program should do one thing and it should do it well.
Programs should handle &lt;em&gt;text streams&lt;/em&gt;, because that is a universal interface.”
(Doug McIlroy)&lt;/blockquote&gt;
&lt;p&gt;(Yes, I know, it seems this library is violating the “one thing” rule because it does two things: segmenting and tokenization.
But as you will see, the library comes with two independent scripts and APIs for each step.)&lt;/p&gt;
&lt;p&gt;The next two sections are for newcomers to this topic and explains why segmenting and tokenizing isn't that trivial as it might appear.
If you are an expert, you can skip the next two sections and read on where &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt; is introduced.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-is-sentence-segmentation-and-tokenization"&gt;
&lt;h2&gt;What is sentence segmentation and tokenization?&lt;/h2&gt;
&lt;p&gt;Nearly any text mining and/or linguistic analysis starts with a sentence and word segmentation step. That is, determining all the individual spans in a piece of text that constitute its sentences or words.
Identifying sentences is important because they form logical units of thought and represent the borders of many grammatical effects.
All our communication - statements, questions, and commands - are expressed by sentences or at least a meaningful sentence-like fragment, i.e., a phrase.
Words on the other hand are the atomic units that form the sentences, and ultimately, our language.
While words are built up from a sequence of symbols in most languages, these symbols have no semantics of their own (except for any single-symbol words in that language, naturally).
Therefore, nearly any meaningful text processing task will require the segmentation of the sequence of symbols (characters in computer lingo) into sentences and words.
These words are, at least in the context of computational text processing, often called &lt;strong&gt;tokens&lt;/strong&gt;.
Beyond the actual words consisting of letters, a token includes atomic units consisting of other symbols.
For example, the sentence terminals (., !, and ? are three such tokens), a currency symbol, or even chains of symbols (for example, the ellipsis: …).
By following through with this terminology, the process of segmenting text into these atomic units is commonly called &lt;em&gt;tokenization&lt;/em&gt; and a computer subroutine doing this segmentation is known as a &lt;em&gt;tokenizer&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sentence-segmentation-and-tokenization-is-hard"&gt;
&lt;h2&gt;Sentence segmentation and tokenization is hard&lt;/h2&gt;
&lt;p&gt;While this segmentation step might initially sound like a rather trivial problem, it turns out the rabbit hole is deep and no perfect solution has been found to date.
Furthermore, the problem is made harder by the different symbols and their usage in distinct languages.
For example, just finding word boundaries in Chinese is non-trivial, because there is not boundary marker (unlike the whitespace used by Indo-European languages).
And when looking into technical documents, the problem can grow even more out of hand.
Names of chemical compounds, web addresses, mathematical expressions, etc. are all complicating the way one would normally define a set of word boundary detection rules.
Another source of problems are texts from public internet fora, such as Twitter.
Words are not spelled as expected, the spaces might be missing, and emoticons and other ASCII-art can make defining the correct tokenization strategy a rather difficult endeavor.
Similarly, the sentence terminal marker in many Indo-European languages is the full-stop dot – which coincidentally is also used as the abbreviation marker in those languages.
For example, detecting the (right!) three sentences in in the following text fragment is not that trivial, at least for a computer program.&lt;/p&gt;
&lt;blockquote&gt;
Hello, Mr. Man. He smiled!! This, i.e. that, is it.&lt;/blockquote&gt;
&lt;p&gt;If the text fragments are large or the span contains many dots, even humans will start to make many errors when trying to identify all the sentence boundaries.
Certain proper nouns (gene names, or “amnesty international”, for example) might demand that the sentence begins with a lower case letter instead of the expected upper-case.
A simple typo might have been the cause for a sentence starting with a lower-case letter, too.
Again in public internet fora, users sometimes resort to using only lower- or upper-case for their  messages or write in an orthographically invalid mix of letter casing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="introducing-segtok-a-python-library-for-these-two-issues"&gt;
&lt;h2&gt;Introducing segtok – a Python library for these two issues&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip3 install segtok
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The Unix approach to software (one thing only, with a text-based interface [that can be used with pipes]) allows you to integrate programs at each stage of your tool-chain and makes it simple to quickly exchange any parts.
If you use a large framework, on the other hand, you are constrained by it and might feel tempted to accept that some of the things you will be doing with it aren't done quite as efficiently as possible.
And the more you make use of that large framework, the less attractive it is to switch your tooling and move out of that &amp;quot;comfort zone&amp;quot;.
I think this issue has a direct, detrimental effect on our ability to experiment and adapt to new tools, software, and methods.&lt;/p&gt;
&lt;p&gt;Due to the many different ways this problem can be solved and the inherent complexity if considering all languages, &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;, the library presented here, is confined to processing orthographically regular Germanic (e.g., English) and Romance (e.g., Spanish) texts.
It has a strong focus on those two and German, which all use Latin letters and standard symbols (like .?!”'([{, etc.).
This is mostly based on the fact that I only know those three languages to some reasonable degree (my tourist-Italian does not count...) - while help/contributions to make segtok work in more languages would be very welcome!
Furthermore, &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt; was made to cope with text having the following properties:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Capable of (correctly!) handling the whole Unicode space.&lt;/li&gt;
&lt;li&gt;A sentence termination marker must be present.&lt;/li&gt;
&lt;li&gt;Texts that follow a mostly regular writing style - in particular, segtok is not tuned for Twitter's highly particular orthography.&lt;/li&gt;
&lt;li&gt;It can handle technical texts (containing, e.g., chemical compounds) and internet URIs (IP addresses, URLs and e-mail addresses).&lt;/li&gt;
&lt;li&gt;The tool is able to handle (valid) cases of sentences starting with a lower-case letter and correctly splits sentences enclosed by parenthesis and/or quotation marks.&lt;/li&gt;
&lt;li&gt;It is able to handle some of the more common cases of heavy abbreviation use (e.g., academic citations).&lt;/li&gt;
&lt;li&gt;It treats all &lt;em&gt;Unicode&lt;/em&gt; dashes (there are quite a few of them in Unicode land) “The Right Way” - a functionality surprisingly absent from most tools.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, the two scripts that come with segtok have a very simple plain-text, line-based interfaces that work well when joined with Unix pipe streams.
The first script, &lt;tt class="docutils literal"&gt;segmenter&lt;/tt&gt;, segments sentences in (plain) text files into one sentence per line.
The other, &lt;tt class="docutils literal"&gt;tokenizer&lt;/tt&gt;, splits tokens on single lines (usually, the sentences from the &lt;tt class="docutils literal"&gt;segmenter&lt;/tt&gt;) by adding whitespaces where necessary.
On the other hand, if you are a Python developer, you can use the functions (&amp;quot;Look Ma, no nouns!&amp;quot;...) provided by this library to incorporate this approach in your own software (the tool is MIT licensed, btw.).
Segtok is designed to handle texts with characters from the entire Unicode space, not just ASCII or Latin-1 (ISO-8859-1).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sentence-segmentation-with-segtok"&gt;
&lt;h2&gt;Sentence segmentation with segtok&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;segtok.segmenter&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;split_single&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_multi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On the sentence level, segtok can detect sentences that are contained inside brackets or quotation marks and maintains those brackets as part of the sentence;
For example:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;(A sentence in parenthesis!)&lt;/li&gt;
&lt;li&gt;Or a sentence with &amp;quot;a quote!&amp;quot;&lt;/li&gt;
&lt;li&gt;'How about handling single quotes?'&lt;/li&gt;
&lt;li&gt;[Square brackets are fine, too.]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The segmenter is constrained to only segment on single lines ( &lt;tt class="docutils literal"&gt;split_single&lt;/tt&gt; - sentences are not allowed to cross line boundaries) or to consecutive lines ( &lt;tt class="docutils literal"&gt;split_multi&lt;/tt&gt; - splitting is allowed across newlines inside “paragraphs” separated by two or more newlines).
(If you really want to extract sentences that cross consecutive newline characters, please remove those line-breaks from your text first.
Segtok assumes your content has some minimal semantical meaning, while superfluous newlines are nothing more than noise.)
It gracefully handles enumerations, dots, multiple terminals, ellipsis, and similar issues:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A. The first assumption.&lt;/li&gt;
&lt;li&gt;2. And that is two.&lt;/li&gt;
&lt;li&gt;(iii) Third things here.&lt;/li&gt;
&lt;li&gt;What the heck??!?!&lt;/li&gt;
&lt;li&gt;A terminal ellipsis...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In essence, a valid sentence terminal must be represented by one of the allowed Unicode markers.
That is, the many Unicode variants of ., ?, and !, and the ideographic full-stop: “。” (a single character).
Therefore, &lt;em&gt;this library cannot guess a sentence boundary if the marker is absent&lt;/em&gt;!
After the marker, up to one quotation mark and one bracket may be present.
Finally, the marker must be separated from the following non-space symbol by at least one whitespace character or a newline.&lt;/p&gt;
&lt;p&gt;This requires that the sentence boundaries do obey some limited amount of regularity.
But at the same time, the pesky requirement that a marker is followed by upper-case letters is absent from this strategy.
In addition, this means that “inner” abbreviation markers are never a candidate (such as in “U.S.A.”).
On the other hand, any markers that do not follow this “minimal” pattern will always result in false negatives (i.e., not be split).
While missing markers and markers not followed by a space character do occur, those cases are very infrequent in orthographically correct texts.&lt;/p&gt;
&lt;p&gt;After these &lt;em&gt;potential&lt;/em&gt; markers have been established, the method goes back and looks at the surrounding text to determine if that marker is not at a sentence boundary after all.
This step recuperates cases like initials (“A. Name”), species names (“S. pombe”) and abbreviations inside brackets, which are common with citations (“[A. Name, B. Other. Title of the work. Proc. Natl. Acad. Sci. 2010]”).
Obvious and common abbreviations (in English, Spanish, and German, so far) followed by a marker are dropped, too.
There are several other enhancements to the segmenter (e.g., checking for the presence of lower-case words that are unlikely start a sentence) that can be studied in the source code and unit tests.
In summary, while coming at a computational cost, this second check is what allows segtok to keep the number of false positive splits to an acceptable low if compared to existing methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="segtok-s-tokenization-strategy"&gt;
&lt;h2&gt;Segtok's tokenization strategy&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;segtok.tokenizer&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;symbol_tokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_tokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;web_tokenizer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;segtok.tokenizer&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;split_possessive_markers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_contractions&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The tokenization approach uses a similar approach.
First, a maximal token split is made, and then several functions wrap this basic approach, encapsulating successively more complex rules that join neighboring tokens back together based on their orthographic properties.
The basic, maximum split rule is to segment everything that is separated by spaces and then within the remaining non-space spans, split anything that is alphanumeric from any other symbols:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;a123, an &lt;span class="pre"&gt;alpha-/-beta...&lt;/span&gt;&lt;/tt&gt; → &lt;tt class="docutils literal"&gt;a123&amp;nbsp; ,&amp;nbsp; an&amp;nbsp; alpha&amp;nbsp; &lt;span class="pre"&gt;-/-&lt;/span&gt;&amp;nbsp; beta&amp;nbsp; ...&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;This functionality is provided by the &lt;tt class="docutils literal"&gt;symbol_tokenizer&lt;/tt&gt; .
Next, the non-alphanumeric &lt;em&gt;symbols&lt;/em&gt; are further analyzed to determine if they should form part of a neighboring alphanumeric &lt;em&gt;word&lt;/em&gt;.
If so, the symbols are merged back together with their alphanumeric spans.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Abbreviation markers are attached back on to the proceeding word (“Mr.”).&lt;/li&gt;
&lt;li&gt;Words with internal dots, dashes, apostrophes, and commas are joined together again (“192.168.1.0”, “Abel-Ryan's”, “a,b-symmetry”).&lt;/li&gt;
&lt;li&gt;The spaces inside a word-hyphen-spaces-word sequence are dropped.&lt;/li&gt;
&lt;li&gt;Superscript and subscript digits, optionally affixed with plus or minus, are attached to a proceeding word that is likely to be a physical unit (“m³”) or part of a chemical formula, respectively (“[Al₂(S₁O₄)₃]²⁻” → “[”, “Al₂”, “(”, “S₁O₄”, “)₃”, “]²⁻”).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This set of functionality is provided by the &lt;tt class="docutils literal"&gt;word_tokenizer&lt;/tt&gt;.
Finally, if desired, a Web-mode function will further ensure that valid e-mail addresses and URLs (including fragments and parameters, but without actual space characters) are always maintained as single tokens (&lt;tt class="docutils literal"&gt;web_tokenizer&lt;/tt&gt;).
All this ensures that while a decent amount of splitting is made, the common over-splitting of tokens is avoided.
Particularly, when processing biomedical documents, Web content, or patents, too much tokenization might have quite a significant negative impact on any subsequent, more advanced processing techniques.
As before with the segmenter, I believe this recovery of false positives is the particular strength of this library.&lt;/p&gt;
&lt;p&gt;After the tokenization step, the API provides two functions to optionally split off English possessive markers (“Fred's”, “Argus'”) and even contractions (“isn't” → “is n't” [note the attachment of the letter n], “he'll” → “he 'll”, “I've” → “I 've”, etc.) as their own tokens, which can be useful for downstream linguistic parsing (&lt;tt class="docutils literal"&gt;split_possessive_markers&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;split_contractions&lt;/tt&gt;).
To use them, just wrap your tokenizer with the preferred method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;SimpleTokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;split_multi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;split_contractions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;  &lt;span class="c1"&gt;# None to signal sentence terminals&lt;/span&gt;


&lt;span class="c1"&gt;# An even shorter usage example:&lt;/span&gt;
&lt;span class="n"&gt;my_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;split_contractions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt;
             &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;split_multi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_text&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="feedback-and-conclusions"&gt;
&lt;h2&gt;Feedback and Conclusions&lt;/h2&gt;
&lt;p&gt;All this functionality and the API itself are briefly documented on &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;'s &amp;quot;homepage&amp;quot;.
As there is not very much functionality around, I hope that between this guide here and the overview there, the library should be fairly easy to use.
Furthermore, in command-line mode, using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--help&lt;/span&gt;&lt;/tt&gt; option will explain you all options provided by the two scripts the PyPI package installs.&lt;/p&gt;
&lt;p&gt;If you are be looking for new features, you are welcome to extend the library or request a new feature on the tool's GitHub &lt;a class="reference external" href="https://github.com/fnl/segtok/issues"&gt;Issues&lt;/a&gt; page (no guarantees, though... ;-)).
As a forum for discussing this tool, please use this &lt;a class="reference external" href="http://www.reddit.com/r/Python/comments/2sala9/segtok_a_rulebased_sentence_segmenter_and_word/"&gt;Reddit&lt;/a&gt; thread.
In addition, if you use this library and run into any problems, I would be glad to receive bug reports there, too.
Overall, I have attempted to keep the strategy used by segtok as slim as possible.
So if you are using any heavy language processing or sequence analysis tools after segtok, it should have no impact on your throughput at all.&lt;/p&gt;
&lt;p&gt;I have created this library after being disappointed by the other approaches in the wild, and for regular texts, my experience is that it works substantially superior in at least one of segmentation capabilities and/or runtime performance.
As I do not wish to bash any existing tool, I will only name one sentence segmentation approach I like very much: Punkt Tokenizer by Kiss and Strunk, 2006.
PT is a unsupervised, statistical approach to segmentation that “learns” whether to split sentences at sentence terminal markers.
While quite impressive and very versatile due to its unsupervised nature, I can state clearly that segtok's segmenter works substantially better on Germanic and Romance texts that (mostly) have a proper orthography.
Unsurprisingly, segtok's sentence segmenter is substantially faster than a comparable Python &lt;a class="reference external" href="http://www.nltk.org/_modules/nltk/tokenize/punkt.html"&gt;implementation&lt;/a&gt; of the Punkt Tokenizer by NLTK.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Machine Learning"></category><category term="text mining"></category><category term="nlp"></category><category term="Python"></category></entry><entry><title>A review of sparse sequence taggers</title><link href="https://fnl.es/a-review-of-sparse-sequence-taggers.html" rel="alternate"></link><published>2014-10-02T00:00:00+02:00</published><updated>2014-10-02T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2014-10-02:/a-review-of-sparse-sequence-taggers.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Right now, use &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt; &lt;em&gt;unless&lt;/em&gt; you want to go beyond first-order and/or linear models, need the fastest possible training cycles, or are a Scala programmer, in which case you would be best advised to choose &lt;a class="reference external" href="http://factorie.cs.umass.edu/"&gt;Factorie&lt;/a&gt;.
OK, so that's that for a stressed out generation;
Read …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Right now, use &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt; &lt;em&gt;unless&lt;/em&gt; you want to go beyond first-order and/or linear models, need the fastest possible training cycles, or are a Scala programmer, in which case you would be best advised to choose &lt;a class="reference external" href="http://factorie.cs.umass.edu/"&gt;Factorie&lt;/a&gt;.
OK, so that's that for a stressed out generation;
Read on if you want to know why I recommend those two tools.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Overview:&lt;/strong&gt;
The goal of this review is to identify the &amp;quot;best&amp;quot; generic CRF- or MEMM-based sequence tagger software with a free (MIT/BSD-like) license.
We will only take discriminative models into account, so if your beef are generative models and/or non-sparse data (e.g., HMMs), you have come to the wrong place.
This article will look into their abilities to define and generate features, training times, tagging throughput, and tagging performance by way of working through a common sequence labeling problem: tagging the parts-of-speech of natural language.
While PoS tagging can be considered a &amp;quot;solved&amp;quot; problem, PoS tagging performance differences are still a source of &lt;a class="reference external" href="http://aclweb.org/aclwiki/index.php?title=POS_Tagging_%28State_of_the_art%29"&gt;academic&lt;/a&gt; controversy and therefore an ideal testing ground.&lt;/p&gt;
&lt;p&gt;Nearly any interesting natural language processing (NLP) task starts with word &lt;a class="reference external" href="http://text-processing.com/demo/tag/"&gt;tagging&lt;/a&gt;:
That is, resolving each word's grammatical sense - it's &lt;em&gt;part-of-speech&lt;/em&gt; (&lt;strong&gt;PoS&lt;/strong&gt;), the phrases they group into, and the semantic meaning the words carry.
PoS refers to a word's morphology, e.g., if it is used a noun or an adjective, or the inflection of the verb.
Words of the same morphology can often be grouped (we say, &lt;strong&gt;chunked&lt;/strong&gt;) into phrases, such as &amp;quot;a noun phrase&amp;quot;.
As for the semantic meaning, text miners are usually interested in identifying the relevant entity class/type (such as person, location, date/time, ...) the word refers to.
Furthermore, in NLP, words are commonly called &lt;strong&gt;tokens&lt;/strong&gt; to use a name that also covers symbols and numbers, including dots, brackets, or commas.
These tokens form the atomic sequence units for many statistical NLP methods.&lt;/p&gt;
&lt;p&gt;Similarly, in bioinformatics, you might want to identify properties of biological sequences, e.g., DNA binding sites or predict locations of post-translational modifications in proteins.
In general, any sequence tagger could be used to classify elements in any kind of sequence you can split into discrete units.
Therefore, in bioinformatics, those units might be nucleic acids (DNA bases) or amino acids (proteins).
However, the devil lays in the details:
The implementations I am interested in here are for &amp;quot;information-sparse sequences&amp;quot;, such as text.
The difference is the element (and resulting feature) sparsity: while there are easily 20,000 different tokens contained in any average text collection (such as a book), there are only four DNA bases and twenty-something amino acids (depending on the species).
All amino acids and bases will be - compared to text tokens, at least - frequently used in their respective sequences, with the obviously lowest sparsity for the four &lt;em&gt;standard&lt;/em&gt; DNA bases (long story hidden here...).
So to define some scope of this review, I am interested in learning patterns from extremely sparse data;
If your data is &amp;quot;dense&amp;quot;, you might be better off looking into more general algorithms, such as hidden Markov models (HMMs).&lt;/p&gt;
&lt;p&gt;You can play around with a few example NLP taggers by following the &lt;a class="reference external" href="http://text-processing.com/demo/tag/"&gt;tagging&lt;/a&gt; link, and you will see that depending on the system and its training data, results can vary widely.
This is due to implementation details, graphical model capabilities, and the sequence features used by the particular model instance.
The common approach to this kind of problem is learning a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Discriminative_model"&gt;discriminative&lt;/a&gt;, dynamic graphical model; commonly, a maximum entropy (MaxEnt, aka. logistic regression) based decision function worked into a reverse &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Markov_model"&gt;Markov model&lt;/a&gt; or into the more complex Markov random field, in this particular case called a &lt;em&gt;Conditional Random Field&lt;/em&gt; (&lt;strong&gt;CRF&lt;/strong&gt;).
In the former &lt;em&gt;MaxEnt Markov model&lt;/em&gt; (&lt;strong&gt;MEMM&lt;/strong&gt;) scenario, you are only allowed to use the current state (element in your sequence, including any meta-data assigned to that element) and the last tag(s) to predict the current tag (aka. label).
The latter CRF model allows you to integrate features not just from the current state, but from any state in the sequence being labeled.
So with CRFs you can even use states from the &amp;quot;future&amp;quot; (i.e., elements later in the sequence that the one currently being tagged) to predict the current label (aka. tag).&lt;/p&gt;
&lt;p&gt;While outside the scope of this article, in case you are now asking yourself &amp;quot;Why do I then even want a MEMM instead of a CRF?&amp;quot;:
Defenders of MEMMs claim that &amp;quot;their&amp;quot; model has an edge because it does not tend to overfit the data (we say, it has a weaker &amp;quot;domain adaptation&amp;quot;) as easily as a CRF can (due to the way features can be generated from any part of the sequence) and therefore produces better tagging results on input that is not similar to (of a &amp;quot;different domain&amp;quot; than) the training data.
Second, due to its feature selection limitation, training of a MEMM tends to be faster than training a CRF with &amp;quot;long-range&amp;quot; features from distant positions in the sequence.&lt;/p&gt;
&lt;p&gt;Both regular Markov models and random fields use the notion of &lt;em&gt;Markov order&lt;/em&gt;.
That is, the number of former (already assigned) labels in a linear chain (sequence) that may be used to calculate the probabilities of each possible label on the current state.
To be more precise, it is the &lt;em&gt;transition probability&lt;/em&gt; from one (first order) or more (second, third, ... order) former labels to the label on the current state that is the statistic being modeled.
This &amp;quot;&lt;tt class="docutils literal"&gt;n&lt;/tt&gt;th-order Markov&amp;quot; limit defines how many prior tags the model will consider when tagging the current state:
For first order Markov chains you only can make use of the last tag, for second order models you get to use the last two tags, and so forth.
Due to the expense of going beyond first-order models, all but one tool we will be looking at do not support more than first-order models:
Linear chain models scale exponentially in the Markov order + 1, meaning that a second order model already has &amp;quot;number of labels&amp;quot;-cubed possible label transitions.
Going beyond second-order Markov models is only desirable if the number of labels and (as we will see) sometimes even states is small (i.e., &amp;quot;dense data&amp;quot;).
In language processing, these states normally are all the unique, observed tokens, also known as the &lt;em&gt;vocabulary&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A quick, shameless self-plug: if you are not familiar with these concepts, have a look at my &lt;a class="reference external" href="https://fnl.es/an-introduction-to-statistical-text-mining.html"&gt;introduction to text mining&lt;/a&gt; slides - or come visit my class in the context of the Madrid summer school on &lt;a class="reference external" href="http://www.dia.fi.upm.es/?q=es/ASDM"&gt;Advanced Statistics and Data Mining&lt;/a&gt; next summer (beginning of July)! The lecture will take you from basic Bayesian statistics all the way to the dynamic, graphical models being discussed here.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sequence-tagger-selection"&gt;
&lt;h2&gt;Sequence tagger selection&lt;/h2&gt;
&lt;p&gt;Recently, I have become anxious to once and for all resolve my doubts about the &amp;quot;best&amp;quot; sparse sequence tagger in terms of ease of use (documentation/UI), feature modeling capabilities, training times, tagging throughput (tokens/second) and the resulting accuracy.
All tools use the same optimization procedures for the learning process, that is a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Limited-memory_BFGS"&gt;L-BFGS&lt;/a&gt; optimizer and a few light-weight gradient descent implementations as alternatives.
However, implementation details, the graphical model abilities, the features the system can work with, the facilities it provides to generate them, the system's throughput, the provided documentation, and its availability (both in open source and free software terms) varies greatly between libraries.
Available software for this task that I considered were &lt;a class="reference external" href="http://crfpp.sourceforge.net/"&gt;CRF++&lt;/a&gt; (Kudo), &lt;a class="reference external" href="http://www.chokkan.org/software/crfsuite/"&gt;CRF Suite&lt;/a&gt; (Okazaki), &lt;a class="reference external" href="http://factorie.cs.umass.edu/"&gt;Factorie&lt;/a&gt; (McCallum &amp;amp;al.), &lt;a class="reference external" href="http://flexcrfs.sourceforge.net/"&gt;FlexCRFs&lt;/a&gt; (Phan, Nguyen &amp;amp; Nguyen), &lt;a class="reference external" href="http://alias-i.com/lingpipe/index.html"&gt;LingPipe&lt;/a&gt; (Carpenter &amp;amp;al.), &lt;a class="reference external" href="http://mallet.cs.umass.edu/"&gt;MALLET&lt;/a&gt; (McCallum &amp;amp;al.), &lt;a class="reference external" href="http://www.umiacs.umd.edu/~hal/megam/"&gt;MEGAM&lt;/a&gt; (Daume III), [Apache] &lt;a class="reference external" href="http://opennlp.apache.org/"&gt;OpenNLP&lt;/a&gt; (Kottmann &amp;amp;al.), &lt;a class="reference external" href="http://nlp.stanford.edu/software/tagger.shtml"&gt;Stanford Tagger&lt;/a&gt; (Manning, Jurafsky &amp;amp; Liang), &lt;a class="reference external" href="http://www.lsi.upc.edu/~nlp/SVMTool/"&gt;SVM Tool&lt;/a&gt; (Giménez &amp;amp; Marquez), &lt;a class="reference external" href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"&gt;TreeTagger&lt;/a&gt; (Schmidt), and &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt; (Lavergne).
There are more options &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Conditional_random_field#Software"&gt;around&lt;/a&gt;, particular in C# and for the .Net platform, but as I do not have the money to pay for the Windows tax, I did not consider them.
If you know of a relevant, &lt;em&gt;generic&lt;/em&gt; sparse sequence tagger implementation I missed (see my filtering criteria below), please &lt;a class="reference external" href="https://fnl.es/pages/about.html"&gt;contact me&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I immediately discarded the Stanford Tagger and the SVM Tool, because they are both orders of magnitude &lt;a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf"&gt;slower&lt;/a&gt; than most other tools considered (and the same goes for MALLET, &lt;a class="reference external" href="http://www.chokkan.org/software/crfsuite/benchmark.html"&gt;too&lt;/a&gt;).
It is worth mentioning that the Stanford Tagger was one of the earliest tools with the software made available for research and a very high accuracy, and as such usually serves as the performance &amp;quot;baseline&amp;quot; for newcomers.
Second, CRF Suite claims to be the fastest first order CRF around and &lt;a class="reference external" href="http://www.chokkan.org/software/crfsuite/benchmark.html"&gt;demonstrates&lt;/a&gt; that CRF++ is significantly slower, which lead me to discard the latter.
That same benchmark claims that CRF Suite is faster than Wapiti, but not only has Lavergne developed several newer versions since then, the difference is far less pronounced, so that tool was not out of the race for me.
Being a free software advocate in the sense of all its aspects - cost, freedom of usage, modifiability, and open source - I feel very uncomfortable about using software with a license that tries to restrict my freedom, including its commercial application.
Therefore, I discarded FlexCRFs, LingPipe, MEGAM, and TreeTagger from the list because of their non-free nature (only being &amp;quot;free for research&amp;quot;, or GPL'ed).
While the GPL is not strictly out of my scope, it creates too many headaches for too many use-cases because it still poses usage restrictions (that I nonetheless support as a necessary evil given the overall copyright SNAFU).
Moreover, excluding the GPL only affects FlexCRFs, which anyways is very similar to CRF++ or CRF Suite.
Two of the already discarded tools would also not make it across this &amp;quot;free software barrier&amp;quot;, by the way (Stanford Tagger and SVM Tool).&lt;/p&gt;
&lt;p&gt;So this left me with CRF Suite, Factorie, OpenNLP, and Wapiti to compare against each other.
Given these harsh pre-filtering criteria, to be honest, I was astonished that I was left with not just one, but four viable and completely free &amp;quot;tools of the trade&amp;quot;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="implementation-considerations"&gt;
&lt;h2&gt;Implementation considerations&lt;/h2&gt;
&lt;p&gt;CRF Suite and Wapiti are both written in C, while Factorie is being coded in Scala, and OpenNLP is based on Java.
So this makes for yet another classical &amp;quot;binary, platform-specific code versus the Java Virtual Machine&amp;quot; comparison!
(Spoiler alert: it does not matter - as you should know already...)
But there a real, noteworthy differences between the taggers; starting with the implemented graphical models and optimization procedures:
Factorie's PoS tagger implementation only makes use of a forward learning procedure, while the other three use the more common (and more expensive) &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm"&gt;forward-backward optimization&lt;/a&gt; approach during training.
So this difference makes up for an interesting test: will forward learning alone be good enough in terms of accuracy, and if so, how much faster will training be?
(You could also read a &lt;a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf"&gt;paper&lt;/a&gt; about this...)
Furthermore, Factorie is a library that allows you to design &lt;em&gt;any kind&lt;/em&gt; of graphical models from basic factor classes (let that sink in if you know what I mean...), so it actually can represent any model you want (including non-linear models).
For the PoS tagging, Factorie uses this forward-only learning approach to maximize a first-order linear-chain CRF.
Next up, Wapiti allows you to choose between a (non-dynamic, pure) MaxEnt model, a MEMM or a CRF model.
Similar to the above &lt;a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf"&gt;paper&lt;/a&gt; linked to, Wapiti can even do dynamic model selection, falling back on simpler models where feasible in the sequence
(Note that Factorie's PoS tagger does not use dynamic model selection.)
Finally, OpenNLP only provides a MEMM implementation, while CRF Suite only provides a CRF.
These implementation details alone might be enough to make your decision:
If you want more than a first-order linear-chain model (say, second-order, or a non-linear graph), your only choice is Factorie.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="software-state-and-documentation"&gt;
&lt;h2&gt;Software state and documentation&lt;/h2&gt;
&lt;p&gt;First, a quick look at the code, implementation, the documentation, and each tool's multi-processor capabilities.
Two remarkable things about &lt;strong&gt;Wapiti&lt;/strong&gt; are how simple and lean the interface is, and its capability of running in multi-threaded mode.
While code is the typical, long spaghetti-code of C, it is clean and very well documented.
The only main downside is that the documentation is a bit sparse; Everything is in there, but they could have done a bit better detailing some of the capabilities, and/or providing examples.
I had to figure out myself that you always need to use both the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-c&lt;/span&gt; &lt;span class="pre"&gt;-s&lt;/span&gt;&lt;/tt&gt; switches when doing (feature-sparse) text labeling and it took me some time to understand how to do feature extraction (designing &amp;quot;patterns&amp;quot;).
The Wapiti authors do not provide a default set of feature pattern templates, only a few pre-trained PoS models for English, German, and Arabic newswire text.&lt;/p&gt;
&lt;p&gt;Similarly, &lt;strong&gt;Factorie&lt;/strong&gt; is able to run in multi-threaded mode and claims to be scalable across machines for hyper-parameter search (which I have not tried).
To use Factorie, I often have to refer back to the code-base, because few of the specifics of Factorie are entirely documented for now.
This means, to use Factorie, you should better know some Scala, as it might be tough having to go through the code otherwise.
In my opinion, some parts of the codebase could have been coded just as well in Java, but that only affects few regions of the library.
One should also note that this opinion is purely subjective and might be of little relevance, so you might be better of judging for yourself.
Finally, the documentation certainly assumes you are an expert for graphical models with plenty of background knowledge in that domain.
Given its state and direction, in comparison to the other tools here, it is probably safe to judge that this library is targeted at the probabilistic programming crack with a background on graphical models, not someone looking for a quick and dirty sequence tagging solution.
The main up-side is that, together with OpenNLP, this is the only library offering a full NLP pipeline (segmentation, tokenization, tagging, and parsing).
But again, except for PoS tagging, the other NLP functionality has to be deduced from the code, as there is not much more documentation on the NLP pipeline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRF Suite&lt;/strong&gt; comes with a nice interface, and although it does not support multi-threading, the C code is well written and very clear.
The functions are short and precise, so it is a nice example how C code can look if you put some effort into it.
The only code-wise downside I detected was the accompanying Python code for preparing and pruning data and benchmarking.
It is clearly written by a C expert with little knowledge of idiomatic Python, no offense (see the performance issues for feature extraction below).
However, I think this is a minor issue given the good documentation and high-quality C code, which in the end is the part that matters.
The worst issue with CRF Suite, however, seems to be that the original author has stopped maintaining the code.
The repository on GitHub has a handful of very good pull requests from serious developers that fix things like a minor memory leak and two or three other issues, but the author has never accepted the requests.
Neither have there been any updates to the library.
To me this means that CRF Suite development seems dead and it would have to be significantly better than the other tools here to make it worth using it.&lt;/p&gt;
&lt;p&gt;Finally, &lt;strong&gt;OpenNLP&lt;/strong&gt; has the expected high quality code found in Apache projects, and it is well documented, too.
The only downsides worthwhile mentioning are that there is no built-in parallel processing support and that it only comes with a MaxEnt Markov model.
As mentioned already, it is important to realize that OpenNLP offers a full NLP pipeline, unlike CRF Suite and Wapiti.&lt;/p&gt;
&lt;p&gt;To summarize, unique capabilities of Wapiti are its built-in template-based feature extraction mechanism and its ability to quickly choose either CRF or MEMM as the target model (more on this below).
A unique capability of Factorie is that it provides you with the necessary base classes to quickly code your own graphical models of any Markov order, both linear and non-linear.
However, admittedly, both Wapiti and Factorie are behind CRF Suite and OpenNLP in terms of documentation and in my opinion, Factorie is not consistently using idiomatic Scala, probably due to its many different developers.
Finally, both OpenNLP and Factorie include a full, documented and undocumented (respectively) NLP pipeline.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="feature-modeling"&gt;
&lt;h2&gt;Feature modeling&lt;/h2&gt;
&lt;p&gt;This leads to the next important consideration: modeling features; for example, via templates (or &amp;quot;patterns&amp;quot;) that define the features used by a model's binary indicator functions.
For example, an indicator function for assigning the PoS tag &amp;quot;VBZ&amp;quot; might be triggered when observing the bigram &amp;quot;I went&amp;quot; and having already assigned the PoS label &amp;quot;PRP&amp;quot; to the token &amp;quot;I&amp;quot;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
f(last_tag, current_tag, states, pos) =
    last_tag == &amp;quot;PRP&amp;quot; &amp;amp;&amp;amp; current_tag == &amp;quot;VBZ&amp;quot; &amp;amp;&amp;amp;
    states[pos-1] == &amp;quot;I&amp;quot; &amp;amp;&amp;amp; states[pos] == &amp;quot;went&amp;quot;
&lt;/pre&gt;
&lt;p&gt;This example is called a &amp;quot;combined (bigram) transition/label and state/token feature&amp;quot;
(And would be minimally encoded as &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;b:%x[-1,0]/%x[0,0]&lt;/span&gt;&lt;/tt&gt; in Wapiti's pattern template language.)
It is a rather &amp;quot;expensive&amp;quot; feature template, as it can easily lead to millions of individual indicator functions:
The number of indicator functions created from this template will be the squared number of PoS tags (label transitions or &amp;quot;bigrams&amp;quot;) times the number of unique token bigrams in the whole training data.
In general, for any template - even a constant one - there will be at least as many indicator functions as there are different tags.&lt;/p&gt;
&lt;p&gt;Except for Wapiti, all taggers come with a pre-defined set of feature templates for common NLP tasks.
Depending on your requirements, this might be either very practical or practically useless, particularly for domain-specific language (tweets, for example) and/or NER tagging.
For NER tagging, your entities might have unique morphological or orthographic properties;
For example, gene names might be used not just as nouns, but as adjectives, too (as in &amp;quot;p53-activated DNA repair&amp;quot;) and contain Roman or Arab numbers, Greek symbols, non-standard dashes and a few other orthographic surprises.
In addition, the entity tag might depend on &amp;quot;knowing the future&amp;quot;, such as the up-coming head token of the noun phrase currently being tagged (e.g., the head &amp;quot;gene&amp;quot; in &amp;quot;the ABC transporter gene&amp;quot; when looking at the token &amp;quot;ABC&amp;quot;).&lt;/p&gt;
&lt;p&gt;The predefined &lt;strong&gt;Factorie&lt;/strong&gt; features are, however, pretty good - so rich indeed, that they are more complete than any other set of features I used or provided myself in the experiments here (see &lt;tt class="docutils literal"&gt;FowardPosTagger.scala&lt;/tt&gt;, &lt;a class="reference external" href="https://github.com/factorie/factorie/blob/master/src/main/scala/cc/factorie/app/nlp/pos/ForwardPosTagger.scala"&gt;features&lt;/a&gt;, for PoS tagging).
That means training could be slow for Factorie, because it needs to optimize over a much larger indicator (feature) function space (turns out it is not, as we will see).
As with all systems except for Wapiti, this means you need to do some coding of your own to adapt the features for NER, while it might or might not be necessary for PoS tagging and phrase chunking tasks.
The real issue with Factorie is figuring out how to define your own NER tagger, as the documentation so far only covers PoS taggers.
More generally speaking, the documentation on generating features and models for your taggers is rather thin in terms of &amp;quot;applied&amp;quot; examples (see User Guide - Learning).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OpenNLP&lt;/strong&gt;, too, comes with a pre-selected list of feature templates for standard NLP tasks.
To change this list, you either have to write your own Java code or, at least for NER tagging, the documentation states that you can conjure up XML configuration files to extract different features.
As I am allergic to any use of XML other than its intended use-case - providing structure to unstructured data - and particularly against the use of XML as a vehicle for configuration files (hello Java/Maven world!), I did not even try this path.
In other words, for OpenNLP I will be using the predefined features and have not experimented with the &amp;quot;XML feature configuration&amp;quot; option, so I cannot tell how well it works or how easy it is to use.
As for PoS tagging, OpenNLP uses pretty much the &lt;em&gt;de facto&lt;/em&gt; standard features (prefixes, suffixes, orthographic features, and a window size of 5 [-2,+2] for the n-grams; see the &lt;tt class="docutils literal"&gt;DefaultPOSContextGenerator&lt;/tt&gt; &lt;a class="reference external" href="http://svn.apache.org/viewvc/opennlp/trunk/opennlp-tools/src/main/java/opennlp/tools/postag/DefaultPOSContextGenerator.java?view=markup"&gt;class&lt;/a&gt;), so that seemed good enough for this test.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRF Suite&lt;/strong&gt; comes with a set of Python scripts to convert simple OWPL files (one word [state] per line, with sentences [sequences] separated by an extra empty line, such as the CoNLL format) into the &amp;quot;per-label feature list files&amp;quot; CRF Suite uses as input.
To create different features, you modify the &amp;quot;template&amp;quot; defined inside the relevant Python script.
For most cases, I think the predefined templates do a pretty good job at generating features for standard NLP tagging tasks.
Additional features uniquely generated by CRF Suite and not OpenNLP or Factorie (for PoS tagging) are quadrigrams, pentagrams, and &amp;quot;long-range interactions&amp;quot;.
The latter are bigrams created from the current word and a word at position +/- 2 to 9 from the current word.
If you commonly work with Python, you might even easily assimilate the Python feature generation process, adapting it to your own needs.
CRF Suite's feature handling has an important shortcoming, however:
It is impossible to work with combined &amp;quot;label bigrams&amp;quot; (1st order Markov transition features) together with other (state) features from the token stream to form more advanced indicator functions.
That is, CRF Suite only models either label transition probabilities or the features from the current state, but does not allow you to create &amp;quot;mixed&amp;quot; indicator functions as described in the beginning of this section (&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;b:%s[-1,0]/%x[0,0]&lt;/span&gt;&lt;/tt&gt;).
This is an important conceptual shortcoming, because it is not possible to define features that condition on both the previous label (i.e., the transition) and the current token.
However, as opposed to the other tools here (that only work with discrete features), CRF Suite provides support for continuos features.
For example, when using &lt;a class="reference external" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"&gt;word embedding&lt;/a&gt; techniques, you might want to directly include the numeric word vectors, which you can simply pass on to CRF Suite.
While this shortcoming is commonly is circumvented by discretizing the real-valued features if continuos feature support is unavailable, it is a noteworthy difference.&lt;/p&gt;
&lt;p&gt;As for &lt;strong&gt;Wapiti&lt;/strong&gt;, you have to figure out how to generate features for each task on your own;
The authors do not provide any predefined templates for &amp;quot;standard&amp;quot; NLP tasks.
But Wapiti provides you with a mechanism to define &amp;quot;patterns&amp;quot;, much like CRF++' feature templates.
Once you fully understand the mechanism, this is indeed quite powerful and it felt like I &amp;quot;missed&amp;quot; it in the other tools.
Particularly, this means that feature extraction is done in C, so it will beat a script-based extraction process, while not requiring any C programming knowledge.
To provide an even playing field, at first I defined the same features &amp;quot;patterns&amp;quot; as CRF Suite does via its Python feature generation scripts.
The problem is that Wapiti uses all possible label and state combinations for its initial training matrix, not just all combinations present in the data.
In other words, it is the only tool that does no feature space reduction &lt;em&gt;prior&lt;/em&gt; to going into training.
For example, if you define a pattern such as the current token (&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;u:%[0,0]&lt;/span&gt;&lt;/tt&gt;), it creates one feature for each label in you training set times the number of unique tokens in your data, no matter if a token is observed with that label in the data or not.
So for token n-grams or label bigrams, the training matrix can quickly grow to extraordinary sizes.
It is worth noting that you can use your own feature extraction method and just feed Wapiti with extracted features directly (i.e., strings that start with &amp;quot;a&amp;quot; or &amp;quot;b&amp;quot;, depending on whether you want to use only the current state or integrate the transition, too).
The advantage - I assume, at least - is that the optimizer might decide to transfer some probability mass to those zero observations.
However, it is not clear to me if or how much performance Wapiti gains from such transfers, particularly when contrasting this unique feature with the greatly increased space penalty:
While Wapiti does compacting and supports sparse matrices during training, as it initially starts of with all features, training becomes rather sluggish when very large feature spaces are defined.
By using the same feature templates as CRF Suite, I ended up with an initial matrix containing a few hundred million features.
This simply was too much for my weak dual-core i5 processor to handle in realistic time.
In the end, I decided to cut down on the number of PoS feature templates with respect to CRF Suite or Factorie.
In particular, I removed the feature templates only CRF Suite has otherwise, thereby reducing the initial setup to 44 million &amp;quot;features&amp;quot; (indicator functions).
After compacting, Wapiti's final model contained 1.8 million indicator functions (&amp;quot;features functions&amp;quot;, or worse, sometimes just called &amp;quot;features&amp;quot;) for the PoS tagging trials (see below).
As should be noted, that reduced set was enough to out-compete all but Factorie in terms of accuracy while using each tool's default parameter settings.&lt;/p&gt;
&lt;p&gt;In summary, with OpenNLP, Factorie, and CRF Suite you will need to work with their respective feature generation API (in Java, Scala, and Python, respectively) to model features beyond anything but newswire PoS tagging, phrase chunking, and some basic NER.
CRF Suite, similar to Factorie, has rich, pre-defined feature templates and can handle them, because unused indicator functions are dropped before the actual training starts, thereby keeping the initial feature weight matrix manageable.
In addition, it is the only tool in this review that can handle real-valued features.
Pre-training feature (space) &lt;em&gt;reduction&lt;/em&gt; is done by all tools except Wapiti.
Feature &lt;em&gt;compaction&lt;/em&gt; after training is done by all tools except OpenNLP, where I could not confirm if any compaction had occurred.
Wapiti provides a very powerful pattern language to define feature templates, including mixed state and transition label templates, which are otherwise only possible to generate with Factorie.
While Wapiti's template (pattern) language lends to a great flexibility when modeling features, it has to be used with care unless training times are not an issue due to the maximal (non-reduced) feature matrix used during the first training cycles.
At the end of the day, in terms of feature generation, once you learn how to use Wapiti's pattern &amp;quot;language&amp;quot;, it will be very efficient and spares you from writing code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="training-time"&gt;
&lt;h2&gt;Training time&lt;/h2&gt;
&lt;p&gt;Next, I looked into the training run-times to see how long each tool takes to create a PoS model.
To make an equal, but simple comparison, I used the &lt;a class="reference external" href="http://www.cnts.ua.ac.be/conll2000/chunking/"&gt;CoNLL 2000&lt;/a&gt; PoS tags to train the models using the default feature templates as discussed in the last section.
Both Factorie and OpenNLP needed slight, but simple modifications to the downloaded CoNLL files.
For Factorie, the reversed parenthesis tags in the CoNLL files had to be fixed.
The main observation here is that Factorie is not very helpful in terms of error messages to understand the problem;
It just throws some obscure exception at you.
This means you will have to figure out what went wrong when you get errors on your own.
OpenNLP's problem was simpler to identify: as documented, it expects one sentence per line, with token-tag pairs separated by underscores instead of the de facto standard OWPL format.&lt;/p&gt;
&lt;p&gt;To train the the taggers for &lt;em&gt;Part-of-Speech&lt;/em&gt;, the commands I used were:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# CRF Suite
crfsuite learn -m pos.model train.txt

# Factorie
java -cp factorie-1.1-SNAPSHOT-nlp-jar-with-dependencies.jar \
     cc.factorie.app.nlp.pos.ForwardPosTrainer -Xmx2g \
     --owpl --train-file=train.txt --test-file=empty.txt \
     --model=pos.model --save-model=true

# OpenNLP
opennlp POSTaggerTrainer -type maxent -model pos.model -data train.txt

# Wapiti
wapiti train -c -s -p patterns.txt train.txt pos.model
&lt;/pre&gt;
&lt;p&gt;The input data is provided in &lt;tt class="docutils literal"&gt;train.txt&lt;/tt&gt;, and the models are saved to &lt;tt class="docutils literal"&gt;pos.model&lt;/tt&gt;.
To measure the training times, I prefixed each command with &lt;tt class="docutils literal"&gt;time&lt;/tt&gt; and used &lt;a class="reference external" href="http://stackoverflow.com/a/556411"&gt;the sum of user+sys&lt;/a&gt; as the measured, total time it took each process to complete.
This means, the measurement includes all relevant CPU time (i.e., over all processor cores) that was consumed by the run.
This might seem unfair to multi-threaded code, which might have an actual runtime lower than the result.
However, this is entirely depended on your machine and its cores, so a direct &amp;quot;total CPU time&amp;quot; comparison seemed fair to me.
In my case, it also is a rather minor issue, because I anyway only have two (hyper-threaded) cores on my laptop.
To be fair, this most significantly only affects Wapiti runtime, so I report total time there, too.
There are other opinions about performance measurements, e.g., that one should only measure post-warm-up training time (minus JVM, input data reading, etc.) or only a single training cycle/iteration should be measured.
I think it is more practical to measure and compare whatever the &amp;quot;out-of-the-box&amp;quot; performance of each tool is.
Each training process is run thrice and the shortest measured time is the one I report here.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="28%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="28%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="40%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Training Time&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;CRF Suite&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;3.63 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;10m 22s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Factorie&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;0.34 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;02m 18s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;OpenNLP&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;???? M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;02m 03s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Wapiti&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;1.56 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;19m 06s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Wapiti-4*&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;1.56 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;10m 09s*&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;[* absolute runtime (&amp;quot;real&amp;quot;) in multi-threaded mode using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-t&lt;/span&gt; 4&lt;/tt&gt; switch to make full use of my hyper-threaded dual-core i5 processor]&lt;/p&gt;
&lt;p&gt;As mentioned earlier, with respect to initial feature template richness, CRF Suite and Factorie are taking the lead, with Wapiti and OpenNLP using less templates.
The models' feature sizes shown here are as reported by each tool &lt;em&gt;after&lt;/em&gt; all feature pruning steps (compaction;
For Wapiti that number is calculated from &amp;quot;initial features&amp;quot; minus &amp;quot;removed features&amp;quot;.)
While OpenNLP does not report final feature set sizes, I &lt;em&gt;assume&lt;/em&gt; it to be in a similar range (somewhere around a million features).
So in terms of feature compaction, Factorie has a clear edge over the competition.&lt;/p&gt;
&lt;p&gt;In terms of training times, Factorie and OpenNLP easily outpace both CRF Suite and Wapiti, but this should not be entirely surprising:
OpenNLP uses a simpler model (MEMM), so it clearly must be faster.
One noteworthy point is the training speed of Factorie - probably due to forward-only learning, it achieves similar training times for its CRF as OpenNLP on a MEMM.
On the other end, as expected, Wapiti is by far the most resource-hungry tagger.
As Wapiti's &lt;em&gt;learning&lt;/em&gt; procedure can easily make use of multiple CPU cores with a simple switch, it gained significantly in terms of absolute (&amp;quot;real&amp;quot;) training time from running in multi-threaded mode on my dual core machine, at least.
This is important, because while tagging is what is called &amp;quot;embarrassingly parallel&amp;quot;, learning/optimization is not.
Still, this means the top model training implementation is provided by Factorie, as OpenNLP has a much simpler model to train.
The remaining question in this respect will be if OpenNLP and Factorie can keep up with the accuracy of the other two CRFs and how fast they all perform their tagging.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="tagging-quality"&gt;
&lt;h2&gt;Tagging quality&lt;/h2&gt;
&lt;p&gt;This section will resolve the final remaining question:
Which implementation can provide you with the most efficient tagger?
I have a SATA-3-attached SSD drive where the data is read from (but a slow i5 CPU...) and took the CoNLL 2000 test set sequences to measure accuracy on the 47,377 tokens using the models I had trained in the last step.
So while my measurements do not include writing the tagged tokens back to the device and reading data should not be an issue with a SSD, my CPU isn't exactly &amp;quot;Speedy Gonzales...&amp;quot;
I timed each system while tagging 100 times those 47,377 tokens in a single row, read from one file (i.e., about 200,000 sentences or roughly 30,000 scientific abstracts) to make a fair comparison of each system's token throughput, marginalizing any warm-up &amp;quot;penalties&amp;quot;.&lt;/p&gt;
&lt;p&gt;To run the the taggers on the generated models, the commands I used were:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# CRF Suite
cat test.txt | pos.py &amp;gt; features.txt
crfsuite tag -m pos.model -tq features.txt

# Factorie
java -cp factorie-1.1-SNAPSHOT-nlp-jar-with-dependencies.jar \
     cc.factorie.app.nlp.pos.ForwardPosTester -Xmx2g \
     --owpl --model=pos.model --test-file=test.txt

# OpenNLP
opennlp POSTaggerEvaluator -model pos.model -data test.txt

# Wapiti
wapiti label -m pos.model -c test.txt &amp;gt; /dev/null
&lt;/pre&gt;
&lt;p&gt;And here are the results, in terms of error rate (&lt;tt class="docutils literal"&gt;1 - accuracy&lt;/tt&gt; over 47k tokens) and throughput (on 201k sentences with 4.7M tokens, as measured with &lt;tt class="docutils literal"&gt;time&lt;/tt&gt;, sys+user):&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="27%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="31%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="38%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Error Rate&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Tokens/Second&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;CRF Suite&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;3.00 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;30,000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Factorie&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;2.19 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;23,800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;OpenNLP&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;2.88 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;19,500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Wapiti&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;2.20 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;21,200&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note that for the CRF Suite, I pre-generated the features from the Python script.
If not, the numbers would look quite bad, as the whole Python-based feature extraction process using the &lt;tt class="docutils literal"&gt;pos.py&lt;/tt&gt; script is several times slower than the tagger itself!
(A good indicator that the underlying Python code could arguably use some polish...)
In terms of tagging throughput, rather to my astonishment, it seems fair to say that the libraries perform roughly equal and there are by and large no noteworthy differences.
It seems that CRF Suite is faster, but then we actually cheated, because we pre-generated the label features.
So at best there is a minor chance that the CRF Suite could be faster than the others, if it had a very fast feature extraction mechanism.
Another remark maybe is that Factorie automatically detects the available cores and equally distributes the tagging load among them
(Note that the throughput calculation is based on CPU time (sys+user), so in absolute numbers on my CPU, Factorie tags at about 47k tokens/second.)
While the tagging process is an &amp;quot;embarrassingly parallel&amp;quot; problem (you could just split up the input between as many cores as you have and run your tagger with GNU &lt;a class="reference external" href="https://www.gnu.org/software/parallel/"&gt;parallel&lt;/a&gt;), it is a nice little extra thrown into the mix.
Overall, in terms of raw tagging throughput, there might be no real &amp;quot;winner&amp;quot;.&lt;/p&gt;
&lt;p&gt;Regarding accuracy, you might want to know that a baseline tagger (using the majority PoS tag and tagging all unseen words as noun) already achieves an accuracy of 90% (or, an error rate of 10%) in standard PoS scenarios.
Probably due to the inability to use mixed features and because it does its feature compaction &lt;em&gt;prior&lt;/em&gt; to the training, the CRF Suite has the worst performance in terms of accuracy, closely followed by OpenNLP.
OpenNLP's shortcoming most likely can be attributed to its model choice, which only really starts to shine in cross-domain experiments.
So in terms of high quality tagging, at least if you have training data specifically for that domain, you will be better off with Factorie or Wapiti - not all that unexpected, given our feature modeling and model implementation insights.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusions"&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;If you only need to do standard PoS tagging, chunking, and/or NER, and don't mind the tagging quality or performance too much, just go with the tool in your favorite language: OpenNLP for Java developers, Factorie for Scala hackers, and Wapiti for C/C++ or Python programmers
(There is a &lt;a class="reference external" href="https://github.com/adsva/python-wapiti"&gt;Python wrapper&lt;/a&gt; for Wapiti available, if Python (for both 2 and 3) is your deal.)
The trade-offs are simply not big enough to make a huge (order-of-magnitude) difference.
But then, if that were your case, you probably would not have read until here...&lt;/p&gt;
&lt;p&gt;Because there is no conceivable advantage in terms of training times, tagging throughput, or accuracy, no support for mixed transition/state features, and, particularly, as the code seems unmaintained, I would not recommend the use of CRF Suite. The missing ability to scale training across computing cores is similar to OpenNLP, and another possible issue. Nonetheless, the tool (and the wrapper) has gotten some love from Mikhail Korobov, who has a commercial interest in (maintaining) it, too. As just hinted, there is a Python wrapper for CRF Suite &lt;a class="reference external" href="https://github.com/tpeng/python-crfsuite"&gt;available&lt;/a&gt;, too, much like Wapiti. So if you need to work with real-valued features (e.g., for adding word-vector representations) and/or do not mind the mentioned issues, CRF Suite might still be an interesting option for you. (Have a look at the discussion linked below.)&lt;/p&gt;
&lt;p&gt;At the end of the day, if you are interested in generating high-performance quality annotations while using mixture (state + transition) features, there really is only the choice between Wapiti and Factorie:
Foremost, they are the only two tools ready for the multi-core world of today.
An error rate reduction of about 25% on the &amp;quot;solved&amp;quot; PoS tagging problem at no cost in throughput is not to be underestimated.
Wapiti definitely is the most attractive tagger in terms of off-the-shelf usability: feature generation is simple with the patterns with no need for doing any coding, and the overall implementation complexity vs. usability balance is excellent.
The only tradeoff are possibly longer training times (in single-core mode or vs. Factorie), so you will need to develop and combine your feature templates with care.
Finally, if you also are looking for true flexibility and the full power of graphical models, or want to venture into higher order Markov space, I see no way around Factorie.
In this case, it might be worth living with the sparse documentation and having to study Scala source code.
However, the team around McCallum are very actively working on this library, and the documentation is certainly getting better and more extensive every other time I come back and a few months have passed.
Another advantage is the fact that Factorie offers a (largely undocumented) full NLP pipeline, starting from sentence segmentation all the way to dependency parsing.&lt;/p&gt;
&lt;p&gt;Nonetheless, if you'd ask me to declare a &lt;em&gt;global&lt;/em&gt; &amp;quot;winner&amp;quot;, unless you are a probabilistic programming or at least Scala expert, I'd say that honor right now still goes to &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt;.
But once Factorie fixes the mentioned issues and makes the library more accessible to a &amp;quot;general public&amp;quot;, that might change, as it certainly already takes the lead in terms of model flexibility and implementation performance.&lt;/p&gt;
&lt;p&gt;If you feel like &lt;strong&gt;discussing&lt;/strong&gt; what is written here, I've posted a link to this article on &lt;a class="reference external" href="http://www.reddit.com/r/LanguageTechnology/comments/2i7fe1/a_review_of_free_sparse_sequence_taggers_for_nlp/"&gt;Reddit&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Machine Learning"></category><category term="text mining"></category><category term="nlp"></category><category term="probabilistic programming"></category></entry><entry><title>MEDLINE Kung-Fu</title><link href="https://fnl.es/medline-kung-fu.html" rel="alternate"></link><published>2014-09-11T00:00:00+02:00</published><updated>2014-09-11T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2014-09-11:/medline-kung-fu.html</id><summary type="html">&lt;p&gt;If you are a computational linguist, data analyst, or bioinformatician working with biological text corpora (on medicine, neuroscience, molecular biology, etc.), you will rather sooner than later need access to &lt;a class="reference external" href="http://www.nlm.nih.gov/bsd/pmresources.html"&gt;MEDLINE&lt;/a&gt;.
Right now, the MEDLINE &lt;a class="reference external" href="http://www.nlm.nih.gov/pubs/factsheets/dif_med_pub.html"&gt;subset&lt;/a&gt; (&amp;quot;baseline&amp;quot;) of PubMed contains nearly &lt;a class="reference external" href="http://www.nlm.nih.gov/bsd/licensee/baselinestats.html"&gt;23 million records&lt;/a&gt;, all with titles, author names, etc …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you are a computational linguist, data analyst, or bioinformatician working with biological text corpora (on medicine, neuroscience, molecular biology, etc.), you will rather sooner than later need access to &lt;a class="reference external" href="http://www.nlm.nih.gov/bsd/pmresources.html"&gt;MEDLINE&lt;/a&gt;.
Right now, the MEDLINE &lt;a class="reference external" href="http://www.nlm.nih.gov/pubs/factsheets/dif_med_pub.html"&gt;subset&lt;/a&gt; (&amp;quot;baseline&amp;quot;) of PubMed contains nearly &lt;a class="reference external" href="http://www.nlm.nih.gov/bsd/licensee/baselinestats.html"&gt;23 million records&lt;/a&gt;, all with titles, author names, etc..
The majority of those records (officially called &lt;strong&gt;citations&lt;/strong&gt;) also have an abstract (on average about 5-8 sentences long).
This means you are looking at a significantly sized text collection with plenty of metadata (links, author names, MeSH terms, chemicals, etc.) you will need to handle if you want to make use of all this information your own data mining application.&lt;/p&gt;
&lt;p&gt;In my now about eight years of BioNLP (natural language processing for biology) work, I have not been able to locate a simple, up-to-date set of command-line tools to manage a MEDLINE DB mirror with all its metadata.
As I am an innately lazy guy, I have worked out a number of useful shell scripts I regularly use to work with MEDLINE data that I am documenting here.
To make this all work locally, and during a less lazy week, I wrote a tool called &lt;a class="reference external" href="https://pypi.python.org/pypi/medic"&gt;medic&lt;/a&gt; to create and manage an up-to-date (i.e., running daily, automatic updates) mirror of MEDLINE with the option of storing the citations either in a &lt;a class="reference external" href="http://www.postgresql.org/"&gt;Postgres&lt;/a&gt; or &lt;a class="reference external" href="http://sqlite.org/"&gt;SQLite&lt;/a&gt; database.&lt;/p&gt;
&lt;div class="section" id="synchronizing-the-medline-archives"&gt;
&lt;h2&gt;Synchronizing the MEDLINE archives&lt;/h2&gt;
&lt;p&gt;First you will most likely need to actually download the MEDLINE archives, given that your institute has a (&lt;a class="reference external" href="http://www.nlm.nih.gov/databases/journal.html"&gt;free&lt;/a&gt;) subscription to the archives.
If you are on a Mac, you are already provided with the necessary tools:
&lt;tt class="docutils literal"&gt;mount_ftp&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;rsync&lt;/tt&gt; should be installed on every Mac.
If you are on Linux, the &lt;tt class="docutils literal"&gt;curlftpfs&lt;/tt&gt; package simulates a file system for a FTP site accessed with with the cURL library, doing the same thing as &lt;tt class="docutils literal"&gt;mount_ftp&lt;/tt&gt; on a Mac.
In other words, we will mount the MEDLINE baseline and updates directories from the FTP site as a file system and then synchronize them with our local copy of each directory.
To do the synchronization step, we will be using &lt;tt class="docutils literal"&gt;rsync&lt;/tt&gt;.
On a GNU/Linux machine, you can use your package manager to install &lt;tt class="docutils literal"&gt;curlftpfs&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;rsync&lt;/tt&gt;, if they are not already present.
Once you have the packages installed, create a directory that acts as mount point, e.g., &lt;tt class="docutils literal"&gt;ftpfs&lt;/tt&gt;, as well as the &lt;tt class="docutils literal"&gt;baseline&lt;/tt&gt; directory, and the &lt;tt class="docutils literal"&gt;updates&lt;/tt&gt; directory.
Then, the relevant commands are:&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;To synchronize the baseline directory (replace &lt;tt class="docutils literal"&gt;curlftpfs&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;mount_ftp&lt;/tt&gt; on a Mac):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
curlftpfs ftp://ftp.nlm.nih.gov/nlmdata/.medleasebaseline/gz/ ftp_mount/
rsync -r -t -v --progress ftp_mount/* baseline/
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;To synchronize the updates directory (replace &lt;tt class="docutils literal"&gt;curlftpfs&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;mount_ftp&lt;/tt&gt; on a Mac):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
curlftpfs ftp://ftp.nlm.nih.gov/nlmdata/.medlease/gz/ ftp_mount/
rsync -r -t -v --progress ftp_mount/* updates/
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To unmount the FTP directories once rsync is done, you can use &lt;tt class="docutils literal"&gt;fusermount &lt;span class="pre"&gt;-u&lt;/span&gt; ftp_mount&lt;/tt&gt; on Linux and &lt;tt class="docutils literal"&gt;umount ftp_mount&lt;/tt&gt; on a Mac.
If you want to do the latter process regularly (MEDLINE sports daily updates), you might consider placing the update command series into a script and add it to your daily cron jobs.&lt;/p&gt;
&lt;p&gt;With this, you now have created the foundations to easily maintain a 24-hourly updated copy of all of MEDLINE on your site.
And because of using rsync, you do not have to worry about broken connections or communication errors - if the process breaks halfway, rsync will restart exactly where it left off.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="creating-a-local-medline-database-from-the-archives"&gt;
&lt;h2&gt;Creating a local MEDLINE database from the archives&lt;/h2&gt;
&lt;p&gt;Once the MEDLINE files are installed, it would be possible to parse (or grep...) the XML and manually extract whatever you need from them.
However, working this way will become cumbersome very fast, while placing the data into a well-structured database schema would help immensely.
To do this on the fly, I have created &lt;a class="reference external" href="https://pypi.python.org/pypi/medic"&gt;medic&lt;/a&gt;, a Python command-line tool to bootstrap and manage a local MEDLINE repository.
All you need to have installed to get this tool to work is &lt;a class="reference external" href="https://www.python.org/downloads/"&gt;Python&lt;/a&gt; (3.x); You can install &lt;a class="reference external" href="https://pypi.python.org/pypi/medic"&gt;medic&lt;/a&gt; with Python's own package manager: &lt;tt class="docutils literal"&gt;pip install medic&lt;/tt&gt; (possibly with the option &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--user&lt;/span&gt;&lt;/tt&gt; if you are not allowed to administer the machine you are working on).
Second, you need to decide which database you want to use for MEDLINE.
You can use either &lt;a class="reference external" href="http://www.postgresql.org/"&gt;Postgres&lt;/a&gt; or &lt;a class="reference external" href="http://sqlite.org/"&gt;SQLite&lt;/a&gt; as the back-end for medic (medic uses SQL Alchemy as its ORM, so in theory at least, it should be possible to use medic with other DBs, too.)&lt;/p&gt;
&lt;p&gt;As soon as you have the database installed and running (and CREATEd a DATABASE with UTF-8 text encoding, in the case of Postgres), you are ready to load the baseline files.
As loading all of MEDLINE through the ORM can be very slow for Postgres, it is better to parse the data into text files and then load them in one go:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;medic parse baseline/medline1?n*.xml.gz

&lt;span class="k"&gt;for&lt;/span&gt; table in citations abstracts authors chemicals databases descriptors &lt;span class="se"&gt;\&lt;/span&gt;
             identifiers keywords publication_types qualifiers sections&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;do&lt;/span&gt; psql medline -c &lt;span class="s2"&gt;&amp;quot;COPY &lt;/span&gt;&lt;span class="nv"&gt;$table&lt;/span&gt;&lt;span class="s2"&gt; FROM &amp;#39;`pwd`/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;table&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;.tab&amp;#39;;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you are loading the files into SQLite, you can load the data directly with &lt;tt class="docutils literal"&gt;medic insert&lt;/tt&gt;, although it will be considerably slower than the Postgres parse-and-dump method:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
medic --url sqlite:///MEDLINE.db insert baseline/medline1?n*.xml.gz
&lt;/pre&gt;
&lt;p&gt;Finally, to update the Postgres database to the latest state of MEDLINE, you can parse the &lt;tt class="docutils literal"&gt;updates&lt;/tt&gt; directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; file in updates/medline1?n*.xml.gz&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;do&lt;/span&gt; medic --update parse &lt;span class="nv"&gt;$file&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="c1"&gt;# NB: the above command created the file &amp;quot;delete.txt&amp;quot; (a list of PMIDs to delete)&lt;/span&gt;
  medic delete delete.txt

  &lt;span class="k"&gt;for&lt;/span&gt; table in citations abstracts authors chemicals databases descriptors &lt;span class="se"&gt;\&lt;/span&gt;
               identifiers keywords publication_types qualifiers sections&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;do&lt;/span&gt; psql medline -c &lt;span class="s2"&gt;&amp;quot;COPY &lt;/span&gt;&lt;span class="nv"&gt;$table&lt;/span&gt;&lt;span class="s2"&gt; FROM &amp;#39;`pwd`/&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;table&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;.tab&amp;#39;;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;done&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With SQLite you can again take the direct, but slower &lt;tt class="docutils literal"&gt;medic update&lt;/tt&gt; route:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
medic --url sqlite:///MEDLINE.db update updates/medline1?n*.xml.gz
&lt;/pre&gt;
&lt;p&gt;In theory, the simpler insert/update commands can be used for Postgres, too, but that is only recommended if you are loading citations in the thousands, not millions.
If you whish to cron-job this, you should only &lt;tt class="docutils literal"&gt;medic update&lt;/tt&gt; the latest file(s) - no need to parse-and-dump for a single file, not even for Postgres. In other words, make sure you are not working through all the files in the &lt;tt class="docutils literal"&gt;updates&lt;/tt&gt; directory every day...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="quickly-bootstrapping-a-subset-of-medline"&gt;
&lt;h2&gt;Quickly bootstrapping a subset of MEDLINE&lt;/h2&gt;
&lt;p&gt;Often I find myself only needing a tiny subset of MEDLINE that I am interested in analyzing.
In this case, you do not want to actually download, parse, and/or load all of PubMed into a heavy-weight Postgres DB, but rather have a small, single-file SQLite DB with the relevant citations.
To do this, medic provides an interface to effortlessly download and bootstrap a database right into the current directory.
Assuming you have the list of PubMed IDs (PMIDs) in a file called &lt;tt class="docutils literal"&gt;pmid_list.txt&lt;/tt&gt; (one ID per line),
and you want to bootstrap a SQLite DB file in the current directory called &lt;tt class="docutils literal"&gt;MEDLINE.db&lt;/tt&gt;,
you call medic like this:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
medic --url sqlite:///MEDLINE.db --pmid-lists insert pmid_list.txt
&lt;/pre&gt;
&lt;p&gt;With this, you have now quickly sampled that subset of MEDLINE citaitons relevant to your work, but still have them properly structured, stored in a single file, and easy to access as we will see next.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="extracting-medline-citations-with-medic"&gt;
&lt;h2&gt;Extracting MEDLINE citations with medic&lt;/h2&gt;
&lt;p&gt;Right now, medic has no interface to query the abstracts.
You can add a Postgres full-text index, but according to my own experience that is not particularly efficient if you have millions of records to index, as in the case of MEDLINE.
The right way would be to index the abstracts with a &amp;quot;real&amp;quot; search engine, for example, Lucene, but so far I have not gotten around to write an indexer for medic.
The best way right now is to query eUtils directly, using the standard PubMed query syntax, which is pretty powerful, anyways;
Note that eSearch queries to the eUtils API are capped, at most 100,000 IDs can be returned at once.
To fetch more, you need to page results with &lt;tt class="docutils literal"&gt;retmax&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;retmin&lt;/tt&gt;; Also by default (without setting &lt;tt class="docutils literal"&gt;retmax&lt;/tt&gt;) only the first 20 results are returned by eUtils:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
QUERY=&amp;quot;p53+AND+cancer&amp;quot;
URL=&amp;quot;http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi&amp;quot;

wget &amp;quot;$URL?db=PubMed&amp;amp;retmax=99&amp;amp;term=$QUERY&amp;quot; -O - 2&amp;gt; /dev/null \
| grep &amp;quot;^&amp;lt;Id&amp;gt;&amp;quot; \
| sed -E 's|&amp;lt;/?Id&amp;gt;||g' \
| cut -f3 \
&amp;gt; pmids.txt
&lt;/pre&gt;
&lt;p&gt;Again, if you do this often, you might want to stick this into a little script, for example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env bash&lt;/span&gt;
&lt;span class="c1"&gt;# for a given query (one term per argument), retrieve (up to retmax) matching PMIDs&lt;/span&gt;

&lt;span class="nv"&gt;QUERY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$@&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; tr &lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt; +&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="nv"&gt;URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi&amp;quot;&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$QUERY&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;

wget &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$URL&lt;/span&gt;&lt;span class="s2"&gt;?db=PubMed&amp;amp;retmax=99999&amp;amp;term=&lt;/span&gt;&lt;span class="nv"&gt;$QUERY&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -O - &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt; /dev/null &lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;^&amp;lt;Id&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; sed -E &lt;span class="s1"&gt;&amp;#39;s|&amp;lt;/?Id&amp;gt;||g&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="p"&gt;|&lt;/span&gt; cut -f3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The query argument to this script now can contain space characters which are replaced with &amp;quot;+&amp;quot; characters, e.g., &amp;quot;&lt;tt class="docutils literal"&gt;search_pubmed.sh p53 AND cancer&lt;/tt&gt;&amp;quot; produces the same output as before (with far more PMIDs, however, so please do not try this particular query too often; and using &lt;tt class="docutils literal"&gt;search_pubmed.sh&lt;/tt&gt; as the name of the above script).&lt;/p&gt;
&lt;p&gt;Given one or a list of PMIDs, however, medic allows you to quickly pull the citations in a number of formats:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;medline&lt;/tt&gt;: write the citations, one per file, in the official MEDLINE format.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;tiab&lt;/tt&gt;: only put the title and abstract, one pair per citations, into the files.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;html&lt;/tt&gt;: write all citations into one large HTML file (&amp;quot;corpus&amp;quot;).&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;tsv&lt;/tt&gt;: write the PMID, title, and abstract into one large TSV file, one citation per line.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example: &lt;tt class="docutils literal"&gt;medic &lt;span class="pre"&gt;--format&lt;/span&gt; tiab &lt;span class="pre"&gt;--pmid-lists&lt;/span&gt; selected_pmids.txt&lt;/tt&gt;, where &lt;tt class="docutils literal"&gt;selected_pmids.txt&lt;/tt&gt; is a file with one PMID per line, will create one file per citation, named &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;&amp;lt;PMID&amp;gt;.txt&lt;/span&gt;&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;If you do not care too much about the actual IDs and just need a few random citations to work with, here is an easy way to select 999 random PMIDs from MEDLINE; on a Mac or FreeBSD machine:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
jot 999 100000 25000000 &amp;gt; pmids.rnd_test.txt
&lt;/pre&gt;
&lt;p&gt;And when running a Linux or Cygwin OS:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
shuf -i 100000-25000000 | head -999 &amp;gt; pmids.rnd_test.txt
&lt;/pre&gt;
&lt;p&gt;However, this approach is a bit like cheating: if the PMID does not exist, you have a non-existing ID in your list.
From a mathematical perspective, if the PMIDs are not evenly distributed over the range you are drawing integers from, you will not have the &lt;em&gt;perfect&lt;/em&gt; random sample.
Ideally, you should select random IDs from your collection, not the whole numeric range.&lt;/p&gt;
&lt;p&gt;Note that I chose 999 PMIDs not just by chance - SQLite has 999 set as a hard limit for the number of arguments for a &amp;quot;prepared statement&amp;quot;.
This means that if you want to fetch more than 999 PMIDs from a SQLite database, you will have to do that in several rounds.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="converting-dois-to-pmids"&gt;
&lt;h2&gt;Converting DOIs to PMIDs&lt;/h2&gt;
&lt;p&gt;To finish, here is a nifty little command-line to convert a list of DOIs into a list of PMIDs by using the NCBI eUtils web service:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; doi in &lt;span class="sb"&gt;`&lt;/span&gt;cat dois.txt&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="nv"&gt;pmid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;wget &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$URL&lt;/span&gt;&lt;span class="s2"&gt;?db=PubMed&amp;amp;retmode=xml&amp;amp;term=&lt;/span&gt;&lt;span class="nv"&gt;$doi&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -O - &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt; /dev/null &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;&amp;lt;Id&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="p"&gt;|&lt;/span&gt; sed -E &lt;span class="s1"&gt;&amp;#39;s|&amp;lt;/?Id&amp;gt;||g&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="p"&gt;|&lt;/span&gt; cut -f3&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$doi&lt;/span&gt; &lt;span class="nv"&gt;$pmid&lt;/span&gt; &amp;gt;&amp;gt; doi2pmid.txt&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you use this much, you might even want to put that into a little script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env sh&lt;/span&gt;
&lt;span class="c1"&gt;# for a argument list of DOIs, print each DOI and matching PubMed ID&lt;/span&gt;

&lt;span class="nv"&gt;URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; doi in &lt;span class="nv"&gt;$@&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
  &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$doi&lt;/span&gt; &lt;span class="sb"&gt;`&lt;/span&gt;wget &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$URL&lt;/span&gt;&lt;span class="s2"&gt;?db=PubMed&amp;amp;retmode=xml&amp;amp;term=&lt;/span&gt;&lt;span class="nv"&gt;$doi&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -O - &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt; /dev/null &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;&amp;lt;Id&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="p"&gt;|&lt;/span&gt; sed -E &lt;span class="s1"&gt;&amp;#39;s|&amp;lt;/?Id&amp;gt;||g&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="p"&gt;|&lt;/span&gt; cut -f3&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Alternatively, you might want the script to take a file with DOIs as input.
But that is a trivial case to handle with this script: just use the file as an argument of &lt;tt class="docutils literal"&gt;xargs&lt;/tt&gt; and pipe the result into this script.&lt;/p&gt;
&lt;p&gt;Unluckily enough, converting PMIDs to DOIs is a lot more trickier: First, the download of MEDLINE from the FTP site does not contain all PubMed mappings of PMIDs to DOIs that the NLM has access to (why not is a mystery to me...). Second, there are still scores of PMIDs that even the NLM did not receive the correct DOI mapping from the publisher. So overall, no matter what you do, there will be holes if trying to go the direct PMID-to-DOI way. The best method right now is probably to query with the title and authors and see if you find an exact, unique match to a DOI on &lt;a class="reference external" href="http://www.crossref.org/"&gt;CrossRef&lt;/a&gt;, but that API is commercial and you need to pay for any serious query volumes.&lt;/p&gt;
&lt;p&gt;Overall, this collection of tools should give you everything you need to quickly and efficiently work with MEDLINE's PubMed citations. If you have not done so already, you can check out the &amp;quot;full capabilities&amp;quot; of &lt;a class="reference external" href="https://pypi.python.org/pypi/medic"&gt;medic&lt;/a&gt; and decide for yourself if my approach is suitable for you, too.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Programming"></category><category term="text mining"></category><category term="bionlp"></category><category term="pubmed"></category></entry><entry><title>An Introduction to Statistical Text Mining</title><link href="https://fnl.es/an-introduction-to-statistical-text-mining.html" rel="alternate"></link><published>2014-07-07T00:00:00+02:00</published><updated>2014-07-07T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2014-07-07:/an-introduction-to-statistical-text-mining.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Update&lt;/em&gt; 2015-07-24: Please check out the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-from-bayes-rule-to-de"&gt;latest slides&lt;/a&gt; for this course, which now includes a quick introduction to dependency parsing, a more terse start, and several corrections and improvements all over.&lt;/p&gt;
&lt;p&gt;Last week we had a really great time at the first hands-on text mining workshop in the context of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Update&lt;/em&gt; 2015-07-24: Please check out the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-from-bayes-rule-to-de"&gt;latest slides&lt;/a&gt; for this course, which now includes a quick introduction to dependency parsing, a more terse start, and several corrections and improvements all over.&lt;/p&gt;
&lt;p&gt;Last week we had a really great time at the first hands-on text mining workshop in the context of the &lt;a class="reference external" href="http://www.dia.fi.upm.es/ASDM"&gt;Advanced Statistics and Data Mining&lt;/a&gt; Summer School. This one-week course is an introduction to text mining from the &amp;quot;bottom up&amp;quot; to a bunch of motivated summer students, with the practical parts based on Python and the &lt;a class="reference external" href="http://www.nltk.org/"&gt;NLTK&lt;/a&gt;. The presentation was part of the 9th iteration of the Summer School that is located in the sunniest capital of Europe: Madrid (well, &lt;em&gt;ex aequo&lt;/em&gt; with Athens, at least). In its context, I presented 15 hours worth of practical and theoretical background on machine learning for text mining to fourteen participants from all over the world. With this post I am sharing the slides and tutorial files (&lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt; Notebooks) with the world for free (see the Creative Commons &lt;a class="reference external" href="https://creativecommons.org/licenses/by-sa/3.0/"&gt;BY-SA&lt;/a&gt; licensing details). While much of the material should speak for itself, it might not &amp;quot;save&amp;quot; you from visiting a text mining class (maybe mine, next summer?).&lt;/p&gt;
&lt;div class="section" id="overview"&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;The theory mostly focuses on explaining the methods and statistics behind machine learning &lt;em&gt;for text mining&lt;/em&gt; - without requiring a particular background other than sharp high-school maths. The practicals on the other hand make use of Python, particularly online &lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt; Notebooks. Therefore, prior contact with Python would be useful to profit the most from the practicals, but it is not required to follow the course. IPython's Notebooks act very much like MATLAB Notebooks, but run online in any modern browser, are based on open source software that requires no license fee, and provide facilities to place documentation and mathematical formulas between the code snippets, evaluation results, and graphical plots. Therefore, these notebooks serve as take home material for the students to study and play around with, even well after the course. To illustrate the power of IPython, learning how to work in this environment immediately enables you to run distributed, high-performance computations &amp;quot;off the shelf&amp;quot; (e.g., via &lt;a class="reference external" href="http://star.mit.edu/cluster/"&gt;StarCluster&lt;/a&gt; on Amazon EC2 Clusters).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-1-introduction"&gt;
&lt;h2&gt;Day 1 - Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/statistical-text-mining-introduction-florian-leitner"&gt;Lecture 1&lt;/a&gt; starts with a light-weight introduction to text mining, natural language understanding and generation, and a statistics approach to artificial intelligence in general. It provides a taste of both what is to come in the course and what might be of interest to look into afterwards. It closes with a brief reprise of elementary Bayesian statistics and conditional probability, using the Monty Hall problem as a practical example of &lt;em&gt;Bayes' Rule&lt;/em&gt;. The &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/IPython%20Notebook%20Introduction.ipynb"&gt;practical #1&lt;/a&gt; introduces the use of &lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt; and its online Notebook, as well as some aspects of &lt;a class="reference external" href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt; (particularly, contrasting NumPy/SciPy to &lt;a class="reference external" href="http://www.r-project.org/"&gt;R&lt;/a&gt;). The most important aspects of the &lt;strong&gt;Natural Language ToolKit&lt;/strong&gt; (&lt;a class="reference external" href="http://www.nltk.org/"&gt;NLTK&lt;/a&gt;) Python library are presented during the &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Introduction%20to%20NumPy%20and%20NLTK.ipynb"&gt;second part&lt;/a&gt; of the practical; The NTLK is frequently used during the course. As an exercise to become familiar with Python and the NLTK, participants are encouraged to implement a function to let two NLTK chat-bots converse between each other.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-2-language-modeling"&gt;
&lt;h2&gt;Day 2 - Language Modeling&lt;/h2&gt;
&lt;p&gt;On the second day, &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-25-language-modeling-florian-leitner"&gt;lecture 2&lt;/a&gt; introduces the chain rule of conditional probability and builds on that to take you from the &lt;em&gt;Markov property&lt;/em&gt; over &lt;em&gt;language modeling&lt;/em&gt; to &lt;em&gt;smoothing techniques&lt;/em&gt;. In class, you learn how to operate on n-gram frequency tables and we work out the conditional probability distributions of bi- and trigram models. This should ensure  students obtain well-grounded foundations of statistical language processing. After a &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Building%20a%20Language%20Model%20in%207%20Steps.ipynb"&gt;quick overview&lt;/a&gt;, the accompanying &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Language%20Modelling%20with%20NLTK.ipynb"&gt;exercises #2&lt;/a&gt; demonstrate how to build language models with the NLTK and participants are tasked with generating their own models using advanced smoothing techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-3-string-processing"&gt;
&lt;h2&gt;Day 3 - String Processing&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-35-string-processing"&gt;third day lecture&lt;/a&gt; focuses on &lt;em&gt;string processing&lt;/em&gt; and the algorithmic aspects of text mining. The slides contain an overview of state machines, like regular expressions, PATRICIA tries, and &lt;em&gt;Minimal Acyclic Deterministic Finite [State] Automata&lt;/em&gt; (MADFA). Particular attention is payed to string metrics and similarity measures. The last theoretical part is a thorough introduction to &lt;em&gt;Locality Sensitive Hashing&lt;/em&gt; (LSH), and its use as a tool to efficiently cluster millions of documents or implement approximate string matching over very large-scale term dictionaries is discussed. During the &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Locality%20Sensitive%20Hashing.ipynb"&gt;3rd exercises&lt;/a&gt; students apply LSH to a toy problem: improving the speed of &lt;a class="reference external" href="http://norvig.com/spell-correct.html"&gt;Peter Norvig&lt;/a&gt;-style &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Spelling%20Correction%20using%20LSH.ipynb"&gt;spelling correction&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-4-text-classification"&gt;
&lt;h2&gt;Day 4 - Text Classification&lt;/h2&gt;
&lt;p&gt;After the more algorithmic topics, the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-45-text-classification"&gt;lecture of day four&lt;/a&gt; takes the participants into the realm of &amp;quot;real&amp;quot; machine learning. For starters, ways to calculate syntactic and semantic document similarity are presented, culminating in &lt;em&gt;Latent Semantic Analysis&lt;/em&gt;. The slides introduce the Naive Bayes classifier as an example to prepare the audience for the &lt;em&gt;Maximum Entropy classifier&lt;/em&gt; (Multinomial Logistic Regression), which forms the central topic of this session. To &amp;quot;cool down&amp;quot; a bit after a math-heavy section, &lt;em&gt;sentiment analysis&lt;/em&gt; is discussed as a popular domain for text classification. Finally, the important topic of evaluating set-based classifier performance via Accuracy, F-measure, and MCC Score are discussed. The &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Twitter%20Sentiment%20Analysis.ipynb"&gt;fourth practical&lt;/a&gt; then walks the participants through a Maximum Entropy sentiment classifier and encourages the audience to improve its performance by designing better features and making clever use of all the gained knowledge in the course so far.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-5-graphical-models"&gt;
&lt;h2&gt;Day 5 - Graphical Models&lt;/h2&gt;
&lt;p&gt;On the final day, &lt;em&gt;probabilistic graphical models&lt;/em&gt; dominate the lecture. With the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-55-information-extraction"&gt;slides for day #5&lt;/a&gt;, first the main differences between Bayesian Networks and Markov Random Fields are introduced. Connected to yesterday's text classification topics, &lt;em&gt;Latent Dirichlet Allocation&lt;/em&gt; is presented as the first graphical model. Then, for the remainder of the talk, we move into &lt;em&gt;dynamic&lt;/em&gt; (temporal) models and their applications. We revisit the Markov chain and go from &lt;em&gt;Hidden Markov Models&lt;/em&gt; all the way to &lt;em&gt;Conditional Random Fields&lt;/em&gt;. To gear participants up for practical text mining, we look into actual applications of the presented models, for example, (semantic) &lt;em&gt;Word Representations&lt;/em&gt; or &lt;em&gt;Named Entity Recognition&lt;/em&gt;. The participants are introduced to the prospect of using the assigned labels for more advanced tasks. For example, &lt;em&gt;Relationship Extraction&lt;/em&gt; now becomes feasible by combining the shallow parse output as features for classifiers presented in other sessions of the Advanced Statistics and Data Mining Summer School. The rank-based performance measures of ROC and PR curves are discussed as way to evaluate the labeled results. During the &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Shallow%20Parsing%20with%20NLTK.ipynb"&gt;last practical&lt;/a&gt;, implementing a &lt;strong&gt;shallow parser&lt;/strong&gt; using NLTK and the &lt;a class="reference external" href="http://nlp.stanford.edu/software/tagger.shtml"&gt;Stanford Taggers&lt;/a&gt; is demonstrated in class. Overall, the practicals should provide the students with the basic software tools to embark on their own text mining adventures right after the course.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="remarks"&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;You can &lt;a class="reference external" href="mailto:florian.leitner&amp;#64;gmail.com"&gt;contact me&lt;/a&gt; if you would like to discuss having this tutorial in the context of your own venue or have questions and comments about the slides' content. Other than that, I hope to have awakened your interest in statistical text mining and/or to have provided you with useful material, both as a student or teacher.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Machine Learning"></category><category term="text mining"></category><category term="python"></category><category term="nltk"></category></entry><entry><title>Getting started with a "virtual" Go environment</title><link href="https://fnl.es/getting-started-with-a-virtual-go-environment.html" rel="alternate"></link><published>2013-11-29T00:00:00+01:00</published><updated>2013-11-29T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2013-11-29:/getting-started-with-a-virtual-go-environment.html</id><summary type="html">&lt;p&gt;Given how easy it is to write highly concurrent code in Go (aka &amp;quot;&lt;a class="reference external" href="http://golang.org"&gt;golang&lt;/a&gt;&amp;quot;), it is probably worth learning this language.
Personally, I believe Go is not yet mature enough for &amp;quot;production&amp;quot; projects other than servers maybe (and I am sure there are people who will not agree with my …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Given how easy it is to write highly concurrent code in Go (aka &amp;quot;&lt;a class="reference external" href="http://golang.org"&gt;golang&lt;/a&gt;&amp;quot;), it is probably worth learning this language.
Personally, I believe Go is not yet mature enough for &amp;quot;production&amp;quot; projects other than servers maybe (and I am sure there are people who will not agree with my belief...).
But for doing science, juggling huge ammounts of data (no, I will not say the &amp;quot;B&amp;quot;-word...), and your typical scripting pipelines, Go is really great.
On top of that, Go is easy to learn because it has a simple syntax and a very &amp;quot;bare bones&amp;quot; approach in so many aspects.
So if you feel like giving it a try, here is a quick recipe to bootstrap a Go development environment within moments:&lt;/p&gt;
&lt;p&gt;First, &lt;a class="reference external" href="http://golang.org/doc/install"&gt;install Go&lt;/a&gt; itself using your package manager; For example, on OSX, you might use &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt;, and on Ubuntu or Debian you'd probably use &lt;tt class="docutils literal"&gt;apt&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew install go --cross-compile-common
apt-get install golang
...
&lt;/pre&gt;
&lt;p&gt;Notice that if you are on a LTS version of Ubuntu (Precise/12.04 right now) or Debian, you might actually want to install a more &lt;a class="reference external" href="http://code.google.com/p/go/downloads/list"&gt;up-to-date binary&lt;/a&gt;.
Next, you need to set the global environment variable &lt;tt class="docutils literal"&gt;GOROOT&lt;/tt&gt;.
It has to point to the directory you installed Go in;
When using Homebrew on OSX, this can be tricky, so here is a snippet that will create the correct path for you:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
export GOROOT=$(brew --prefix)/Cellar/go/$(go \
version | cut -f3 -d' ' | sed 's/go//')/libexec
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Protip&lt;/em&gt;: If during the next steps you see a lot of errors of the form:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
imports PKGNAME: unrecognized import path &amp;quot;PATH&amp;quot;
&lt;/pre&gt;
&lt;p&gt;They probably occur while running &lt;tt class="docutils literal"&gt;go get SOME_PKG&lt;/tt&gt;, and it most likely just means that your &lt;tt class="docutils literal"&gt;GOROOT&lt;/tt&gt; is wrong or unset.
Finally, you want to quickly bootstrap a &amp;quot;virtual&amp;quot; development environment for Go.
For this, I use a simple shell script (that I call &lt;tt class="docutils literal"&gt;goinit&lt;/tt&gt;) to set up the directory structure and an &amp;quot;&lt;tt class="docutils literal"&gt;activate&lt;/tt&gt;-able&amp;quot; environment [UPDATE 2013-12-17: additional go get package lines that are extremely useful for development]&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="c1"&gt;# setup a directory structure for programming in go&lt;/span&gt;
&lt;span class="nv"&gt;VCS_HUB&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;github.com/username
&lt;span class="nv"&gt;PROJECT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;basename &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;

mkdir -p &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
mkdir -p &lt;span class="s2"&gt;&amp;quot;src/&lt;/span&gt;&lt;span class="nv"&gt;$VCS_HUB&lt;/span&gt;&lt;span class="s2"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;$PROJECT&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
mkdir bin
mkdir pkg
&lt;span class="nv"&gt;GOPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
go get github.com/nsf/gocode
go get github.com/jstemmer/gotags
go get github.com/davecheney/godoc2md
go get github.com/grobins2/gobrew
cat &lt;span class="s"&gt;&amp;lt;&amp;lt; ACTIVATE &amp;gt; bin/activate&lt;/span&gt;
&lt;span class="s"&gt;export GOPATH=&amp;quot;`pwd`&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;export PATH=&amp;quot;\$GOPATH/bin:\$PATH&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;export PS1=&amp;quot;($PROJECT)\$PS1&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;ACTIVATE&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Put this script somewhere on you &lt;tt class="docutils literal"&gt;PATH&lt;/tt&gt;, replace with your own GitHub &lt;tt class="docutils literal"&gt;username&lt;/tt&gt; (or any other version control system you use), make it executable (&lt;tt class="docutils literal"&gt;chmod 755 goinit&lt;/tt&gt;), and ensure you have the &lt;tt class="docutils literal"&gt;GOROOT&lt;/tt&gt; set in your environment and &lt;tt class="docutils literal"&gt;goinit&lt;/tt&gt; on your path.
For example, let's assume you want to start with the Go Tour to learn about the language itself (&lt;em&gt;highly&lt;/em&gt; recommendable!);
With this setup, bootstrapping your next Go project (simply called &amp;quot;&lt;tt class="docutils literal"&gt;project&lt;/tt&gt;&amp;quot; here) now is as simple as:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
goinit path/to/project
cd path/to/project
source bin/activate

# if you are new to Go, you might want to try this:
go get code.google.com/p/go-tour/gotour
gotour
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;gotour&lt;/tt&gt; should have opened in your browser.
Happy Go coding!&lt;/p&gt;
</content><category term="Programming"></category><category term="golang"></category><category term="posix"></category></entry><entry><title>Concurrent Node.js</title><link href="https://fnl.es/concurrent-nodejs.html" rel="alternate"></link><published>2013-10-08T00:00:00+02:00</published><updated>2013-10-08T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2013-10-08:/concurrent-nodejs.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, a &lt;a class="reference external" href="https://twitter.com/SoftActiva"&gt;colleague&lt;/a&gt; of mine asked me to introduce the most important concepts of &lt;a class="reference external" href="http://nodejs.org/"&gt;Node&lt;/a&gt; programming to a flock of interested people in our &lt;a class="reference external" href="http://www.cnio.es/es/grupos/plantillas/presentacion.asp?grupo=50004294"&gt;research group&lt;/a&gt;.
Initially, I declined, considering the &lt;a class="reference external" href="http://howtonode.org/"&gt;vast&lt;/a&gt; &lt;a class="reference external" href="http://docs.nodejitsu.com/"&gt;number&lt;/a&gt; of &lt;a class="reference external" href="http://www.nodebeginner.org/"&gt;tutorials&lt;/a&gt; and &lt;a class="reference external" href="https://duckduckgo.com/?q=node.js+book"&gt;books&lt;/a&gt;, but then thought it might be quite an interesting challenge:
Is there …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, a &lt;a class="reference external" href="https://twitter.com/SoftActiva"&gt;colleague&lt;/a&gt; of mine asked me to introduce the most important concepts of &lt;a class="reference external" href="http://nodejs.org/"&gt;Node&lt;/a&gt; programming to a flock of interested people in our &lt;a class="reference external" href="http://www.cnio.es/es/grupos/plantillas/presentacion.asp?grupo=50004294"&gt;research group&lt;/a&gt;.
Initially, I declined, considering the &lt;a class="reference external" href="http://howtonode.org/"&gt;vast&lt;/a&gt; &lt;a class="reference external" href="http://docs.nodejitsu.com/"&gt;number&lt;/a&gt; of &lt;a class="reference external" href="http://www.nodebeginner.org/"&gt;tutorials&lt;/a&gt; and &lt;a class="reference external" href="https://duckduckgo.com/?q=node.js+book"&gt;books&lt;/a&gt;, but then thought it might be quite an interesting challenge:
Is there any aspect of Node development that is not easily understood by Node beginners and that is poorly covered by the existing posts?
Taking this into account, my main goals for this tutorial are:
Which part of developing Node programs is the hardest to grasp for programmers proficient in imperative languages (Java, JavaScript, Objective-C, PHP, Python, Ruby, etc.)?
In my opinion, the biggest issue is writing asynchronous, concurrent Node applications.
At the same time, this seems to be the least covered aspect of all existing introductory tutorials.
Last, I wanted to present relevant issues &lt;em&gt;without&lt;/em&gt; getting too far ahead of the status quo (e.g., &lt;a class="reference external" href="http://wiki.ecmascript.org/doku.php?id=harmony:generators"&gt;generators&lt;/a&gt;, which are still considerd experimental right now) or anything else that you would not want to use in production.&lt;/p&gt;
&lt;p&gt;Overall, this tutorial should teach you to &lt;em&gt;code&lt;/em&gt; &lt;a class="reference external" href="http://www.reactivemanifesto.org/"&gt;reactive&lt;/a&gt; programs that are both maintainable and concise.
Nonetheless, I will assume you have a basic idea about Node and JavaScript, so I do expect you have read at least one of those excellent &lt;a class="reference external" href="https://github.com/maxogden/art-of-node#the-art-of-node"&gt;introductions&lt;/a&gt;.
In particular, you should have a basic idea of how Node uses &lt;a class="reference external" href="http://docs.nodejitsu.com/articles/getting-started/control-flow/what-are-callbacks"&gt;callbacks&lt;/a&gt; to handle &lt;a class="reference external" href="http://docs.nodejitsu.com/articles/getting-started/control-flow/what-are-event-emitters"&gt;events&lt;/a&gt;, given that they are the core concept of Node apps.
And although I will cover this aspect in more detail, Nodejitsu also has a short section on writing &lt;a class="reference external" href="http://docs.nodejitsu.com/articles/getting-started/control-flow/how-to-write-asynchronous-code"&gt;asynchronous&lt;/a&gt; JavaScript and spawning &lt;a class="reference external" href="http://docs.nodejitsu.com/articles/child-processes/how-to-spawn-a-child-process"&gt;child processes&lt;/a&gt; in Node.&lt;/p&gt;
&lt;p&gt;To make this discussion more valuable for real-world projects, I restrict myself to use “battle-tested” packages;
After checking with &lt;a class="reference external" href="https://npmjs.org/"&gt;npm&lt;/a&gt; and GitHub, I decided to only use &lt;strong&gt;async&lt;/strong&gt; and &lt;strong&gt;Q&lt;/strong&gt;, plus some &lt;strong&gt;underscore&lt;/strong&gt; magic (only &lt;a class="reference external" href="http://underscorejs.org/#partial"&gt;partial&lt;/a&gt; function application).
The tutorial should explain to the reader how, using a limited set of tools, a Node developer can write highly asynchronous Node apps without ending up in &amp;quot;concurrency hell&amp;quot;.
This introduction will take the reader familiar with Node's basic callback mechanism (&lt;a class="reference external" href="http://www.webdesignerdepot.com/2012/09/jquery-the-good-the-bad-and-the-ugly/"&gt;evil tongues&lt;/a&gt; would say &amp;quot;callback spaghetti&amp;quot;) via asynchronous control structures to a concurrent, functional programming style that uses &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Futures_and_promises"&gt;promises&lt;/a&gt; - also know as &lt;strong&gt;futures&lt;/strong&gt; - to delegate the logical flow from the JavaScript code to the V8 engine.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-quick-refresher"&gt;
&lt;h2&gt;A Quick Refresher&lt;/h2&gt;
&lt;p&gt;Lets look at a static HTTP file server, using the basic Node modules only.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;url&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;path&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;fs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;fs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;

&lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;createServer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;request&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;uri&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;url&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;request&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;url&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;pathname&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;path&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;cwd&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nx"&gt;uri&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;path&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nx"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;404&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Content-Type&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;text/plain&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;404 Not Found\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;statSync&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;isDirectory&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="nx"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/index.html&amp;#39;&lt;/span&gt;

      &lt;span class="nx"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;readFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;binary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Content-Type&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;text/plain&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;binary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;}).&lt;/span&gt;&lt;span class="nx"&gt;listen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;parseInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;port&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;file server running at http://localhost:&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This highlights basic Node concepts; Upon receiving a request, the server makes an asynchronous check via the OS to ensure the file exists.
The asynchronicity is indicated by the callback that gets sent to &lt;tt class="docutils literal"&gt;path.exists()&lt;/tt&gt;.
Then, again asynchronously (note the callback sent into &lt;tt class="docutils literal"&gt;fs.readFile()&lt;/tt&gt;), Node reads the file, and if this job completes successfully, sends the file (data) to the response handler.
Because both checks are done asynchronously, this leaves the server process free to handle other requests while the OS is doing the file lookups (i.e., &amp;quot;non-blocking&amp;quot;).
However, this demonstrates a problem with this callback coding style: the functions are stacked one on top of another, making this code quite uncomfortable to read and maintain.&lt;/p&gt;
&lt;p&gt;Before we begin, it is also worth to refresh the &lt;strong&gt;six commandments&lt;/strong&gt; (taken from Caolan McMahon's excellent post on Node &lt;a class="reference external" href="http://caolanmcmahon.com/posts/nodejs_style_and_structure/"&gt;style and structure&lt;/a&gt;) you should follow when coding any concurrent program:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Never mix sync and async style: return a value &lt;em&gt;or&lt;/em&gt; a callback, but not either.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Return&lt;/em&gt; on a callback: write &lt;tt class="docutils literal"&gt;return callback(null, result)&lt;/tt&gt; to prevent execution continuing beyond the CB.&lt;/li&gt;
&lt;li&gt;Always check errors in callbacks: make sure the CB has a way to deal with error states, as in &lt;tt class="docutils literal"&gt;if (err) { ... }&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Avoid mutable state - it leads to &amp;quot;debugging hell&amp;quot; in concurrent programs (e.g., &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Heisenbug"&gt;Heisenbugs&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Only use try, throw &amp;amp; catch within imperative code sections (you cannot throw &amp;quot;across&amp;quot; a CB).&lt;/li&gt;
&lt;li&gt;Write tiny functions: maybe 3 statements, and no more than 5.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="multiprocessing-in-node-js"&gt;
&lt;h2&gt;Multiprocessing in Node.js&lt;/h2&gt;
&lt;p&gt;Node is process-oriented, i.e., unlike some languages where you create multiple threads in your process, in node you need to &amp;quot;open&amp;quot; multiple OS processes (using &lt;tt class="docutils literal"&gt;popen(3)&lt;/tt&gt;).
The default tool to run multiple processes in Node is the &lt;a class="reference external" href="http://nodejs.org/api/child_process.html"&gt;child_process&lt;/a&gt; module, and, as an experimental add-on since Node v9, &lt;a class="reference external" href="http://nodejs.org/api/cluster.html"&gt;cluster&lt;/a&gt; can run multiple Node processes in parallel that share the same port or socket.
When forking/spawning/executing child process, be aware that segmentation faults and other nasty errors in a child process have the potential to bring down your whole stack.
In general, there are three calls that are relevant for running parallel processes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://nodejs.org/api/child_process.html#child_process_child_process_spawn_command_args_options"&gt;spawn&lt;/a&gt; provides a &lt;strong&gt;streaming&lt;/strong&gt; process that you communicate with through Unix pipes. Think of it as in shell syntax &amp;quot;&lt;tt class="docutils literal"&gt;process &amp;lt; instream &amp;gt; outstream &amp;amp;&lt;/tt&gt;&amp;quot;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://nodejs.org/api/child_process.html#child_process_child_process_exec_command_options_callback"&gt;exec&lt;/a&gt; runs a single shell &lt;strong&gt;command&lt;/strong&gt; and returns the exit status of the computation. Think of it as in shell syntax &amp;quot;&lt;tt class="docutils literal"&gt;exec input; echo $?&lt;/tt&gt;&amp;quot;. Note that while exec does have an output buffer, it is tiny and you should use the streaming interface of spawn if you expect to receive output from the process you are running.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://nodejs.org/api/child_process.html#child_process_child_process_fork_modulepath_args_options"&gt;fork&lt;/a&gt; runs a &lt;strong&gt;worker&lt;/strong&gt; script you can communicate with via a &lt;em&gt;channel&lt;/em&gt;. Think of it as a bidirectional version of the shell pipe &amp;quot;&lt;tt class="docutils literal"&gt;server | worker &amp;amp;&lt;/tt&gt;&amp;quot;. Fork is also useful to isolate blocking calls from your otherwise non-blocking Node program.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that &lt;tt class="docutils literal"&gt;spawn&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;fork&lt;/tt&gt; are much like background daemons, while &lt;tt class="docutils literal"&gt;exec&lt;/tt&gt; runs some binary to completion.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-case-a-service-broker"&gt;
&lt;h2&gt;Example Case: A Service Broker&lt;/h2&gt;
&lt;p&gt;Throughout this tutorial, we will use a theoretical service broker that might query a DB or use XHR to communicate with another server.
It also could be some worker writing or reading data, or anything else that is somehow &amp;quot;IO-bound&amp;quot;.
Whatever you prefer, the point is that this is a blocking (&amp;quot;side-effecting&amp;quot;) IO call in your program that you want to handle asynchronously.
In other words, this means you will need to use &lt;strong&gt;callbacks&lt;/strong&gt; (CB) to be notified when the IO event finished and with which result and error state.&lt;/p&gt;
&lt;p&gt;First, we create a directory where we can put our tutorial code, and for the sake of popularity, use the &lt;a class="reference external" href="http://expressjs.com/"&gt;Express&lt;/a&gt; web app framework:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ mkdir reactive-node-tutorial
$ cd reactive-node-tutorial
$ npm install express
$ node_modules/express/bin/express
yes
$ npm install
&lt;/pre&gt;
&lt;p&gt;We will use the following &lt;tt class="docutils literal"&gt;broker.js&lt;/tt&gt; script, placed in this same directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* a &amp;quot;blocking&amp;quot; query */&lt;/span&gt;
&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nb"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;start&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;: query started&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nb"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;start&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
  &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nb"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; query finished&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;


&lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="c1"&gt;// normally, here would probably be:&lt;/span&gt;
  &lt;span class="c1"&gt;// process.send(query(m))&lt;/span&gt;

  &lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;900&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;send&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Obviously, this is quite a a toy example, because our worker receives and returns only one single message before shutting down.
Similarly, our server API for this broker in &lt;tt class="docutils literal"&gt;app.js&lt;/tt&gt; will be just as simple:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;broker&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;child_process&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;channel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;broker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;fork&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;__dirname&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/db.js&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;send&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="nx"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;error&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Go ahead and replace everything in &lt;tt class="docutils literal"&gt;app.js&lt;/tt&gt; with this code.
In a real-world program, you would use a pool of workers and, each time you start a &lt;tt class="docutils literal"&gt;query&lt;/tt&gt;, use another channel from your pool.
As a matter of fact, most Node libraries will provide you with non-blocking versions of the API they wrap, so you might need no workers at all.
Rather, you are more likely to start scaling Node across multiple cores and even machines using workers and IPC events.
However, this will be enough to run our queries in parallel and demonstrate idioms that can help you write your code to be highly concurrent and easy to scale.
For demonstration purposes this setting will do, so let's start coding our &amp;quot;reactive&amp;quot; app!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-simple-json-rest-service"&gt;
&lt;h2&gt;A Simple JSON-REST Service&lt;/h2&gt;
&lt;p&gt;Let's assume we want to provide a JSON-REST web service connecting a query for a blogger ID against the broker that in turn interfaces to some form of external API or DB.
Upon receiving the user's ID, the server should query the broker for the user's data, and then, in a second step can use that data to fetch any posts the user wrote and any comments she made.
In other words, we have a serial flow of fetching user and the an parallel step of fetching post and comment data.
Once all this data is collected, the server should respond with a JSON object containing this data.&lt;/p&gt;
&lt;p&gt;In the simplest case, you would string a few calls to your query API together and return the results, maybe like this (&lt;em&gt;anti-pattern&lt;/em&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;express&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)()&lt;/span&gt;

&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/sync/:userId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;

  &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;users&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;poster&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;comments&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;commenter&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="p"&gt;})&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;


&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;listen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(Again, you can append this to the app.js you created earlier, just as all following code snippets.)
The above example gives rise to the (wrong) argument why callbacks are a Bad Idea (you will learn the true reason why callbacks are evil later):
By placing one CB inside the former, or returning (think &amp;quot;break&amp;quot;) early if an error occurred, we end up in &lt;a class="reference external" href="http://callbackhell.com/"&gt;callback hell&lt;/a&gt;.
In other words, code that is no longer maintainable and hard to unit test, as we will see.
Worse, we just lost the advantage of using Node... this code is actually being run synchronously, one broker query after the next!
If you run this app in the shell and query the URL (e.g., &lt;tt class="docutils literal"&gt;curl &lt;span class="pre"&gt;http://localhost:8000/sync/example&lt;/span&gt;&lt;/tt&gt;), you will get something like this output:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ node app.js
1381334176812: query started
1381334177793 query finished
1381334177858: query started
1381334178830 query finished
1381334178897: query started
1381334179868 query finished
&lt;/pre&gt;
&lt;p&gt;As can be seen, despite our best intentions, all queries are run one after the next instead of running the last two in parallel.
So we need to dispatch our last two &lt;tt class="docutils literal"&gt;query&lt;/tt&gt; calls simultaneously once we have the &lt;tt class="docutils literal"&gt;user&lt;/tt&gt; data and use the callbacks to collect the results.
In addition, this will allow us to &amp;quot;unstring&amp;quot; this callback chain a bit.
This takes us straight into the realm of asynchronous programming.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="home-brew-asynchronicity"&gt;
&lt;h2&gt;Home-brew Asynchronicity&lt;/h2&gt;
&lt;p&gt;While this certainly is not the cleanest code, here is a quick shot at solving the problem of concurrently running these three queries:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/brew/:userId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;cbCounter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;gotError&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;

  &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;checkError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;gotError&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;gotError&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="nx"&gt;cbCounte&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;users&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;checkError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;

      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;poster&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;checkError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;posts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;
          &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cbCounter&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;

      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;comments&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;commenter&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;checkError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;comments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;
          &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cbCounter&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="p"&gt;})&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;While this version now runs the last two queries concurrently, and, if you have more than one CPU core at least, in parallel, the code has become quite bloated.
You need to ensure that all three calls have completed before returning a valid result.
You need to ensure to only return the error once.
And you need to make sure you have collected the intermediate results.
Last, if you add another query, you might forget to update the counter checks, use a wrong counter value, or forget to store the intermediate result.
Luckily, these issues have been solved with the second most popular node module used by fellow Node coders, &lt;a class="reference external" href="https://github.com/caolan/async"&gt;async&lt;/a&gt; (the most popular dependency being ... you guessed it, &lt;a class="reference external" href="http://underscorejs.org/"&gt;underscore&lt;/a&gt;, which we will use in a bit, too).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="controlling-data-flow"&gt;
&lt;h2&gt;Controlling Data Flow&lt;/h2&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;$ npm install async&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;We wil use the &lt;a class="reference external" href="https://github.com/caolan/async#auto"&gt;auto&lt;/a&gt; method of &lt;a class="reference external" href="https://github.com/caolan/async"&gt;async&lt;/a&gt; to make the code more readable, leaving all the mentioned issues up to the library to take care of.
&lt;tt class="docutils literal"&gt;auto&lt;/tt&gt; takes a &amp;quot;tasks object&amp;quot; (an object where each property is a function considered a task) and returns an equally shaped object with the results if all tasks were run without errors.
In addition to the tasks, you can list any other tasks you handed to auto that have to be completed before that specific task is run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;async&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;async&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/async/:userId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;

  &lt;span class="nx"&gt;async&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;auto&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;users&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;poster&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}],&lt;/span&gt;
    &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;commenter&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}]&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As shown, we send the auto function an object with three tasks, &lt;tt class="docutils literal"&gt;user&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;posts&lt;/tt&gt;, and &lt;tt class="docutils literal"&gt;comments&lt;/tt&gt;.
Both the &lt;tt class="docutils literal"&gt;posts&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;comments&lt;/tt&gt; tasks require that the &lt;tt class="docutils literal"&gt;user&lt;/tt&gt; task has been run successfully before them, as indicated by the array value they map to.
The tasks (functions) have to accept a standard Node callback (&lt;tt class="docutils literal"&gt;function(err, result)&lt;/tt&gt;) and should delegate that to some asynchronous task.
In the case of dependent functions, they can also accept the result of the prior task(s).
The overall outcome then is pushed into another callback that is the last argument of &lt;tt class="docutils literal"&gt;auto&lt;/tt&gt;;
This CB will receive the error status or, if no error occurred, the result of each individual tasks, in the same &amp;quot;shape&amp;quot; as the object sent to auto.&lt;/p&gt;
&lt;p&gt;We have now solved our issues and have achieved our goal of a clean coding of our asynchronous tasks.
However, if you later need to write more complex code, the control to structure defined by your callbacks still gets back to you.
Because you are sending the data as well as the callbacks along your function chain, you are mixing two things you should keep separate:
In a nutshell, you are coding the &lt;strong&gt;control flow&lt;/strong&gt; of your program, while instead you should be describing what is known as &lt;strong&gt;data dependencies&lt;/strong&gt; and let the Node engine figure out when to run which task (callback).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-promised-land"&gt;
&lt;h2&gt;The Promised Land&lt;/h2&gt;
&lt;p&gt;As a matter of fact, with &lt;a class="reference external" href="https://github.com/caolan/async"&gt;async&lt;/a&gt; you will be able to write concurrent Node.js apps perfectly fine.
This last section will introduce you to concepts that will help you get rid of the problems described at the end of the former section.&lt;/p&gt;
&lt;p&gt;To avoid the use of control flow statements, declarative languages instead have developed the concept of the &lt;strong&gt;future&lt;/strong&gt;, also known as a &lt;a class="reference external" href="http://promises-aplus.github.io/promises-spec/"&gt;promise&lt;/a&gt;.
So far, we were using callbacks, that is, functions that do not return a value. This means they are hard to use when &lt;em&gt;composing&lt;/em&gt; new functions from them and, as they do not return a value, are exclusively executed for their &lt;em&gt;side-effects&lt;/em&gt;.
This means that both &lt;em&gt;function composition&lt;/em&gt; and &lt;em&gt;unit testing&lt;/em&gt; of these callbacks is rather a pain.
And it means that you have to manually control the &amp;quot;flow&amp;quot; of your data through these callbacks.
For example, to read a file in Node, the idiomatic structure is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;underscore&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finish&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;sendData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;finish&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;reportError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;finish&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;onResult&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;reportError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="nx"&gt;sendData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;callback&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;onResult&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;readFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;callback&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead, if the &lt;tt class="docutils literal"&gt;readFile&lt;/tt&gt; function were to return a value, this could be expressed in a much cleaner way, without having to nest (structure) the tasks into callback chains.
However, as we know, at the time of calling &lt;tt class="docutils literal"&gt;readFile&lt;/tt&gt;, no such value exists - as a matter of fact, we do not even know if we will be able to read the file at all.
So the only thing we can return is a &amp;quot;future&amp;quot; value, or a &amp;quot;promise&amp;quot; to return such a value (hence the name of these data structures).
For this reason, such a data type would have to provide three methods:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;reject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;reason&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;onFulfilled&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nx"&gt;onRejected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;error&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The called function that returned the promise will, in the future, decide to resolve or reject the promise.
And that new promise will &lt;tt class="docutils literal"&gt;then&lt;/tt&gt; have to describe what the program should do if the promise has been &lt;em&gt;fulfilled&lt;/em&gt; or &lt;em&gt;rejected&lt;/em&gt;.
The most important concept is that promises are &lt;strong&gt;propagating&lt;/strong&gt;:
The return value of &lt;tt class="docutils literal"&gt;then&lt;/tt&gt; (&lt;tt class="docutils literal"&gt;result&lt;/tt&gt; in the above API description) is yet another promise (the &amp;quot;output promise&amp;quot;):&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;If you return a value from any of the two handlers (fulfilled, rejected), the output promise will get fulfilled, too.&lt;/li&gt;
&lt;li&gt;If you throw an exception in any of the two handlers, the output promise will get rejected.&lt;/li&gt;
&lt;li&gt;If you return yet another promise from any of the two handlers, the &lt;tt class="docutils literal"&gt;result&lt;/tt&gt; will &lt;em&gt;become&lt;/em&gt; that promise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means you can comfortably chain promises, while you can rely on &lt;em&gt;return values&lt;/em&gt; (instead of side effects).
With this, we now can separate the concerns and instead describe the asynchronous reading of a file using the promise:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nb"&gt;Promise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;q&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;// wrap fs.readFile as a &amp;quot;promise-returning function&amp;quot;:&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;readFile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;Promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nfbind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;readFile&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;setHeader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;header&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setHeader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;header&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;set200&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;setHeader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;set500&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;setHeader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;readFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;set500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;send&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;set200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;send&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;fin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Promises therefore make it possible to resolve function values in a &lt;em&gt;time-independent&lt;/em&gt; manner:
We can call the &lt;tt class="docutils literal"&gt;then&lt;/tt&gt; method after or &lt;em&gt;before&lt;/em&gt; either &lt;tt class="docutils literal"&gt;resolve&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;reject&lt;/tt&gt; have been executed.
Furthermore, libraries that implement promises must guarantee that no matter how often we check the promise' state (call &lt;tt class="docutils literal"&gt;then&lt;/tt&gt;), that promise always will be resolved or rejected the same way.
We have also eliminated the linear dependencies of each function on the next, thus our functions become much more convenient to reuse.
Last, this version is actually safer than the callback implementation:
If setting the header fails for any reason, this gets logged, and the parameter-less &lt;tt class="docutils literal"&gt;fin&lt;/tt&gt; method is &lt;strong&gt;always&lt;/strong&gt; executed with &lt;tt class="docutils literal"&gt;res.end()&lt;/tt&gt;, ensuring all responses will be closed.&lt;/p&gt;
&lt;p&gt;This makes promises ideal to resolve I/O bound (blocking) tasks.
(Do not use promises for CPU intensive tasks!
Protip: If reading blogs about comparing promise libraries, make sure the tests are using the right kind of task.)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="from-control-flow-to-data-dependencies"&gt;
&lt;h2&gt;From Control Flow to Data Dependencies&lt;/h2&gt;
&lt;p&gt;As we have seen so far, the elegance of using promises instead of callbacks is that blocking functions now can &lt;em&gt;return values&lt;/em&gt; (promises) instead of requiring you to &lt;em&gt;send functions&lt;/em&gt; (callbacks) to them.
In other words, promise-returning functions behave like any other function.
Furthermore, the return value of &lt;tt class="docutils literal"&gt;then&lt;/tt&gt; will always be another (&amp;quot;output&amp;quot;) promise, possibly even the return value from or exception thrown in either handler sent to it.
Because of this behavior, you can keep chaining promises over your success (&lt;tt class="docutils literal"&gt;onFulfilled(result)&lt;/tt&gt;) and error functions (&lt;tt class="docutils literal"&gt;onRejected(error)&lt;/tt&gt;), and the final outcome will always be &lt;em&gt;the same&lt;/em&gt; result value or error.
As you return promises from your functions instead of having to send them continuation functions, you have separated the control flow from your code.&lt;/p&gt;
&lt;p&gt;If you want to learn the nitty-gritty theory behind the benefits of moving from callback-based control flow to data dependencies encoded as promises, James Coglan has an &lt;a class="reference external" href="http://blog.jcoglan.com/2013/03/30/callbacks-are-imperative-promises-are-functional-nodes-biggest-missed-opportunity/"&gt;excellent blog post&lt;/a&gt; on that matter.
And if you need more hands-on experience than what we have discussed so far, &lt;a class="reference external" href="http://strongloop.com/strongblog/promises-in-node-js-with-q-an-alternative-to-callbacks/"&gt;StrongLoop&lt;/a&gt; provides a very comprehensive tutorial of using promises.
We instead will directly get our hands dirty and use this knowledge to describe our three queries problem in terms of promises.
To do so, we will require the popular &lt;a class="reference external" href="http://documentup.com/kriskowal/q/"&gt;Q&lt;/a&gt; and &lt;a class="reference external" href="http://underscorejs.org/"&gt;underscore&lt;/a&gt; libraries:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ npm install q
$ npm install underscore
&lt;/pre&gt;
&lt;p&gt;Without much fanfare, this is how our JSON-REST service looks like using promises and some functional trickery:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;Q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;q&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;underscore&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;q_query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;Q&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nfbind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;fetchPostsAndComments&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;Q&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;all&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nx"&gt;q_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;poster&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;}),&lt;/span&gt;
    &lt;span class="nx"&gt;q_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;comments&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;commenter&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
  &lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;respond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/promise/:userId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;respond&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;q_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;users&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fetchPostsAndComments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;spread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So what has changed?
Admittedly, except for the main function itself being a bit shorter, this code is longer than the asynchronous version.
Nonetheless, after introducing you to the benefits of promises, you should be able to see the elegance of this final solution.
In particular, because of the separation of concerns, your code has become much easier to unit test.
And because each function is clear and expressive, the code is easy to understand.
Last, we have reached our goal of three statements per function, too.&lt;/p&gt;
&lt;p&gt;Well, I hope to have enlightened you in a way or another and maybe made you a better Node developer!&lt;/p&gt;
&lt;/div&gt;
</content><category term="Programming"></category><category term="javascript"></category><category term="node.js"></category></entry><entry><title>Installing a full stack Python data analysis environment on OSX</title><link href="https://fnl.es/installing-a-full-stack-python-data-analysis-environment-on-osx.html" rel="alternate"></link><published>2013-02-11T00:00:00+01:00</published><updated>2013-02-11T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2013-02-11:/installing-a-full-stack-python-data-analysis-environment-on-osx.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: Installing the Scientific Python stack from &amp;quot;source&amp;quot; has become a lot simpler recently and this tutorial was updated accordingly in November 2013 to use with OSX Mavericks and, in particular, &lt;strong&gt;Python 3&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Installing a full-stack scientific data analysis environment on Mac OSX for Python 3 and making sure the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: Installing the Scientific Python stack from &amp;quot;source&amp;quot; has become a lot simpler recently and this tutorial was updated accordingly in November 2013 to use with OSX Mavericks and, in particular, &lt;strong&gt;Python 3&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Installing a full-stack scientific data analysis environment on Mac OSX for Python 3 and making sure the correct, underlying Fortran and C libraries are used is (was?) not trivial.
Thanks to Apple, parts of the required libraries are already on your box when you install XCode (code-named the &amp;quot;&lt;a class="reference external" href="https://developer.apple.com/library/ios/documentation/Accelerate/Reference/AccelerateFWRef/_index.html"&gt;Accelerate&lt;/a&gt; Framework&amp;quot;), and the remaining pieces can easily be installed due to the great &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; project.
In other words, for the &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms"&gt;BLAS&lt;/a&gt; optimizations this setup will use Apple's pre-installed &lt;a class="reference external" href="https://developer.apple.com/library/ios/documentation/Accelerate/Reference/AccelerateFWRef/_index.html"&gt;Accelerate&lt;/a&gt; framework and you can choose to add the &lt;a class="reference external" href="http://www.cise.ufl.edu/research/sparse/SuiteSparse/"&gt;SuiteSparse&lt;/a&gt; and &lt;a class="reference external" href="http://www.fftw.org/"&gt;FFTW&lt;/a&gt; libraries via Homebrew for some extra speed when factorizing sparse matrices and doing Fourier transforms.
This guide will describe how to properly install the following software stack on Mac OSX from their sources and ensuring all the relevant C/Fortran &amp;quot;acceleration&amp;quot; is available:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.scipy.org/scipylib"&gt;SciPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this stack, it is a breeze to add other cool data analysis tools such as &lt;a class="reference external" href="http://scikit-learn.org/stable"&gt;scikit-learn&lt;/a&gt;, &lt;a class="reference external" href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt;, &lt;a class="reference external" href="http://sympy.org/en/index.html"&gt;SymPy&lt;/a&gt;, or &lt;a class="reference external" href="http://github.com/pymc-devs/pymc"&gt;PyMC&lt;/a&gt; in your &lt;a class="reference external" href="http://www.virtualenv.org/"&gt;VirtualEnv&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="preparatory-setup"&gt;
&lt;h2&gt;Preparatory Setup&lt;/h2&gt;
&lt;p&gt;First, you need to make sure you have &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; installed and running without any issues:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew doctor
&lt;/pre&gt;
&lt;p&gt;If that produces any other output than:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Your system is ready to brew.
&lt;/pre&gt;
&lt;p&gt;you need to stop &lt;em&gt;right now&lt;/em&gt; and fix the issues or install &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; first.
Note that if you &lt;em&gt;upgraded&lt;/em&gt; to OSX Mavericks, you also need to upgrade your XCode command line tools (or download them if you have not installed them) by executing:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
xcode-select --install
&lt;/pre&gt;
&lt;p&gt;(And this means that you will have to re-install/compile most brew libraries, too, because of a change of XCode libraries...)
Once you have a clean version of Homebrew up and running, you can proceed to install the actual requirements.&lt;/p&gt;
&lt;p&gt;First, you need to install a Fortran compiler and &lt;a class="reference external" href="http://docs.python.org/3"&gt;Python3&lt;/a&gt; itself:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew tag homebrew/science
brew install gfortran
brew install python3
&lt;/pre&gt;
&lt;p&gt;All of these commands should work nicely and you should encounter no issues.&lt;/p&gt;
&lt;p&gt;Second, it is obviously necessary to set up a minimal Python environment.
This tutorial will be using &lt;strong&gt;distribute&lt;/strong&gt; and &lt;strong&gt;pip&lt;/strong&gt; to install Python packages:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
curl -O http://python-distribute.org/distribute_setup.py
python3 distribute_setup.py
curl -O https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py
python3 get-pip.py
&lt;/pre&gt;
&lt;p&gt;Note that you &lt;em&gt;do not&lt;/em&gt; need to prefix &lt;tt class="docutils literal"&gt;sudo&lt;/tt&gt; to any of this - because you installed Python 3 using &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt;, you are relieved from having to &amp;quot;root&amp;quot; everything.
And you should consider using &lt;a class="reference external" href="http://www.virtualenv.org/"&gt;VirtualEnv&lt;/a&gt; and &lt;a class="reference external" href="http://nose.readthedocs.org/en/latest"&gt;nose&lt;/a&gt; for your Python development, too:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip3 install virtualenv
pip3 install nose
&lt;/pre&gt;
&lt;p&gt;With this setup, you have Homebrew plus Python 3000 with &lt;strong&gt;pip&lt;/strong&gt;,  &lt;strong&gt;nosetests&lt;/strong&gt;, and &lt;strong&gt;virtualenv&lt;/strong&gt; all set up.
This is a great start for any kind of Python development;
Normally, it is suggested to &amp;quot;stop&amp;quot; here and install all further Python packages only in each &amp;quot;virtual environment&amp;quot;.
However, this scientific stack you are building is quite a lot of work to set up (compile-wise), so it is a time-saver to have this stack installed globally and then make use of it via &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--system-site-packages&lt;/span&gt;&lt;/tt&gt; when creating a new virtual environment instead of having to install it each time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="numpy"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;First, download the latest stable &lt;a class="reference external" href="http://sourceforge.net/projects/numpy/files/NumPy/"&gt;NumPy sources from SourceForge&lt;/a&gt;.
By installing from source, NumPy will automatically detect that you are using OSX and therefore configure itself to use the &lt;em&gt;Accelerate&lt;/em&gt; framework for the BLAS/LAPACK optimizations:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 setup.py config
&lt;/pre&gt;
&lt;p&gt;Below &lt;tt class="docutils literal"&gt;atlas_info&lt;/tt&gt;, at the end the config output, you should see the following message:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
FOUND:
  extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']
  extra_compile_args = ['-msse3']
  define_macros = [('NO_ATLAS_INFO', 3)]
&lt;/pre&gt;
&lt;p&gt;As NumPy recognized Accelerate, you can proceed with the installation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 setup.py build
python3 setup.py install
&lt;/pre&gt;
&lt;p&gt;If you installed &lt;a class="reference external" href="http://nose.readthedocs.org/en/latest"&gt;nose&lt;/a&gt; (as advised), you also can test that your installation is working correctly (note that you must &lt;em&gt;move to another directory&lt;/em&gt; than where you build NumPy before running the tests):&lt;/p&gt;
&lt;blockquote&gt;
python3 -c &amp;quot;import numpy; numpy.test('full')&amp;quot;&lt;/blockquote&gt;
&lt;p&gt;All tests should pass without errors or issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="scipy"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://www.scipy.org/scipylib"&gt;SciPy&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To use SciPy, you need to install &lt;a class="reference external" href="http://cython.org/"&gt;Cython&lt;/a&gt; and &lt;a class="reference external" href="http://swig.org/"&gt;SWIG&lt;/a&gt; first:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip3 install Cython
brew install swig
&lt;/pre&gt;
&lt;p&gt;Optionally, you can also install &lt;a class="reference external" href="http://www.openblas.net/"&gt;OpenBLAS&lt;/a&gt;, &lt;a class="reference external" href="http://www.fftw.org/"&gt;FFTW&lt;/a&gt; and &lt;a class="reference external" href="http://www.cise.ufl.edu/research/sparse/SuiteSparse/"&gt;SuiteSparse&lt;/a&gt; (for the AMD and UMFPACK libraries) for some extra speedups on Fourier Transform and sparse asymmetric matrix factorizations:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew install openblas
brew install fftw --with-fortran
brew install suite-sparse --with-openblas
&lt;/pre&gt;
&lt;p&gt;This step is probably recommended (although it is entirely optional).&lt;/p&gt;
&lt;p&gt;Next you now can fetch the &lt;a class="reference external" href="http://sourceforge.net/projects/scipy/files/scipy/"&gt;SciPy sources from SourceForge&lt;/a&gt; and build them:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 setup.py config
python3 setup.py build
python3 setup.py install
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;config&lt;/tt&gt; step is only there so you can make sure SciPy found the Accelerate framework and the UMFPACK/AMD SuiteSparse libraries.
The &lt;a class="reference external" href="http://www.fftw.org/"&gt;FFTW&lt;/a&gt; library you installed earlier with Homebrew is not listed in this output, but will be used during the build, too.&lt;/p&gt;
&lt;p&gt;As with NumPy, you can run some tests to ensure our installation is working properly after moving to another directory:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 -c &amp;quot;import scipy; scipy.test()&amp;quot;
&lt;/pre&gt;
&lt;p&gt;None of the tests should fail (except for KNOWNFAIL and SKIP tests, naturally).&lt;/p&gt;
&lt;p&gt;If you have come this far, congratulations! Everything from here on will be a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Cakewalk"&gt;cake-walk&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="matplotlib"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The next step is the installation of matplotlib:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip3 install matplotlib
&lt;/pre&gt;
&lt;p&gt;As it is trivial to install and only takes a few minutes, you might consider adding it to your virtual environments only.
However, the next packge that will be installed, IPython, makes use of matplotlib and is quite a hassle to install in every virtual environment.&lt;/p&gt;
&lt;p&gt;To ensure the plotting library is working, try this in an interpreter:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
&amp;gt;&amp;gt;&amp;gt; from pylab import \*; plot([1,2,3]); show()
&lt;/pre&gt;
&lt;p&gt;You should see a plot with a straight diagonal.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ipython"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Now it is time to install a great MATLAB-like interpreter and environment.
The first, optional, step is to install PyQt4 so you can use IPython's &lt;tt class="docutils literal"&gt;qtconsole&lt;/tt&gt;.
This is not required, but it is nice to render plots inline in a Qt terminal window, making the IPython &amp;quot;experience&amp;quot; more like MATLAB:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew install sip --with-python3
brew install qt --HEAD # currently, on Mavericks, the --HEAD option is required
&lt;/pre&gt;
&lt;p&gt;Finally, you need to &lt;a class="reference external" href="http://www.riverbankcomputing.com/software/pyqt/download"&gt;download&lt;/a&gt; and install PyQt4 using:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 configure-ng.py
make &amp;amp;&amp;amp; make install
&lt;/pre&gt;
&lt;p&gt;Apart from PyQt4, installing IPython itself is again straightforward:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip3 install ipython[zmq,qtconsole,notebook,test]
&lt;/pre&gt;
&lt;p&gt;To make sure the installation worked, execute the newly installed &lt;tt class="docutils literal"&gt;iptest3&lt;/tt&gt; script.
Again as before, there should be no failures.&lt;/p&gt;
&lt;p&gt;From now on, instead of &lt;tt class="docutils literal"&gt;python3&lt;/tt&gt;, you should be using &lt;tt class="docutils literal"&gt;ipython3&lt;/tt&gt; if you want to work in a Python interpreter and you have reached the &amp;quot;holy grail&amp;quot; of having set up a MATLAB-like scientific computing environment:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
ipython3 qtconsole --pylab=inline
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="additional-data-science-libraries"&gt;
&lt;h2&gt;Additional Data Science Libraries&lt;/h2&gt;
&lt;p&gt;Finally, here is a list of mature, interesting data science libraries that all will use the stack you just installed.
These could all go either into the global site-packages, or you can just add them to your projects in your virtual environments as needed.
In the latter case, do not forget to enable the globabl stack with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--system-site-packages&lt;/span&gt;&lt;/tt&gt; when creating a new &lt;a class="reference external" href="http://www.virtualenv.org/"&gt;VirtualEnv&lt;/a&gt;.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://scikit-learn.org/stable"&gt;scikit-learn&lt;/a&gt; machine learning library: &lt;tt class="docutils literal"&gt;pip3 install &lt;span class="pre"&gt;scikit-learn&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt; statistical data analysis: &lt;tt class="docutils literal"&gt;pip3 install pandas&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://sympy.org/en/index.html"&gt;SymPy&lt;/a&gt; symbolic computer algebra system: &lt;tt class="docutils literal"&gt;pip3 install sympy&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://github.com/pymc-devs/pymc"&gt;PyMC&lt;/a&gt; probabilistic programming environment (see this &lt;a class="reference external" href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"&gt;PyMC tutorial&lt;/a&gt;):
&lt;tt class="docutils literal"&gt;pip3 install pymc&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other noteworthy analytical tools include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.pytables.org/"&gt;PyTables&lt;/a&gt; large data management: &lt;tt class="docutils literal"&gt;pip3 install tables&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://rpy.sourceforge.net/"&gt;RPy2&lt;/a&gt; Python-R interface: &lt;tt class="docutils literal"&gt;pip3 install rpy2&lt;/tt&gt; (assuming you have R installed)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://github.com/pydata/patsy"&gt;patsy&lt;/a&gt; and &lt;a class="reference external" href="http://statsmodels.sourceforge.net/"&gt;StatsModels&lt;/a&gt; statistical models:
&lt;tt class="docutils literal"&gt;pip3 install patsy &amp;amp;&amp;amp; pip3 install statsmodels&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;E voilà&lt;/em&gt; - you now have a fully functioning environment for running
all kinds and sorts of statistical data analyses and developing machine
learning algorithms!&lt;/p&gt;
&lt;/div&gt;
</content><category term="Programming"></category><category term="python"></category><category term="apple"></category><category term="data mining"></category></entry><entry><title>Rails: RSpec'ing controllers with declarative authorization AND AuthLogic</title><link href="https://fnl.es/rails-rspecing-controllers-with-declarative-authorization-and-authlogic.html" rel="alternate"></link><published>2010-03-12T00:00:00+01:00</published><updated>2010-03-12T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2010-03-12:/rails-rspecing-controllers-with-declarative-authorization-and-authlogic.html</id><summary type="html">&lt;p&gt;I just had a rough time figuring out how to bypass all the security
features of the Rails project I am developing to write decent controller
specs with RSpec. I am using AuthLogic as authentication module and
declarative authorization (DA) for exactly that. However, when I started
to write controller …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just had a rough time figuring out how to bypass all the security
features of the Rails project I am developing to write decent controller
specs with RSpec. I am using AuthLogic as authentication module and
declarative authorization (DA) for exactly that. However, when I started
to write controller specs that would simulate (HTTP) GET requests, I ran
into a wall: I simply could not digg what the cleanest way would be to
bypass both AuthLogic and DA. Finally, after finding the right queries
in Google, I managed to get the necessary snippets. To avoid that the
same tedious task might befall you, here's what you need to add, e.g.,
to your spec_helpers directory - I called the file
&amp;quot;controller_helpers.rb&amp;quot;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;module&lt;/span&gt; &lt;span class="nn"&gt;SessionHelper&lt;/span&gt;
  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;current_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;
    &lt;span class="vi"&gt;@current_user&lt;/span&gt; &lt;span class="o"&gt;||=&lt;/span&gt; &lt;span class="n"&gt;mock_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;User&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;user_session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;user_stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;
    &lt;span class="vi"&gt;@current_user_session&lt;/span&gt; &lt;span class="o"&gt;||=&lt;/span&gt; &lt;span class="n"&gt;mock_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="no"&gt;UserSession&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="ss"&gt;:user&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;current_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_stubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;login&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;session_stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;user_stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;
    &lt;span class="no"&gt;UserSession&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stub!&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:find&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;and_return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;user_session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;session_stubs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_stubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;logout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="vi"&gt;@user_session&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;nil&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;disable_authorization&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="no"&gt;Authorization&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ignore_access_control&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The trick is that, for AuthLogic, you can now &amp;quot;authenticate&amp;quot; the user
by the stubbed UserSession that returns a mocked User model. DA is less
complicated: the &lt;tt class="docutils literal"&gt;disable_authorization()&lt;/tt&gt; method is all that is
needed. Now, in your &amp;quot;spec_helper.rb&amp;quot;, you add this line to the top:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;require&lt;/span&gt; &lt;span class="no"&gt;File&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dirname&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;__FILE__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/spec_helpers/controller_helpers&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And this line somewhere in the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;Spec::Runner.configure&lt;/span&gt;&lt;/tt&gt; loop:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;SessionHelper&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, in your controller specs, it is more than trivial to disable
authorization and authentication at once - simply add the following
line, e.g., to your &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;before(:each)&lt;/span&gt;&lt;/tt&gt; definitions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;disable_authorization&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;login&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Voila - your GET requests pass; and you can even add stubs to your
User model, if needed, by adding them as key-value pairs to the
&lt;tt class="docutils literal"&gt;login()&lt;/tt&gt; call above! So now you can get back to make your specs
pass...&lt;/p&gt;
</content><category term="Programming"></category><category term="rspec"></category><category term="rails"></category></entry><entry><title>MobileMe vs. SugarSync vs. DropBox</title><link href="https://fnl.es/mobileme-vs-sugarsync-vs-dropbox.html" rel="alternate"></link><published>2009-05-26T00:00:00+02:00</published><updated>2009-05-26T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2009-05-26:/mobileme-vs-sugarsync-vs-dropbox.html</id><summary type="html">&lt;p&gt;I now have tested MobileMe, SugarSync, and DropBox for quite a while to
decide which service to buy for syncing my “electronic life” between my
Macs (soon I’ll be managing two OSX Server blades, one Mini, and two
MBPs!). After this period, there is no doubt to me: I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I now have tested MobileMe, SugarSync, and DropBox for quite a while to
decide which service to buy for syncing my “electronic life” between my
Macs (soon I’ll be managing two OSX Server blades, one Mini, and two
MBPs!). After this period, there is no doubt to me: I’m syncing my iCal
calendars and Address Book content via Google, my bookmarks with XMarks,
and everything else via DropBox.&lt;/p&gt;
&lt;p&gt;MobileMe’s iDisk is nothing more than a pain and a piece of junk, which
I honestly did not expect. After all the problems they had last year
launching Me.com, I thought they would have by now created a working
service. But the iDisk and syncing my PIM (Personal Information Manager
- I still use Yojimbo, as Evernote’s and Together’s handling of
encryption are pure patched add-ons) was just a [bad] joke: You even
need to buy extra software if you want to do file syncing, as iDisk’s
“offline” sync is so slow and error prone I could not believe Apple
dares to offer something like that. So you need to either use Lingon and
rsync to sync to your online mode iDisk which doesn’t win a medal for
simplicity, or buy something like ChronoSync - and that takes hours
(!!!) to ensure 10 GB of data in about 30-40k Files are synced, &lt;strong&gt;every
time&lt;/strong&gt;. All the more, ChronoSync may be the fastest and safest syncer in
the wild! What finally got me mad was the sync agent using 90% CPU all
of the time, at times virtually locking you out of your own machine,
while performing almost nothing. Finally, if you ever try to navigate
that online iDisk, get yourself a cup of tea, you will have plenty of
time to drink it up until that file is open…&lt;/p&gt;
&lt;p&gt;Compared to SugarSync, DropBox with its simplicity and real versioning
of files is significantly better performing than SugarSync, espcially if
we talk upload and real volume, and if you ever tried SugarSync, it is a
resource hugger (not as bad as iDisk, but it will stop your workflow).
So in the end the choice for me was based on “mutual exclusion”, there
is simply still no service that can hold the candle to DropBox - and
just got me Pro account. As I am writing this, I am syncing up dozens of
gigs of data to my 50 GB DropBox, and I hardly notice it happening!&lt;/p&gt;
&lt;p&gt;Oh, if you get yourself an account for DropBox, either the free 2 GB or
a full Pro account, I would appreciate if you register via &lt;a class="reference external" href="https://www.getdropbox.com/referrals/NTE0NTA0OTk"&gt;this link&lt;/a&gt;,
as it creates me some 500 MB extra space for referring you :o).&lt;/p&gt;
</content><category term="Miscellaneous"></category><category term="apple"></category><category term="cloud storage"></category></entry><entry><title>My first visit to a volcano</title><link href="https://fnl.es/my-first-visit-to-a-volcano.html" rel="alternate"></link><published>2009-05-01T00:00:00+02:00</published><updated>2009-05-01T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2009-05-01:/my-first-visit-to-a-volcano.html</id><summary type="html">&lt;p&gt;Actually, I already considered myself very lucky this year for visiting
the jungle in the Amazons. I would have not thought I could repeat such
an experience that soon again, but was proven wrong shortly after. A few
weeks ago, my girlfriend spent some weeks in Spain and we used …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Actually, I already considered myself very lucky this year for visiting
the jungle in the Amazons. I would have not thought I could repeat such
an experience that soon again, but was proven wrong shortly after. A few
weeks ago, my girlfriend spent some weeks in Spain and we used the last
long weekend we had together to go to Tenerife, Canary islands. There I
had the pleasure to visit yet another really strange and amazing place
in our world: a volcano! My girlfriend, who grew up in Mexico, thought
it was quite funny I had not seen a volcano before... Well, she has not
been on a glacier so far :-P. I can only recommend this place - if you
do not know where to go for a short trip for a few days, you just have
found it. We rented a very romantic small house on the north-eastern tip
of the island, far away from all the tourist places. The owner renting
it to us (Juan Ramón) is a very kind and friendly guy, and he even went
into the trouble to send me my keys and a whole load of money I forgot
in the house - without even taking a cent for the troubles, although I
kept insisting! (Anybody knowing me don't laugh! Okay, I know, typically
Flo...) The house itself is built into the cliffs, with a marvelous view
over the sea and the beach below, and a great terrace. Mayte, my
girlfriend, found it and I can only say this was a great choice. If you
feel like renting it, follow this &lt;a class="reference external" href="http://www.casas-turismo-rural.com/alojamiento.phtml?alojamiento=2740"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We spent five days there, which is just enough time to visit the island
in a hurry - time flew by like nothing. You have to ask permission to
access the volcano in the capital city, Santa Cruz, which will then be
granted to you for a following day at a certain hour. Naturally, we
managed to be there too late the next day, but arrived again the day
after and the friendly guys guarding the entrance let us up anyways. Oh,
and pleeease dress appropriately: you can't imagine the scores of
(Russian) girls trying to walk a volcano in high heels and with almost
nothing on - freezing to death at 3.700 m ASL and not able to walk in the
rocky area... Well, at least this allowed us to be completely alone at the
top of the crater! Another great feat of the park is, as it's at 2000+ m
ASL, that even if it's cloudy down on the beach, it might be a gorgeous
day in the park!&lt;/p&gt;
&lt;p&gt;Finally, one last word of warning: do not take residence on the south
side of the island except if you like artificial touristic sprawls: the
whole area is one urban block of concrete and completely hideous. As far
as I can tell, the north-east tip is best, maybe the (north-)western
areas may have some nice hideaways from the touristic towns, too. As for
the climate: just expect 20-25 ˚C in winter, 20-30 ˚C in summer - great!
One final recommendation if you like driving on narrow, curvy mountain
roads: rent a powerful car - I haven't had so much fun riding a car in a
long time!&lt;/p&gt;
</content><category term="Travelling"></category><category term="mayte"></category><category term="spain"></category><category term="travel"></category></entry><entry><title>News, Swines &amp; Pigs</title><link href="https://fnl.es/news-swines-pigs.html" rel="alternate"></link><published>2009-04-30T00:00:00+02:00</published><updated>2009-04-30T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2009-04-30:/news-swines-pigs.html</id><summary type="html">&lt;p&gt;Usually, I prefer to steer free from the day-to-day mainstream news, yet
even I have to accept a low level of &amp;quot;noise&amp;quot; if I want to know at least
something about the most significant things going on. However, currently
I get the overwhelming feeling that the whole news world is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Usually, I prefer to steer free from the day-to-day mainstream news, yet
even I have to accept a low level of &amp;quot;noise&amp;quot; if I want to know at least
something about the most significant things going on. However, currently
I get the overwhelming feeling that the whole news world is grunting and
snorting like a pigsty. You guessed it, I am concerned about all this
&amp;quot;swine flu&amp;quot; reporting going on. As can be easily demonstrated, this
whole &amp;quot;pandemic alert&amp;quot; and panic-making has gone completely out of
proportion. It is yet another example of how ridiculous news agencies
exaggerate or even blur the facts, and this attitude might be much more
lethal than most biological illnesses if measured by its indirect
impact.&lt;/p&gt;
&lt;p&gt;First of, some flu virus classification: There are three types of
influenza virus: A, B, and C. B and C play only very minor roles as they
mutate slower, are less virulent, and affect far less species. Usually,
when talking about a flu virus, type A is implied. Type A influenza then
is further categorized by the HxNx nomenclature. The H refers to the
hemagglutinin (HA) lectin and the N to the neuroaminidase (NA)
glycoprotein. Both are found on the outside coat of the virus particle.
HA mediates the virus' binding to target cells, while NA is responsible
for the release of progeny virus from infected host cells. The numbers
denote the antibody response of the virus, ordered by historic discovery
- meaning, a virus with the same H/N number is identified by the same
&lt;em&gt;type&lt;/em&gt; of antibodies, which form part of your immune system/defense. HA
and NA are essential for the virulence (the relative ability to cause
disease) in terms of infectiveness and the epidemic capabilities of the
strain. H1N1 denotes the class of flu virus HA/NA proteins that is the
most commonly found form in human influenza. Our immune system defends
us by recognizing mainly those two proteins (the &amp;quot;antigenes&amp;quot;) through
our antibodies. This means, from a pure immuno-defense point of view,
this kind of strain is the most well known to the human body and immune
system. This is one of the reasons why the new Hong Kong avian flu, with
its H5N1 composition, is much more virulent than the current H1N1 swine
flu or any other &amp;quot;regular&amp;quot; flu.&lt;/p&gt;
&lt;p&gt;Endemic states and lethal properties: H1N1 most deadly appearance and
the worst pandemic in modern human history was what we now know as the
&amp;quot;Spanish Flu&amp;quot; in 1918; This specific influenza virus transformed its
endemic properties (the ability to propagate within one kind of species,
in this case birds) to a panzootic state (affecting animals including
humans - epizootic would be the intermediate state that does not affect
humans). Note that this pandemic occurred during WW I, largely
facilitating its spread. In general, any kind of virus capable of
overcoming the species barrier is potentially more dangerous, as it is
likely to carry genetic material and protein structures the newly
infected species has never seen before, therefor being much more
virulent and lethal than the existing endemic strains. However, the
Spanish Flu killed somewhere around 20 million people (taking
conservative, low estimates), and its symptoms were so strong it was
often misdiagnosed as some much more severe infection. Apart from the
HA/NA properties already mentioned, there is the actual RNA (viruses
commonly use RNA instead of DNA to carry their genetic information) that
a virus uses to encode its proteins and other functions that are
important to the survival and impact of a virus. Our immune defense
looks for RNA sequences those are different to any sequence found in our
body and &amp;quot;destroys&amp;quot; those foreign sequences by cleaving the strands into
non-functional pieces with the help of so-called RNases. For this to
work, the immune system therefor has to identify the RNA [as foreign].
But a strand of RNA coming from another species might actually contain
nucleotide base sequences our immune system does not recognize (because
it only recognizes already &lt;em&gt;known&lt;/em&gt; foreign sequences), making the virus
much more lethal. This process of changing the RNA and protein
configuration is amplified by two related mechanisms called &amp;quot;genetic
drift&amp;quot; and &amp;quot;antigenic shift&amp;quot;. In the case of the Spanish flu, the RNA
was very &amp;quot;new&amp;quot; to the human's immune system and had a very high mutation
rate: how often the RNA sequence changes, roughly the meaning of the
aforementioned genetic drift and antigenic shift. On the contrary, the
new &amp;quot;Mexican&amp;quot; swine flu virus has a very similar RNA composition to
regular virus strains found in humans, i.e. it does not appear to be
significantly more leathal than any other flu. The only known difference
to the regular flu circulating in humans is that it seems to affect more
younger than older people, possibly due to the fact that older people's
immune systems already have &amp;quot;seen&amp;quot; a similar version of this virus some
time ago and therefor have antibodies that recognize the HA/NA
glycoproteins.&lt;/p&gt;
&lt;p&gt;Now compare this swine flu against any regular flu by numbers: The
regular flu kills about 250,000 to &lt;em&gt;half a million&lt;/em&gt; people &lt;em&gt;per year&lt;/em&gt;.
This overhyped swine flu managed to kill &lt;em&gt;eight&lt;/em&gt; (8!) humans so far and
it has been confirmed to have infected about 150 people worldwide (WHO
data, 29th of April, 2009). I.e., on a daily average about a
&lt;strong&gt;thousand times more&lt;/strong&gt; people die from the regular flu than this new
strain. Regular flu is almost continuously spreading somewhere in the
world, i.e., if the WHO took this into account, we would be living in a
nearly constant influenza pandemic. The swine flu just now made it to
the last stage before &lt;em&gt;even being defined&lt;/em&gt; as a WHO &amp;quot;pandemic&amp;quot;: There
must be a few known infections in at least two countries. Recall this
definition and check the real numbers when somebody is talking about a
new &amp;quot;pandemic&amp;quot;. The infection with swine flu seems to be no more lethal
than with any other flu, so your chances of dying from the swine flu
outside of Mexico are so marginal it makes no sense to take them into
account, while if you do go to Mexico, all you seriously need to do is
make sure your current health state is good enough to survive any flu
anywhere, which is much more prevalent and 1,000 times more likely to
kill you by a global statistic&lt;em&gt;.&lt;/em&gt; But the main point is: there is
nothing dangerous or wrong about going to Mexico, at least concerning
the flu. I would be much more worried about drug gangsters and hijackers
there if I were you: They managed to kill several thousands of people
this year alone already. In other words, the WHO rulings and
suggestions, that are close to ridiculous given the circumstances,
combined with the media hype are about to isolate Mexico from the world,
which leads me to my final and most important point.&lt;/p&gt;
&lt;p&gt;In general, this &amp;quot;pig-hyped&amp;quot; flu without any review of its background,
no factual content, and exclusively based on beliefs and propaganda has
only one really worrisome influence: it is weakening and isolating
Mexico, both socially and economically. Our ignorance to real facts are
estimated to cost Mexico &lt;em&gt;City&lt;/em&gt; (&amp;quot;D.F.&amp;quot;) alone around $88 million &lt;em&gt;per
day&lt;/em&gt;, and this figure will need huge updates for the crash Mexico's main
(legal) economic sector, tourism, will suffer, plus the costs incurred
on the country as a whole. This number will be similar to or more likely
even exceed the daily cost of the U.S. oil war in Irak (estimated to
about $250 million, in case you didn't know) - the only good news being
that instead of about 100 (direct, not counting the indirect toll, which
is estimated to be around five times higher) deaths per day, the swine
flu's daily death toll is still below one. In other words, the combined
direct and indirect negative impact of this insubstantial media hype on
a close to imaginary &amp;quot;Mexican Flu&amp;quot; will cost and destroy much more lives
than the virus itself most likely ever will have been capable of. Media
propaganda crusades against a country nowadays have the same
socioeconomic impact as the largest &amp;quot;real&amp;quot; war in decades if measured by
the daily cost. Keep this in mind the next time you read news about
swines from pigs.&lt;/p&gt;
</content><category term="Miscellaneous"></category><category term="politics"></category></entry><entry><title>Why I love Python 3.0: Unicode + UTF-8</title><link href="https://fnl.es/why-i-love-python-30-unicode-utf-8.html" rel="alternate"></link><published>2009-04-27T00:00:00+02:00</published><updated>2009-04-27T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2009-04-27:/why-i-love-python-30-unicode-utf-8.html</id><summary type="html">&lt;div class="section" id="tl-dr-summary"&gt;
&lt;h2&gt;tl;dr summary&lt;/h2&gt;
&lt;table&gt; &lt;tbody&gt;
&lt;tr&gt; &lt;th align="left"&gt; &lt;strong&gt;Python pre-3.0&lt;/strong&gt; &lt;/th&gt;
     &lt;th align="left"&gt; &lt;strong&gt;Python post-3.0&lt;/strong&gt; &lt;/th&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; str.encode &lt;/td&gt; &lt;td&gt; bytes.translate or (new) str.encode &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; str.decode &lt;/td&gt; &lt;td&gt; bytes.decode &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; unicode &lt;/td&gt; &lt;td&gt; str &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; unicode.encode &lt;/td&gt; &lt;td&gt; str.encode &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; unicode.decode &lt;/td&gt; &lt;td&gt; *n/a* &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; str("x") == unicode("x") &amp;nbsp;&amp;nbsp; &lt;/td&gt; &lt;td&gt; bytes("x") != str("x") &lt;/td&gt; &lt;/tr&gt;
&lt;/tbody&gt; &lt;/table&gt;&lt;p&gt;This change in Python 3.0 might be more than useful …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="tl-dr-summary"&gt;
&lt;h2&gt;tl;dr summary&lt;/h2&gt;
&lt;table&gt; &lt;tbody&gt;
&lt;tr&gt; &lt;th align="left"&gt; &lt;strong&gt;Python pre-3.0&lt;/strong&gt; &lt;/th&gt;
     &lt;th align="left"&gt; &lt;strong&gt;Python post-3.0&lt;/strong&gt; &lt;/th&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; str.encode &lt;/td&gt; &lt;td&gt; bytes.translate or (new) str.encode &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; str.decode &lt;/td&gt; &lt;td&gt; bytes.decode &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; unicode &lt;/td&gt; &lt;td&gt; str &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; unicode.encode &lt;/td&gt; &lt;td&gt; str.encode &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; unicode.decode &lt;/td&gt; &lt;td&gt; *n/a* &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; str("x") == unicode("x") &amp;nbsp;&amp;nbsp; &lt;/td&gt; &lt;td&gt; bytes("x") != str("x") &lt;/td&gt; &lt;/tr&gt;
&lt;/tbody&gt; &lt;/table&gt;&lt;p&gt;This change in Python 3.0 might be more than useful for
anybody intending to write programs that use more than the ASCII
characters (A-z, 0-9, and some symbols), which, given how i18n'ed most
applications are today, is rather the norm than the exception. I also
hope to encourage my fellow Pythoneers to update to 3.0 as soon as
humanly possible, not only because of this change, but because of the
general advantages of Python 3.0 (aka &amp;quot;no-where near 3000&amp;quot;...).&lt;/p&gt;
&lt;p&gt;In case you do not understand the difference between Unicode and String
arrays, here is a short paragraph to get you started. A String (str in
pre-3.0 Python, bytes/bytearray in Python 3.x+) is a byte-array already
&lt;em&gt;bound&lt;/em&gt; to a specific character-lookup table (e.g. ASCII, Latin-1,
UTF-8, etc.) to find the correct representation for that String. Note
that this is not the &lt;em&gt;glyph&lt;/em&gt; itself you see on-screen, as this depends
on, e.g., what font you are using, and is handled by the GUI toolkit or
the terminal. A Unicode array (unicode in pre-3.0, str in 3.x+) on the
other hand is an array of &amp;quot;universal&amp;quot; bytes, so-called &lt;strong&gt;code-points&lt;/strong&gt;
usually managed as two-byte arrays, but has no native representation.
Therefore, to create something readable from an Unicode object, you have
to &lt;em&gt;encode&lt;/em&gt; its bytes by using a codetable, such as ASCII or UTF-16, to
the correct String representation (&amp;quot;&lt;em&gt;bind&lt;/em&gt; the Unicode array to a code
table&amp;quot;). On the contrary, to create a Unicode array from a String array,
you need to &lt;em&gt;decode&lt;/em&gt; (&amp;quot;unbind&amp;quot;) the String's coding to get the
&amp;quot;universal&amp;quot; (in quotation marks as not all programming langues have to
use base 16 integers (aka hex, or two bytes)) Unicode. If you are not
used to thinking in these terms, a general tip for pre-3.0 Python: your
program should, when handling String input (SAX parsers for example
already do the conversion for you), convert it to Unicode (decode the
Strings), and when outputing your Unicode arrays, convert them back to
the desired String representation (encode them) - while working with
Unicode internally to avoid bugs and possible exploits. A (rather
stupid, but you can interpolate the danger, I hope) snippet from
Python's Unicode HOWTO might exemplify this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;u&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;#39;/&amp;#39; not allowed in filename&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looks good at first, but what about sending that function a String not
in any standard encoding? For example, the UTF-7 encoding for
u&amp;quot;/etc/passwd&amp;quot; is &amp;quot;+AC8-etc+AC8-passwd&amp;quot; - a nasty mistake if that file
is presented to a user... (the work-around in this trivial example is
obvious: just decode before the if-clause - or, even better, when the
string enters your program - and compare to u'/'). To summarize, in
Python (not so in C, for example!) a Unicode array consists of two-byte
elements (base 16 integers) called code-points, Strings are arrays of
bytes which are bound to a codetable that helps the Python interpreter
look up the bytes' character representations and send them to your
terminal or GUI. Unicode to String conversion is called encoding
(&amp;quot;binding&amp;quot;), String to Unicode conversion is decoding (&amp;quot;unbinding&amp;quot;). The
fact that, when using the Python shell, you see &amp;quot;real&amp;quot; characters for a
String or Unicode object is pure convenience and should not distract you
from how they truly work internally.&lt;/p&gt;
&lt;p&gt;After this lengthy Unicode vs. String intro, the best news first: if you
can allow yourself the luxury to program with any Python version and are
not dependent on external libraries, Python 3.0 is just made for you:
The new native String object is always a Unicode representation, and the
default encoding chosen for representing your strings is UTF-8. In other
words, if you use Python 3.0 and are happy with UTF-8, you no longer
have to worry about decoding your (byte) strings to Unicode arrays or
binding your Unicode code-points to the right (byte-) string
representations. While this might seem like something that should have
been done long ago, for historic reasons older programming languages
(plus Python pre-3.0) use ASCII as the default encoding, meaning you had
to look after de-/encoding the whole time when working with input/output
functionality of your programs and using most other languages other than
English - and even there you might want to have special characters
(don't be so naïve...). Sad side to this: what I am talking about here
is standard in Java...&lt;/p&gt;
&lt;p&gt;However, you no longer need to worry with 3.0: First, the totally
useless old String object (str) has been removed (to be exact, it could
be said it is now &amp;quot;integrated&amp;quot; into the bytes and bytearray objects),
including the even more ridiculous &amp;quot;encode&amp;quot; method for old str objects:
bytes and bytearray only support a &amp;quot;decode&amp;quot; message (to the new Unicode
str objects), while the intended use of str.encode, transforming Byte
objects that were represented as str objects in pre-3.0, like zip or
base64, now has to be done through a new method called &amp;quot;translate&amp;quot; on
the new bytes and bytearray objects in 3.0, or via encode on the new str
object. This was a dangerous duck typing strategy to have str.encode in
pre-3.0: as Unicode objects can and should have this method, too, but as
you could not tell if you were calling encode on a Unicode object or a
String object (without something like writing:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
assert isinstance(my_obj, unicode)
&lt;/pre&gt;
&lt;p&gt;before every call to encode, at least), you could have been decoding
Unicode and encoding Strings - and because Python was (yes, was (!) -
see below) as &amp;quot;nice&amp;quot; as to do auto-coercion for you, without very
thorough testing libraries such a bug could go unnoticed for a long time
in pre-3.0. So, my praise to whomever was responsible for that decision!&lt;/p&gt;
&lt;p&gt;On the other hand, the unicode object is now the new str object, sans
the even more useless and dangerous &amp;quot;decode&amp;quot; functionality: the new
(Unicode) str object only supports str.encode (for cases where you want
something else than UTF-8), while str.decode is finally dropped from the
Python Standarad Library. Obviously, you might have a system that does
not want UTF-8, and encoding your Unicode str to whatever schema you
need with str.encode the whole time would be a pain; To define a
different encoding globally, Python uses your &amp;quot;coding&amp;quot; declaration in
the first lines of your program as the default encoding schema for all
your new, shiny Unicode str objects. I.e., writing:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# -*- coding: funny-arab-dialect -*-
&lt;/pre&gt;
&lt;p&gt;will be enough if you have some strange language sporting glyphs that
require characters not found in the Unicode consortium's codetables, or
you might want to set it back to ASCII (the default in pre-3.0) if you
really need to ensure nothing other than good, old &amp;quot;7-bit&amp;quot; is output by
your program. On a side note: UTF-8 is compatible with ASCII, while
UTF-16 is not; i.e., an ASCII string encoded using the UTF-8 codetable
still gives the right characters, trying this with UTF-16 encoding does
not - and a good explanation why we have still not moved to UTF-16 in
general.&lt;/p&gt;
&lt;p&gt;Finally, the really dangerous auto-coercion of Python between Strings,
Unicode representations, and Byte arrays is gone for good. Your
message's argument types must now match the receiving object's type and
comparisons between the different types always evaluate to false. This
last change might sound drastic if seen from a purely rapid prototyping
view, but everybody with some intent on not going crazy while
programming will greatly appreciate this change. The bugs and exploits
stemming from wrong (en/de-) coding, or, let's say, too much duck typing
the str and unicode objects in pre-3.0 Python (yeah, I love to put the
fault on somebody else...) are finally gone! Also, as all Strings are
now represented as Unicode str objects, you no longer need to worry if,
while comparing two str objects, they are using the same encoding -
which was another fountain of bugs in pre-3.0 Python - as any String is
internally managed as universal Unicode.&lt;/p&gt;
&lt;p&gt;What is left to say? These changes are dramatic (even if they should
have been made already long ago with 2.0), and it will take a while
until Python 3.0 will have replaced 2.7 (the final, upcoming stable 2.x
release, which will warn you about code that will break with 3.0). But
the message should be clear: the effort of converting your libraries to
the next generation of Python is more than worth it, and the 2to3
converter should help if you had your encoding/decoding correct. If not,
converting to 3.0 might help you uncover some nasty bugs you were not
even aware of! Other reasons to &amp;quot;convert&amp;quot; would be:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;no more longs, which are now ints and unlimited in size (think of
what happend when reaching maxint before...),&lt;/li&gt;
&lt;li&gt;generator/views from most operations formerly returning lists (think:
time used for creating and garbage collecting those temporary lists),&lt;/li&gt;
&lt;li&gt;function annotations for metaclassing and advanced decorators,&lt;/li&gt;
&lt;li&gt;nonlocal scope (similar to LISPs lexical scope),&lt;/li&gt;
&lt;li&gt;dictionary comprehensions (&amp;quot;{k: v for k, v in my_dict}&amp;quot;) and set
literals (&amp;quot;my_set = {1, 2}&amp;quot;),&lt;/li&gt;
&lt;li&gt;and tons of streamlining the syntax and Standard Library.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="Programming"></category><category term="unicode"></category><category term="python"></category></entry><entry><title>Amazonas 101</title><link href="https://fnl.es/amazonas-101.html" rel="alternate"></link><published>2008-04-14T00:00:00+02:00</published><updated>2008-04-14T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2008-04-14:/amazonas-101.html</id><summary type="html">&lt;p&gt;Colombia is one of the most magnificent countries I have been so far.
If you know a tiny bit of its history with all the bloody civil wars
which have been almost continuously tormenting the country since the
40s, it is more than astonishing to find that the people themselves …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Colombia is one of the most magnificent countries I have been so far.
If you know a tiny bit of its history with all the bloody civil wars
which have been almost continuously tormenting the country since the
40s, it is more than astonishing to find that the people themselves are
very open and friendly. The lush green on the countryside and the
amazing beaches on the coasts, mixed with some really high mountain
peaks allow for a variety you will not find in many other countries.
Finally, as it is next to the equator, the climate is almost all year
round the same: warm and mostly sunny (well, Bogotá is at 2,600 ASL, so
bring a warm jacket for the capital...).&lt;/p&gt;
&lt;p&gt;The current president, Uribe, is sending military forces everywhere in
the country to protect the civilians and tourists from attacks from
guerilla and paramilitary groups. Although you might not agree that this
is the best solution to the problem, you have to admit that he so far is
probably the most successful president in terms of restoring relative
security - at the great price of sacrificing the peoples personal
freedom, obviously; Colombia probably never has been as safe in the past
few decades as it is now - and it has by far less hostage-takings per
year as most uninformed people would make you believe (by now, something
below 600 per year). In my travels, I never felt more unsafe than in any
place in Europe - at least if you learn to ignore that at every second
corner you see a small group of military personnel or police.&lt;/p&gt;
&lt;p&gt;Both major cities I visited, Bogotá and Cartagena, are really nice
cities - although the crazy traffic and taxi drivers in Bogotá can make
you feel a little ill to you stomach... Especially Cartagena is really
worth the visit, the old town being built in a style slightly similar to
the houses in Extremadura (Spain) - and not without reason probably,
as most Conquistadores came from this region of Spain. Yet, the bright
colors and plenty of flowers and green give the town a very unique and
lovely look. Last but not least, Cartagena is on the Caribbean coast and
therefore you have plenty of wonderful beaches to choose to spend your
day. On a general advice: usually, you take boats to get to those
beaches, which leave from the town center. Insist, really, to be taken
there by a tiny private boat (not more than 20 or so passengers) and do
not take one of the big ships: it cuts your beach stay in half, you are
dumped at a super-touristic aquarium nobody seemed interested in anyway,
and spend the boat time with hundreds of Colombian families (including
everything from grandmother to great-grand child). Not that it was my
worst experience ever, but there are more fun ways to waste your time...&lt;/p&gt;
&lt;p&gt;The major part of our travels - with my [now (*kiss*)] girlfriend
Mayte and two of her friends from Portugal and Spain, Xana and Ivan - we
spent in the Amazons. It is hard to describe the experience of visiting
the jungle, but if you have been to a real desert, on top of high
mountains or glaciers, or other really strange places, you know what I
am talking about. The biodiversity is so amazing you will never be able
to go back to a zoo or botanic garden without a knowing smile on your
face. I have no idea how many different plants I saw - including the
Victoria amazonica, the largest lotus in the world - but when one of our
guides was telling us about which plant heals what while in the jungle,
I got the impression of walking though a pharmacy... The animal life is
just as amazing: alligators, snakes, birds in all varieties and kinds,
spiders, sloths, insects of all sizes, piranhas (we even caught our own
- but as far as I can tell, it took us more chicken meat to catch them
than they yielded...), even dolphins (which get a pink tint when they
hunt because of their circulation - a really unique view) - plus another
few dozen or so I have missed to list here.&lt;/p&gt;
&lt;p&gt;Moving around the jungle might seem tough and dangerous at first - and
I make a bet it is, if you try to move cross-country straight through
the jungle for days. We were not intending to do that - we were moving
along the Amazonas river only - and always had at least a hut to sleep
in, with beds and mosquito nets in the most &amp;quot;extreme&amp;quot; cases. Most of the
time we spent in villages and towns (Leticia -&amp;gt; Puerto Nariño [a little
jewel in the Colombian Amazons and a must visit] -&amp;gt; Leticia/Tabatinga -&amp;gt;
and Manaus) in hotels, where you do not even need to use mosquito nets.
The greatest danger is you doing something stupid on your own. The
interested might want an advice on what to bring, so I'd suggest to
pack:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;lots of insect repellent (we used Relec extra fuerte, about 1 bottle
[100 mL each] per person and week),&lt;/li&gt;
&lt;li&gt;a light rain protection (if it rains really heavy - which it does
more or less every morning - just forget protection... this is more
against the wind and on boats),&lt;/li&gt;
&lt;li&gt;some long-sleeves and long pants (protection against sun and/or while
hiking in the jungle),&lt;/li&gt;
&lt;li&gt;decent (!) hiking shoes,&lt;/li&gt;
&lt;li&gt;very strong sun protection (factor 40 or more - I used 60!),&lt;/li&gt;
&lt;li&gt;a towel (no, this is no Hitchhikers Guide joke :-) - preferentially,
one of those micro-towels to save space and weight),&lt;/li&gt;
&lt;li&gt;a flashlight,&lt;/li&gt;
&lt;li&gt;a small first aid kit with antibiotics against diarrhea, pills to
dampen fever and Malaria effects (Ibuprofen 600, Malaron - just in
case - I strongly advise against taking those Malaria pills on the
trip as prevention if you do not go jungle-trekking for many days!
The side effects are rather ugly, and actually the pills only reduce
the effects of Malaria, they do not really help prevent it.),&lt;/li&gt;
&lt;li&gt;Suero (salt &amp;amp; electrolytes) powder, and a lots of it! This protects you
from dehydrating and diarrhea; My friends all laughed at me for
drinking a liter of it a day (it tastes rather ugly), but I was the
only of our group to not have to spend a day on the pot in the end...&lt;/li&gt;
&lt;li&gt;a very light (I used silk) sleeping bag, and&lt;/li&gt;
&lt;li&gt;something to pass time (books, card games, etc. - hey, it's Colombia,
no need to be in a hurry).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On a final shout-out: many thanks to my amazing travel companions (aka
&amp;quot;los pendejos/tottos&amp;quot;...) and some very special ones to the most
gorgeous girl in the world that I am more than just happy to have become
close with during this trip: You provided me with one of the most
exciting trips I ever have made in my life!&lt;/p&gt;
</content><category term="Travelling"></category><category term="travel"></category></entry><entry><title>TextMate Python and Django cheat sheet</title><link href="https://fnl.es/textmate-python-and-django-cheat-sheet.html" rel="alternate"></link><published>2007-09-07T00:00:00+02:00</published><updated>2007-09-07T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-09-07:/textmate-python-and-django-cheat-sheet.html</id><content type="html">&lt;p&gt;After not finding anything appropriate, I decided to do my own reference
card (aka cheat sheet) for the pythonic and django commands you can use
in TextMate. If you want to have it, download it from &lt;a class="reference external" href="http://www.scribd.com/doc/7759743/TextMate-PythonDjango-Cheat-Sheet"&gt;here&lt;/a&gt;.&lt;/p&gt;
</content><category term="Programming"></category><category term="python"></category><category term="django"></category></entry><entry><title>Travelling around Spain</title><link href="https://fnl.es/travelling-around-spain.html" rel="alternate"></link><published>2007-08-19T00:00:00+02:00</published><updated>2007-08-19T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-08-19:/travelling-around-spain.html</id><summary type="html">&lt;p&gt;In the last few weeks I had been to Cadiz and Almeria, both in
the south of Spain. Well, actually I was in none of those cities, but in
places close by.&lt;/p&gt;
&lt;p&gt;The first trip was down to Cadiz, about 30 km south
of the city some of our friends …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the last few weeks I had been to Cadiz and Almeria, both in
the south of Spain. Well, actually I was in none of those cities, but in
places close by.&lt;/p&gt;
&lt;p&gt;The first trip was down to Cadiz, about 30 km south
of the city some of our friends were renting a flat in &lt;a class="reference external" href="http://maps.google.es/maps?f=q&amp;amp;hl=en&amp;amp;geocode=&amp;amp;q=Conil,+Spain&amp;amp;amp;ie=UTF8&amp;amp;z=11&amp;amp;om=1"&gt;Conil&lt;/a&gt;, a
village nearby. South of Cadiz you basically find the probably most
beautiful beaches of Spain, at least that was my impression and what my
Spanish friends told me (and, on a side note, the most incredible amount
of beautiful girls, too...). The only downside is that it can be kind of
windy, but if you know it, just bring some very stable wind protection,
and you should be fine. Actually, the one day it was windy, I was quite
happy, because with the burning sun having a swim and then cooling in
the breeze is a real relief. We also visited Tarifa, one of the main
wind- and kite surfing sites of Europe. You never saw a village so small
just full of surf shops - if you ask me, more dense than the skiing
shops in the tourist-trap villages in Tirol. Also we spent some time in
Los Caños de Meca, right between Conil and Tarifa, which is a real hippy
town and has a cool bar which is made up a bit like a ship deck and is
above a cliff looking out over the Atlantic. Great place to have some
&amp;quot;copas&amp;quot; (drinks)! On our way back to Madrid we took the time to stop in
Cordoba, which might be, together with Sevilla, the hottest (in
temperature, man!) town of Spain. In the center you can visit an
enormous (like in really enormous) mosque, as Cordoba and Granade where
interchangeably the capitals of the Arabian empire during the medieval
ages. Now it houses several (!!!) churches inside and it is really worth
paying the 8 Euros entry. My GF was slightly angry about my cultural
blasphemy, as the first thing that came to my mind upon entering was
&amp;quot;and where is Indiana Jones?&amp;quot;...&lt;/p&gt;
&lt;p&gt;The second trip to Almeria in the southeast corner of Spain was
actually for a festival, the &lt;a class="reference external" href="http://www.creamfields-andalucia.com/"&gt;Creamfields&lt;/a&gt; in Spain. This festival is
exclusively for electronic music, and the lineup were bands and DJs like
Basement Jaxx, The Prodigy, or John Digweed. It's a 14 h festival
&amp;quot;only&amp;quot;, starting at 6 pm and lasting until about 8 am the next morning.
But believe me, that was by far enough for me, and it took me several
days to recover... The greatest thing about this location is that you
have the After Hour right smack on the beach, hanging out in the sun and
taking an occasional bath to cool down. Although it was great, I am not
sure to recommend it again: This year they had 50k visitors, and the
only access is a single little road, which meant we were jamming 2 1/2 h
in the car just to get in, and then another 1/2 h walk from the parking
place to the actual festival. Considering that they have more attendees
every year, I don't want to know how bad it will be next year. So if you
feel like going, I might recommend you come a day earlier and put up
tent there on the beach (actually, better in the wood at the side of the
beach - if you can't imagine why, just do as I recommend...).&lt;/p&gt;
&lt;p&gt;So, all in all, two really gorgeous trips around Spain, and I am
looking forward to some more in the days to come this year. The only
thing I could slap my ass for is that I on both occasions forgot to
bring my camera...&lt;/p&gt;
</content><category term="Travelling"></category><category term="spain"></category><category term="travel"></category><category term="music"></category></entry><entry><title>Skiing in Spain</title><link href="https://fnl.es/skiing-in-spain.html" rel="alternate"></link><published>2007-03-27T00:00:00+02:00</published><updated>2007-03-27T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-03-27:/skiing-in-spain.html</id><summary type="html">&lt;p&gt;When I decided to move to Spain, I would never have thought of
skiing here. There are the Pyrenees, where you could expect some areas,
but I was not expecting anything anywhere else. Meanwhile, I have been
skiing in Asturias (north, center) and Sierra Nevada (south, with a view
of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When I decided to move to Spain, I would never have thought of
skiing here. There are the Pyrenees, where you could expect some areas,
but I was not expecting anything anywhere else. Meanwhile, I have been
skiing in Asturias (north, center) and Sierra Nevada (south, with a view
of Africa!)... Especially the latter was quite convincing: a large skiing
area with a lot of people who can actually ski (for Tyrolean standards
quite hard to achieve!). Just thought it would be kind of crazy to be
skiing end of March in Spain, while having a glimpse at Africa and
enjoying almost 20 ˚C in the &amp;quot;valley&amp;quot; (that is, Granada).&lt;/p&gt;
&lt;p&gt;While I am at it: Granada is quite a beautiful town, and naturally there
is the Alhambra to visit (but I did not enter: you have to reserve at
least 2 months ahead if you want to get in...). There are more foreigners
here than in Madrid, naturally, and it seems to have slightly less extreme
temperatures than the capital. Nice place to live, I guess: 1 hour for
skiing, 1 hour to the beach!&lt;/p&gt;
&lt;p&gt;In summary, there are 3 decent areas for skiing in Spain: the Pyrenees,
Asturias and Granada/Sierra Nevada. I have not seen the first, but
everybody tells me it is the best place (Update: I've been there, in
Formigal - yes, it is nice there). Anyway, I'd say Granada is
definitely the fanciest and worth a visit, too. And then you can ski on
about any other mountain range here, too (e.g., Gredos, Guadarrama, etc.),
but for my taste, the slopes there are not quite as steep and long as I'd
like them.&lt;/p&gt;
</content><category term="Travelling"></category><category term="spain"></category><category term="skiing"></category></entry></feed>