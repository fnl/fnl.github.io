<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>fnl.es</title><link href="http://fnl.es/" rel="alternate"></link><link href="http://fnl.es/feeds/florian-leitner.atom.xml" rel="self"></link><id>http://fnl.es/</id><updated>2015-04-06T00:00:00+02:00</updated><entry><title>An efficient online sequence tagger resource for GATE</title><link href="http://fnl.es/an-efficient-online-sequence-tagger-resource-for-gate.html" rel="alternate"></link><updated>2015-04-06T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2015-04-06:an-efficient-online-sequence-tagger-resource-for-gate.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; for a stressed out generation:
&lt;a class="reference external" href="https://gate.ac.uk"&gt;GATE&lt;/a&gt;'s &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch23.html#sec:parsers:taggerframework"&gt;Generic Tagger&lt;/a&gt; framework is a CREOLE plug-in that allows you to wrap any existing &lt;a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;sequence tagger&lt;/a&gt; and use it to create annotations in your pipeline, but it is very poorly implemented.
Therefore, I have created the &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework"&gt;Online Tagger&lt;/a&gt; GATE plug-in that works similarly to the Generic Tagger framework, but does not do any disk I/O for inter-process communication or launch more than one &amp;quot;singleton&amp;quot; sub-process per Processing Resource instance.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://gate.ac.uk"&gt;GATE&lt;/a&gt; (General Architecture for Text Engineering) has crystallized itself as my preferred tool for teaching text mining and information extraction.
While anybody might argue that there are leaner and faster frameworks around, it has one pretty outstanding, unique quality: it is (mostly) GUI-based.&lt;/p&gt;
&lt;p&gt;During any text mining course I teach, the most frequent question I get is what text mining software is around that can be used &lt;em&gt;without any a priori programming skills&lt;/em&gt;.
In other words, most of my audience is looking for a &amp;quot;graphical&amp;quot; text mining environment that can be used without first having to learn how to program.
For example, to use NLTK, LingPipe, OpenNLP, StandfordNLP, UIMA, etc., you will first have to learn how to program in the chosen framework's API language.
Therefore, the only entirely true answer is that only commercial tools can offer a &lt;strong&gt;pure&lt;/strong&gt; &amp;quot;graphical user interface&amp;quot; and require no programming experience.&lt;/p&gt;
&lt;p&gt;However, GATE can be used &lt;em&gt;mostly&lt;/em&gt; without having to write code - with the exception of its &amp;quot;JAPE-glue&amp;quot;.
&lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch8.html#chap:jape"&gt;JAPE&lt;/a&gt; stands for &amp;quot;Java Annotation Patterns Engine&amp;quot; and is GATE's solution to make data inter-operable between different text mining resources that commonly have different I/O requirements.
Furthermore, JAPE can be used to design entire rule-based annotation resources of their own right.
However, JAPE &amp;quot;grammars&amp;quot; consist of rules where the left-hand side of the grammatical rule matches (existing) GATE annotations using a (clear and simple) syntax, while the right-hand side of those rules can contain Java code that will somehow modify those annotations.
Therefore, GATE rids you from the need of writing code of your own, except for (small) blocks of (simple) code for the right-hand sides of JAPE's rules.
Luckily, GATE's extensive documentation provides lots of &lt;a class="reference external" href="https://gate.ac.uk/wiki/jape-repository/"&gt;examples&lt;/a&gt; to start with for the novice.&lt;/p&gt;
&lt;p&gt;Overall, this makes GATE the only free open source text mining software that provides a graphical interface &lt;em&gt;and&lt;/em&gt; requires (nearly) no programming skills to use it.
As I stated initially, this fact alone makes it the best fit for main need of my typical tutorial attendees, because most of them are neither computer scientists nor do they (want to) know how to code.&lt;/p&gt;
&lt;p&gt;As mentioned in the introduction, &amp;quot;out-of-the-box&amp;quot; GATE isn't always the fastest solution.
However, due to its open source nature that only means that if you need to go faster, you always can replace any slow pieces with whatever you consider a better fit (if you know how to program, that is...)
For example, the &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch23.html#sec:parsers:taggerframework"&gt;Generic Tagger&lt;/a&gt; framework is a &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch4.html#x7-690004"&gt;CREOLE&lt;/a&gt; &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch3.html#x6-540003.7"&gt;Processing Resource&lt;/a&gt; that allows you to take any existing &lt;a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;sequence tagger&lt;/a&gt; and use it to create annotations for a GATE pipeline.
This is pretty nifty, because you can use whatever Part-of-Speech tagger or Named Entity Recognition system you like.
You can even use a generic sequence tagger, train your own model, and integrate it in your text mining and information extraction pipelines, all without having to learn how to program first.&lt;/p&gt;
&lt;p&gt;However, precisely due to the highly generic nature of the Generic Tagger framework, it is not very efficient.
To create GATE annotations with it, this tagger &amp;quot;wrapper&amp;quot; operates as follows &lt;strong&gt;on each input&lt;/strong&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A file with the input text for the tagger is written to disk.&lt;/li&gt;
&lt;li&gt;A new tagger sub-process is launched by the wrapper, reading the input from the file.&lt;/li&gt;
&lt;li&gt;The tagger's results are written back to disk.&lt;/li&gt;
&lt;li&gt;The wrapper resource reads the result file, generates the annotations, and deletes the two temporary files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I have highlighted &amp;quot;on each input&amp;quot;, because this loop might be run for each processed document, for each sentence in each document, or, even worse, for each and every token in your documents.
If you have already thought that doing this for each document is pretty bad, doing that loop for each token grinds your pipeline to a standstill.
Second, if you are a programmer, the expression &amp;quot;a new sub-process is launched&amp;quot; in the second point should be alarming to you.
If the tagger uses some large resources, like a dictionary (which they quite frequently do), starting up a new tagger process can be extremely expensive.
In general, of all concurrent programming concepts, launching a new &lt;em&gt;process&lt;/em&gt; is the probably most expensive resource you can create &amp;quot;within&amp;quot; a program and should be done as sparingly as possible.
The reason the plug-in is designed in this peculiar way isn't because the framework was written by inexperienced programmers, however.
It is that way because due to this design, it truly generic: Most I/O formats and tagger can be handled with this wrapper.&lt;/p&gt;
&lt;p&gt;However, while the Generic Tagger is pretty cool to have on board so you can to try out &amp;quot;foreign&amp;quot; sequence taggers, the way it is implemented makes it rather useless for a &amp;quot;real&amp;quot; pipeline, i.e., beyond experimentation.
For example, just tagging all gene mentions in a few thousand &lt;a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed"&gt;PubMed&lt;/a&gt; sentences with this wrapper takes &lt;em&gt;days&lt;/em&gt;.
But PubMed has over 24 million abstracts and (I think to recall) roughly around 100 million sentences, so go figure...&lt;/p&gt;
&lt;p&gt;Therefore, I am releasing my own CREOLE processing resource that works similarly to the Generic Tagger, but does not do any disk I/O for inter-process communication or launch more than one &amp;quot;singleton&amp;quot; process for the entire pipeline you are designing.
However, this puts some restrictions on the kinds of taggers you can use:&lt;/p&gt;
&lt;p&gt;1. The tagger must support a &lt;strong&gt;streaming I/O&lt;/strong&gt; model.
That is, the tagger must be able to read from some &amp;quot;input stream&amp;quot;, such as UNIX' &lt;cite&gt;STDIN&lt;/cite&gt;, and write to some &amp;quot;output stream&amp;quot;, commonly UNIX' &lt;cite&gt;STDOUT&lt;/cite&gt;.
Another way of putting this is that your tagger should be able to handle UNIX' piped command syntax, something like this: &lt;tt class="docutils literal"&gt;cat plain_text.txt | some_tagger &amp;gt; tagged_text.txt&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;2. The tagger must work with POSIX' classical line-based interface.
That is, the tagger must take one continuous block of text as &lt;em&gt;input&lt;/em&gt;, &lt;strong&gt;terminated with a newline&lt;/strong&gt; character.
For example, it should take one token, sentences or block of text as input (not containing any newlines), and, once it receives a newline character, start tagging that input.&lt;/p&gt;
&lt;p&gt;3. The tagger must produce &lt;strong&gt;one annotation per line&lt;/strong&gt; as &lt;em&gt;output&lt;/em&gt;, and those annotations must be &lt;strong&gt;in the same order&lt;/strong&gt; as the (input) text spans which they annotate.
Those annotations commonly are expected to be in the OTPL (one token per line) format.
For example, the output line &lt;cite&gt;Nouns noun NN B-NP O&lt;/cite&gt; might annotate the token &amp;quot;Noun&amp;quot; (verbatim, as found in the input text) with the lemma &amp;quot;noun&amp;quot;, the PoS-tag &amp;quot;NN&amp;quot;, the BIO-chunk &amp;quot;B-NP&amp;quot; and the BIO-NER-tag &amp;quot;O&amp;quot; (&amp;quot;outside&amp;quot; any entity mention).&lt;/p&gt;
&lt;p&gt;If you have a tagger that follows those requirements (it turns out, most sequence taggers I know of work precisely like this), you can instead use my &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework"&gt;Online Tagger&lt;/a&gt; framework.
What it does differently to the Generic Tagger is the following:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;On Processing Resource &lt;strong&gt;initialization&lt;/strong&gt;, you have to specify the location of the tagger and GATE launches the tagger with the supplies parameters (directory where to run the tagger and any arguments, such as dictionaries to load or command-line flags to set).&lt;/li&gt;
&lt;li&gt;The Processing Resource &lt;strong&gt;configuration&lt;/strong&gt; is nearly the same, but some of the defaults have been adapted to better reflect the nature of the on-line processing model.&lt;/li&gt;
&lt;li&gt;Once your pipeline is &lt;strong&gt;running&lt;/strong&gt;, the text is piped into the tagger sub-process and results are read from the output stream, while intermediary files are no longer created.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please clone the tagger from GitHub (&lt;tt class="docutils literal"&gt;git clone &lt;span class="pre"&gt;https://github.com/fnl/OnlineTaggerFramework&lt;/span&gt;&lt;/tt&gt;) into your local CREOLE &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch3.html#x6-530003.6"&gt;user plugin directory&lt;/a&gt;.
Then you can load my plug-in from the CREOLE Plugin Manager and once you instantiate a new &lt;tt class="docutils literal"&gt;GenericOnlineTagger&lt;/tt&gt; Processing Resource, you will be asked to supply the initial configuration data to launch the tagger in its own sub-process (tagger binary path, directory to run in [if any], runtime flags and arguments [if any]).&lt;/p&gt;
&lt;p&gt;If you run into any issue using this plug-in, please consider filing a &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework/issues"&gt;bug report&lt;/a&gt; on GitHub so I can fix the problem for everybody using it.
I hope the this plug-in will make you enjoy the new-found efficiency when integrating sequence taggers into your GATE pipelines!&lt;/p&gt;
</summary><category term="text mining"></category><category term="nlp"></category><category term="Java"></category></entry><entry><title>segtok - a segmentation and tokenization library</title><link href="http://fnl.es/segtok-a-segmentation-and-tokenization-library.html" rel="alternate"></link><updated>2015-01-12T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2015-01-12:segtok-a-segmentation-and-tokenization-library.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Surprisingly, it is hard to find a good command-line tool for sentence segmentation and word tokenization that works well with European languages.
Here, I present &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;, a &lt;strong&gt;Python 2.7 and 3&lt;/strong&gt; package, API, and Unix command-line tool to remedy this shortcoming.&lt;/p&gt;
&lt;div class="section" id="text-processing-pipelines"&gt;
&lt;h2&gt;Text processing pipelines&lt;/h2&gt;
&lt;p&gt;This is the second in a series of posts that will present lean and elegant text processing tools to take you across the void from your document collection to the linguistic processing tools (&lt;em&gt;and back&lt;/em&gt;, but more on that in the future).
In the &lt;a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;last post&lt;/a&gt;, I discussed sequence taggers, as they form the entry point to natural language processing (NLP).
Today I am presenting existing tools and a little library of mine for pre-processing of Germanic and Romance language text (essentially, because I am not knowledgeable of any others...).
Text processing pipelines roughly consist of the following three steps:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Document extraction&lt;/li&gt;
&lt;li&gt;Text pre-processing&lt;/li&gt;
&lt;li&gt;Language processing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are many good tools around for document extraction, in particular Apache &lt;a class="reference external" href="http://tika.apache.org/"&gt;Tika&lt;/a&gt; is a great software library I highly recommend for this task.
(As a matter of fact, if you want to improve your programming skills and see some real-life, clean implementations of nearly all Java/GoF software patterns, have a look at the innards of Tika.)
If you want to extract data from PDFs in particular, you should be looking for your preferred tool in the huge collection of software that is based on the excellent &lt;a class="reference external" href="http://www.foolabs.com/xpdf/"&gt;xpdf&lt;/a&gt; library or even OCR libraries like &lt;a class="reference external" href="https://code.google.com/p/tesseract-ocr/"&gt;Tesseract&lt;/a&gt;.
As for language processing, there are several tools for chunking and tagging with dynamic graphical models that you can choose from, as outlined in an &lt;a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;earlier post&lt;/a&gt; of mine, and for uncovering more involved semantic relationships, dependency parsers like &lt;a class="reference external" href="https://github.com/syllog1sm/redshift"&gt;RedShift&lt;/a&gt; are available.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="overview-and-motivation"&gt;
&lt;h2&gt;Overview and motivation&lt;/h2&gt;
&lt;p&gt;However, for the intermediate pre-processing, short from cooking up your own solution, the currently best solution is to use one of the large natural language processing (NLP) frameworks.
There are a few sentence segmentation and tokenization libraries around, particularly in Perl, but they do not have the desired properties to handle more complex cases or are long &lt;a class="reference external" href="http://mailman.uib.no/public/corpora/2007-October/005429.html"&gt;forgotten&lt;/a&gt;.
The only more recent, statistics-based tool I could find is &lt;a class="reference external" href="https://code.google.com/p/splitta/"&gt;splitta&lt;/a&gt;, but development seems to have died off yet again.
If you check out its Issues page, nobody seems to be fixing the problems, it does not work with Python 3, and its command-line implementation is not really ready for text processing with Unix.
Another example is &lt;a class="reference external" href="https://www.tm-town.com/natural-language-processing"&gt;Pragmatic Segmenter&lt;/a&gt;, a rule-based segmenter written in Ruby.
But if you feed it examples with abbreviations or other occurrences of problematic issues I will discuss in this post, you will see that it performs worse than even the statistical approaches provided by the frameworks discussed next.&lt;/p&gt;
&lt;p&gt;This leaves you with &lt;a class="reference external" href="http://www.nltk.org"&gt;NLTK&lt;/a&gt;'s &lt;a class="reference external" href="http://www.nltk.org/_modules/nltk/tokenize/punkt.html"&gt;PunktTokenizer&lt;/a&gt;, &lt;a class="reference external" href="http://alias-i.com/lingpipe/"&gt;LingPipe&lt;/a&gt;'s &lt;a class="reference external" href="http://alias-i.com/lingpipe/demos/tutorial/sentences/read-me.html"&gt;SentenceModel&lt;/a&gt;, &lt;a class="reference external" href="http://opennlp.apache.org/"&gt;OpenNLP&lt;/a&gt;'s &lt;a class="reference external" href="http://opennlp.sourceforge.net/api/opennlp/tools/sentdetect/SentenceDetectorME.html"&gt;SentenceDetectorME&lt;/a&gt;, and quite a few more frameworks that have APIs to bridge that gap.
(Again, if you enjoy looking at Java [Kingdom of Nouns...] source code, check out LingPipe - while commercial, it is really well designed.)
However, such a heavy-handed approach is to break a butterfly with a wheel, and in my personal opinion/experience, none of them are doing a particularly good job at segmenting, either.
If you don't believe me (never do!), just compare their performance/results against the rule-based library I am about to present here - you will see that these statistical segmenters produce a significant number of false positives on  orthographically (mostly) correct texts.
Their strength compared to this library here, however, is language independence: the library discussed here (so far?) only works with Indo-European languages, while the mentioned segmenters from the above frameworks can be trained on nearly any language and domain.
Particularly, the unsupervised PunktTokenizer only needs sufficient text and the presence of a terminal marker to learn the segmentation rules on its own.
Similarly, if you want to parse noisy text with bad spelling (Twitter and other &amp;quot;social&amp;quot; media sources), you might be best advised to use those frameworks.
So while I do think these libraries are all great as a whole - and I would recommend any one of them - it is mildly annoying that you have to learn a framework if all you want to do is common text pre-processing.&lt;/p&gt;
&lt;p&gt;If you are analyzing corporate documents, patents, news articles, scientific texts, technical manuals, web pages, etc., that tend to have good orthography these statistical tools are not quite up to it and introduce far too many splits, at least for my taste.
This then affects your downstream language processing, because the errors made by the pre-processing will be propagated and, in the worst case, even amplified.
Therefore, text processing pipelines commonly end up doing both (2) and (3) using one such framework, but years of experience have shown me that you soon will be wanting to explore methods beyond whatever particular framework you chose offers.
That then can mean that you have to re-conceptualize all of (2) to add a different tool or even need to move to a whole new framework.
If the performance of the newly integrated framework then isn't that stellar either, it becomes disputable if it even was worth the effort.
Last but not least, this framework-based software development approach violates one of the most fundamental Unix philosophies:&lt;/p&gt;
&lt;blockquote&gt;
“A  program should do one thing and it should do it well.
Programs should handle &lt;em&gt;text streams&lt;/em&gt;, because that is a universal interface.”
(Doug McIlroy)&lt;/blockquote&gt;
&lt;p&gt;(Yes, I know, it seems this library is violating the “one thing” rule because it does two things: segmenting and tokenization.
But as you will see, the library comes with two independent scripts and APIs for each step.)&lt;/p&gt;
&lt;p&gt;The next two sections are for newcomers to this topic and explains why segmenting and tokenizing isn't that trivial as it might appear.
If you are an expert, you can skip the next two sections and read on where &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt; is introduced.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-is-sentence-segmentation-and-tokenization"&gt;
&lt;h2&gt;What is sentence segmentation and tokenization?&lt;/h2&gt;
&lt;p&gt;Nearly any text mining and/or linguistic analysis starts with a sentence and word segmentation step. That is, determining all the individual spans in a piece of text that constitute its sentences or words.
Identifying sentences is important because they form logical units of thought and represent the borders of many grammatical effects.
All our communication - statements, questions, and commands - are expressed by sentences or at least a meaningful sentence-like fragment, i.e., a phrase.
Words on the other hand are the atomic units that form the sentences, and ultimately, our language.
While words are built up from a sequence of symbols in most languages, these symbols have no semantics of their own (except for any single-symbol words in that language, naturally).
Therefore, nearly any meaningful text processing task will require the segmentation of the sequence of symbols (characters in computer lingo) into sentences and words.
These words are, at least in the context of computational text processing, often called &lt;strong&gt;tokens&lt;/strong&gt;.
Beyond the actual words consisting of letters, a token includes atomic units consisting of other symbols.
For example, the sentence terminals (., !, and ? are three such tokens), a currency symbol, or even chains of symbols (for example, the ellipsis: …).
By following through with this terminology, the process of segmenting text into these atomic units is commonly called &lt;em&gt;tokenization&lt;/em&gt; and a computer subroutine doing this segmentation is known as a &lt;em&gt;tokenizer&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sentence-segmentation-and-tokenization-is-hard"&gt;
&lt;h2&gt;Sentence segmentation and tokenization is hard&lt;/h2&gt;
&lt;p&gt;While this segmentation step might initially sound like a rather trivial problem, it turns out the rabbit hole is deep and no perfect solution has been found to date.
Furthermore, the problem is made harder by the different symbols and their usage in distinct languages.
For example, just finding word boundaries in Chinese is non-trivial, because there is not boundary marker (unlike the whitespace used by Indo-European languages).
And when looking into technical documents, the problem can grow even more out of hand.
Names of chemical compounds, web addresses, mathematical expressions, etc. are all complicating the way one would normally define a set of word boundary detection rules.
Another source of problems are texts from public internet fora, such as Twitter.
Words are not spelled as expected, the spaces might be missing, and emoticons and other ASCII-art can make defining the correct tokenization strategy a rather difficult endeavor.
Similarly, the sentence terminal marker in many Indo-European languages is the full-stop dot – which coincidentally is also used as the abbreviation marker in those languages.
For example, detecting the (right!) three sentences in in the following text fragment is not that trivial, at least for a computer program.&lt;/p&gt;
&lt;blockquote&gt;
Hello, Mr. Man. He smiled!! This, i.e. that, is it.&lt;/blockquote&gt;
&lt;p&gt;If the text fragments are large or the span contains many dots, even humans will start to make many errors when trying to identify all the sentence boundaries.
Certain proper nouns (gene names, or “amnesty international”, for example) might demand that the sentence begins with a lower case letter instead of the expected upper-case.
A simple typo might have been the cause for a sentence starting with a lower-case letter, too.
Again in public internet fora, users sometimes resort to using only lower- or upper-case for their  messages or write in an orthographically invalid mix of letter casing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="introducing-segtok-a-python-library-for-these-two-issues"&gt;
&lt;h2&gt;Introducing segtok – a Python library for these two issues&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pip3 install segtok
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The Unix approach to software (one thing only, with a text-based interface [that can be used with pipes]) allows you to integrate programs at each stage of your tool-chain and makes it simple to quickly exchange any parts.
If you use a large framework, on the other hand, you are constrained by it and might feel tempted to accept that some of the things you will be doing with it aren't done quite as efficiently as possible.
And the more you make use of that large framework, the less attractive it is to switch your tooling and move out of that &amp;quot;comfort zone&amp;quot;.
I think this issue has a direct, detrimental effect on our ability to experiment and adapt to new tools, software, and methods.&lt;/p&gt;
&lt;p&gt;Due to the many different ways this problem can be solved and the inherent complexity if considering all languages, &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;, the library presented here, is confined to processing orthographically regular Germanic (e.g., English) and Romance (e.g., Spanish) texts.
It has a strong focus on those two and German, which all use Latin letters and standard symbols (like .?!”'([{, etc.).
This is mostly based on the fact that I only know those three languages to some reasonable degree (my tourist-Italian does not count...) - while help/contributions to make segtok work in more languages would be very welcome!
Furthermore, &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt; was made to cope with text having the following properties:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Capable of (correctly!) handling the whole Unicode space.&lt;/li&gt;
&lt;li&gt;A sentence termination marker must be present.&lt;/li&gt;
&lt;li&gt;Texts that follow a mostly regular writing style - in particular, segtok is not tuned for Twitter's highly particular orthography.&lt;/li&gt;
&lt;li&gt;It can handle technical texts (containing, e.g., chemical compounds) and internet URIs (IP addresses, URLs and e-mail addresses).&lt;/li&gt;
&lt;li&gt;The tool is able to handle (valid) cases of sentences starting with a lower-case letter and correctly splits sentences enclosed by parenthesis and/or quotation marks.&lt;/li&gt;
&lt;li&gt;It is able to handle some of the more common cases of heavy abbreviation use (e.g., academic citations).&lt;/li&gt;
&lt;li&gt;It treats all &lt;em&gt;Unicode&lt;/em&gt; dashes (there are quite a few of them in Unicode land) “The Right Way” - a functionality surprisingly absent from most tools.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, the two scripts that come with segtok have a very simple plain-text, line-based interfaces that work well when joined with Unix pipe streams.
The first script, &lt;tt class="docutils literal"&gt;segmenter&lt;/tt&gt;, segments sentences in (plain) text files into one sentence per line.
The other, &lt;tt class="docutils literal"&gt;tokenizer&lt;/tt&gt;, splits tokens on single lines (usually, the sentences from the &lt;tt class="docutils literal"&gt;segmenter&lt;/tt&gt;) by adding whitespaces where necessary.
On the other hand, if you are a Python developer, you can use the functions (&amp;quot;Look Ma, no nouns!&amp;quot;...) provided by this library to incorporate this approach in your own software (the tool is MIT licensed, btw.).
Segtok is designed to handle texts with characters from the entire Unicode space, not just ASCII or Latin-1 (ISO-8859-1).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sentence-segmentation-with-segtok"&gt;
&lt;h2&gt;Sentence segmentation with segtok&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;from segtok.segmenter import split_single, split_multi
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On the sentence level, segtok can detect sentences that are contained inside brackets or quotation marks and maintains those brackets as part of the sentence;
For example:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;(A sentence in parenthesis!)&lt;/li&gt;
&lt;li&gt;Or a sentence with &amp;quot;a quote!&amp;quot;&lt;/li&gt;
&lt;li&gt;'How about handling single quotes?'&lt;/li&gt;
&lt;li&gt;[Square brackets are fine, too.]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The segmenter is constrained to only segment on single lines ( &lt;tt class="docutils literal"&gt;split_single&lt;/tt&gt; - sentences are not allowed to cross line boundaries) or to consecutive lines ( &lt;tt class="docutils literal"&gt;split_multi&lt;/tt&gt; - splitting is allowed across newlines inside “paragraphs” separated by two or more newlines).
(If you really want to extract sentences that cross consecutive newline characters, please remove those line-breaks from your text first.
Segtok assumes your content has some minimal semantical meaning, while superfluous newlines are nothing more than noise.)
It gracefully handles enumerations, dots, multiple terminals, ellipsis, and similar issues:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A. The first assumption.&lt;/li&gt;
&lt;li&gt;2. And that is two.&lt;/li&gt;
&lt;li&gt;(iii) Third things here.&lt;/li&gt;
&lt;li&gt;What the heck??!?!&lt;/li&gt;
&lt;li&gt;A terminal ellipsis...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In essence, a valid sentence terminal must be represented by one of the allowed Unicode markers.
That is, the many Unicode variants of ., ?, and !, and the ideographic full-stop: “。” (a single character).
Therefore, &lt;em&gt;this library cannot guess a sentence boundary if the marker is absent&lt;/em&gt;!
After the marker, up to one quotation mark and one bracket may be present.
Finally, the marker must be separated from the following non-space symbol by at least one whitespace character or a newline.&lt;/p&gt;
&lt;p&gt;This requires that the sentence boundaries do obey some limited amount of regularity.
But at the same time, the pesky requirement that a marker is followed by upper-case letters is absent from this strategy.
In addition, this means that “inner” abbreviation markers are never a candidate (such as in “U.S.A.”).
On the other hand, any markers that do not follow this “minimal” pattern will always result in false negatives (i.e., not be split).
While missing markers and markers not followed by a space character do occur, those cases are very infrequent in orthographically correct texts.&lt;/p&gt;
&lt;p&gt;After these &lt;em&gt;potential&lt;/em&gt; markers have been established, the method goes back and looks at the surrounding text to determine if that marker is not at a sentence boundary after all.
This step recuperates cases like initials (“A. Name”), species names (“S. pombe”) and abbreviations inside brackets, which are common with citations (“[A. Name, B. Other. Title of the work. Proc. Natl. Acad. Sci. 2010]”).
Obvious and common abbreviations (in English, Spanish, and German, so far) followed by a marker are dropped, too.
There are several other enhancements to the segmenter (e.g., checking for the presence of lower-case words that are unlikely start a sentence) that can be studied in the source code and unit tests.
In summary, while coming at a computational cost, this second check is what allows segtok to keep the number of false positive splits to an acceptable low if compared to existing methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="segtok-s-tokenization-strategy"&gt;
&lt;h2&gt;Segtok's tokenization strategy&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;from segtok.tokenizer import symbol_tokenizer, word_tokenizer, web_tokenizer
from segtok.tokenizer import split_possessive_markers, split_contractions
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The tokenization approach uses a similar approach.
First, a maximal token split is made, and then several functions wrap this basic approach, encapsulating successively more complex rules that join neighboring tokens back together based on their orthographic properties.
The basic, maximum split rule is to segment everything that is separated by spaces and then within the remaining non-space spans, split anything that is alphanumeric from any other symbols:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;a123, an &lt;span class="pre"&gt;alpha-/-beta...&lt;/span&gt;&lt;/tt&gt; → &lt;tt class="docutils literal"&gt;a123&amp;nbsp; ,&amp;nbsp; an&amp;nbsp; alpha&amp;nbsp; &lt;span class="pre"&gt;-/-&lt;/span&gt;&amp;nbsp; beta&amp;nbsp; ...&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;This functionality is provided by the &lt;tt class="docutils literal"&gt;symbol_tokenizer&lt;/tt&gt; .
Next, the non-alphanumeric &lt;em&gt;symbols&lt;/em&gt; are further analyzed to determine if they should form part of a neighboring alphanumeric &lt;em&gt;word&lt;/em&gt;.
If so, the symbols are merged back together with their alphanumeric spans.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Abbreviation markers are attached back on to the proceeding word (“Mr.”).&lt;/li&gt;
&lt;li&gt;Words with internal dots, dashes, apostrophes, and commas are joined together again (“192.168.1.0”, “Abel-Ryan's”, “a,b-symmetry”).&lt;/li&gt;
&lt;li&gt;The spaces inside a word-hyphen-spaces-word sequence are dropped.&lt;/li&gt;
&lt;li&gt;Superscript and subscript digits, optionally affixed with plus or minus, are attached to a proceeding word that is likely to be a physical unit (“m³”) or part of a chemical formula, respectively (“[Al₂(S₁O₄)₃]²⁻” → “[”, “Al₂”, “(”, “S₁O₄”, “)₃”, “]²⁻”).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This set of functionality is provided by the &lt;tt class="docutils literal"&gt;word_tokenizer&lt;/tt&gt;.
Finally, if desired, a Web-mode function will further ensure that valid e-mail addresses and URLs (including fragments and parameters, but without actual space characters) are always maintained as single tokens (&lt;tt class="docutils literal"&gt;web_tokenizer&lt;/tt&gt;).
All this ensures that while a decent amount of splitting is made, the common over-splitting of tokens is avoided.
Particularly, when processing biomedical documents, Web content, or patents, too much tokenization might have quite a significant negative impact on any subsequent, more advanced processing techniques.
As before with the segmenter, I believe this recovery of false positives is the particular strength of this library.&lt;/p&gt;
&lt;p&gt;After the tokenization step, the API provides two functions to optionally split off English possessive markers (“Fred's”, “Argus'”) and even contractions (“isn't” → “is n't” [note the attachment of the letter n], “he'll” → “he 'll”, “I've” → “I 've”, etc.) as their own tokens, which can be useful for downstream linguistic parsing (&lt;tt class="docutils literal"&gt;split_possessive_markers&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;split_contractions&lt;/tt&gt;).
To use them, just wrap your tokenizer with the preferred method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def SimpleTokenizer(text):
    for sentence in split_multi(text):
        for token in split_contractions(word_tokenizer(sentence)):
            yield token
        yield None  # None to signal sentence terminals


# An even shorter usage example:
my_tokens = [split_contractions(word_tokenizer(sentence)) for
             sentence in split_multi(my_text)]
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="feedback-and-conclusions"&gt;
&lt;h2&gt;Feedback and Conclusions&lt;/h2&gt;
&lt;p&gt;All this functionality and the API itself are briefly documented on &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;'s &amp;quot;homepage&amp;quot;.
As there is not very much functionality around, I hope that between this guide here and the overview there, the library should be fairly easy to use.
Furthermore, in command-line mode, using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--help&lt;/span&gt;&lt;/tt&gt; option will explain you all options provided by the two scripts the PyPI package installs.&lt;/p&gt;
&lt;p&gt;If you are be looking for new features, you are welcome to extend the library or request a new feature on the tool's GitHub &lt;a class="reference external" href="https://github.com/fnl/segtok/issues"&gt;Issues&lt;/a&gt; page (no guarantees, though... ;-)).
As a forum for discussing this tool, please use this &lt;a class="reference external" href="http://www.reddit.com/r/Python/comments/2sala9/segtok_a_rulebased_sentence_segmenter_and_word/"&gt;Reddit&lt;/a&gt; thread.
In addition, if you use this library and run into any problems, I would be glad to receive bug reports there, too.
Overall, I have attempted to keep the strategy used by segtok as slim as possible.
So if you are using any heavy language processing or sequence analysis tools after segtok, it should have no impact on your throughput at all.&lt;/p&gt;
&lt;p&gt;I have created this library after being disappointed by the other approaches in the wild, and for regular texts, my experience is that it works substantially superior in at least one of segmentation capabilities and/or runtime performance.
As I do not wish to bash any existing tool, I will only name one sentence segmentation approach I like very much: Punkt Tokenizer by Kiss and Strunk, 2006.
PT is a unsupervised, statistical approach to segmentation that “learns” whether to split sentences at sentence terminal markers.
While quite impressive and very versatile due to its unsupervised nature, I can state clearly that segtok's segmenter works substantially better on Germanic and Romance texts that (mostly) have a proper orthography.
Unsurprisingly, segtok's sentence segmenter is substantially faster than a comparable Python &lt;a class="reference external" href="http://www.nltk.org/_modules/nltk/tokenize/punkt.html"&gt;implementation&lt;/a&gt; of the Punkt Tokenizer by NLTK.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="text mining"></category><category term="nlp"></category><category term="Python"></category></entry><entry><title>A review of sparse sequence taggers</title><link href="http://fnl.es/a-review-of-sparse-sequence-taggers.html" rel="alternate"></link><updated>2014-10-02T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2014-10-02:a-review-of-sparse-sequence-taggers.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Right now, use &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt; &lt;em&gt;unless&lt;/em&gt; you want to go beyond first-order and/or linear models, need the fastest possible training cycles, or are a Scala programmer, in which case you would be best advised to choose &lt;a class="reference external" href="http://factorie.cs.umass.edu/"&gt;Factorie&lt;/a&gt;.
OK, so that's that for a stressed out generation;
Read on if you want to know why I recommend those two tools.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Overview:&lt;/strong&gt;
The goal of this review is to identify the &amp;quot;best&amp;quot; generic CRF- or MEMM-based sequence tagger software with a free (MIT/BSD-like) license.
We will only take discriminative models into account, so if your beef are generative models and/or non-sparse data (e.g., HMMs), you have come to the wrong place.
This article will look into their abilities to define and generate features, training times, tagging throughput, and tagging performance by way of working through a common sequence labeling problem: tagging the parts-of-speech of natural language.
While PoS tagging can be considered a &amp;quot;solved&amp;quot; problem, PoS tagging performance differences are still a source of &lt;a class="reference external" href="http://aclweb.org/aclwiki/index.php?title=POS_Tagging_%28State_of_the_art%29"&gt;academic&lt;/a&gt; controversy and therefore an ideal testing ground.&lt;/p&gt;
&lt;p&gt;Nearly any interesting natural language processing (NLP) task starts with word &lt;a class="reference external" href="http://text-processing.com/demo/tag/"&gt;tagging&lt;/a&gt;:
That is, resolving each word's grammatical sense - it's &lt;em&gt;part-of-speech&lt;/em&gt; (&lt;strong&gt;PoS&lt;/strong&gt;), the phrases they group into, and the semantic meaning the words carry.
PoS refers to a word's morphology, e.g., if it is used a noun or an adjective, or the inflection of the verb.
Words of the same morphology can often be grouped (we say, &lt;strong&gt;chunked&lt;/strong&gt;) into phrases, such as &amp;quot;a noun phrase&amp;quot;.
As for the semantic meaning, text miners are usually interested in identifying the relevant entity class/type (such as person, location, date/time, ...) the word refers to.
Furthermore, in NLP, words are commonly called &lt;strong&gt;tokens&lt;/strong&gt; to use a name that also covers symbols and numbers, including dots, brackets, or commas.
These tokens form the atomic sequence units for many statistical NLP methods.&lt;/p&gt;
&lt;p&gt;Similarly, in bioinformatics, you might want to identify properties of biological sequences, e.g., DNA binding sites or predict locations of post-translational modifications in proteins.
In general, any sequence tagger could be used to classify elements in any kind of sequence you can split into discrete units.
Therefore, in bioinformatics, those units might be nucleic acids (DNA bases) or amino acids (proteins).
However, the devil lays in the details:
The implementations I am interested in here are for &amp;quot;information-sparse sequences&amp;quot;, such as text.
The difference is the element (and resulting feature) sparsity: while there are easily 20,000 different tokens contained in any average text collection (such as a book), there are only four DNA bases and twenty-something amino acids (depending on the species).
All amino acids and bases will be - compared to text tokens, at least - frequently used in their respective sequences, with the obviously lowest sparsity for the four &lt;em&gt;standard&lt;/em&gt; DNA bases (long story hidden here...).
So to define some scope of this review, I am interested in learning patterns from extremely sparse data;
If your data is &amp;quot;dense&amp;quot;, you might be better off looking into more general algorithms, such as hidden Markov models (HMMs).&lt;/p&gt;
&lt;p&gt;You can play around with a few example NLP taggers by following the &lt;a class="reference external" href="http://text-processing.com/demo/tag/"&gt;tagging&lt;/a&gt; link, and you will see that depending on the system and its training data, results can vary widely.
This is due to implementation details, graphical model capabilities, and the sequence features used by the particular model instance.
The common approach to this kind of problem is learning a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Discriminative_model"&gt;discriminative&lt;/a&gt;, dynamic graphical model; commonly, a maximum entropy (MaxEnt, aka. logistic regression) based decision function worked into a reverse &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Markov_model"&gt;Markov model&lt;/a&gt; or into the more complex Markov random field, in this particular case called a &lt;em&gt;Conditional Random Field&lt;/em&gt; (&lt;strong&gt;CRF&lt;/strong&gt;).
In the former &lt;em&gt;MaxEnt Markov model&lt;/em&gt; (&lt;strong&gt;MEMM&lt;/strong&gt;) scenario, you are only allowed to use the current state (element in your sequence, including any meta-data assigned to that element) and the last tag(s) to predict the current tag (aka. label).
The latter CRF model allows you to integrate features not just from the current state, but from any state in the sequence being labeled.
So with CRFs you can even use states from the &amp;quot;future&amp;quot; (i.e., elements later in the sequence that the one currently being tagged) to predict the current label (aka. tag).&lt;/p&gt;
&lt;p&gt;While outside the scope of this article, in case you are now asking yourself &amp;quot;Why do I then even want a MEMM instead of a CRF?&amp;quot;:
Defenders of MEMMs claim that &amp;quot;their&amp;quot; model has an edge because it does not tend to overfit the data (we say, it has a weaker &amp;quot;domain adaptation&amp;quot;) as easily as a CRF can (due to the way features can be generated from any part of the sequence) and therefore produces better tagging results on input that is not similar to (of a &amp;quot;different domain&amp;quot; than) the training data.
Second, due to its feature selection limitation, training of a MEMM tends to be faster than training a CRF with &amp;quot;long-range&amp;quot; features from distant positions in the sequence.&lt;/p&gt;
&lt;p&gt;Both regular Markov models and random fields use the notion of &lt;em&gt;Markov order&lt;/em&gt;.
That is, the number of former (already assigned) labels in a linear chain (sequence) that may be used to calculate the probabilities of each possible label on the current state.
To be more precise, it is the &lt;em&gt;transition probability&lt;/em&gt; from one (first order) or more (second, third, ... order) former labels to the label on the current state that is the statistic being modeled.
This &amp;quot;&lt;tt class="docutils literal"&gt;n&lt;/tt&gt;th-order Markov&amp;quot; limit defines how many prior tags the model will consider when tagging the current state:
For first order Markov chains you only can make use of the last tag, for second order models you get to use the last two tags, and so forth.
Due to the expense of going beyond first-order models, all but one tool we will be looking at do not support more than first-order models:
Linear chain models scale exponentially in the Markov order + 1, meaning that a second order model already has &amp;quot;number of labels&amp;quot;-cubed possible label transitions.
Going beyond second-order Markov models is only desirable if the number of labels and (as we will see) sometimes even states is small (i.e., &amp;quot;dense data&amp;quot;).
In language processing, these states normally are all the unique, observed tokens, also known as the &lt;em&gt;vocabulary&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A quick, shameless self-plug: if you are not familiar with these concepts, have a look at my &lt;a class="reference external" href="http://fnl.es/an-introduction-to-statistical-text-mining.html"&gt;introduction to text mining&lt;/a&gt; slides - or come visit my class in the context of the Madrid summer school on &lt;a class="reference external" href="http://www.dia.fi.upm.es/?q=es/ASDM"&gt;Advanced Statistics and Data Mining&lt;/a&gt; next summer (beginning of July)! The lecture will take you from basic Bayesian statistics all the way to the dynamic, graphical models being discussed here.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sequence-tagger-selection"&gt;
&lt;h2&gt;Sequence tagger selection&lt;/h2&gt;
&lt;p&gt;Recently, I have become anxious to once and for all resolve my doubts about the &amp;quot;best&amp;quot; sparse sequence tagger in terms of ease of use (documentation/UI), feature modeling capabilities, training times, tagging throughput (tokens/second) and the resulting accuracy.
All tools use the same optimization procedures for the learning process, that is a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Limited-memory_BFGS"&gt;L-BFGS&lt;/a&gt; optimizer and a few light-weight gradient descent implementations as alternatives.
However, implementation details, the graphical model abilities, the features the system can work with, the facilities it provides to generate them, the system's throughput, the provided documentation, and its availability (both in open source and free software terms) varies greatly between libraries.
Available software for this task that I considered were &lt;a class="reference external" href="http://crfpp.sourceforge.net/"&gt;CRF++&lt;/a&gt; (Kudo), &lt;a class="reference external" href="http://www.chokkan.org/software/crfsuite/"&gt;CRF Suite&lt;/a&gt; (Okazaki), &lt;a class="reference external" href="http://factorie.cs.umass.edu/"&gt;Factorie&lt;/a&gt; (McCallum &amp;amp;al.), &lt;a class="reference external" href="http://flexcrfs.sourceforge.net/"&gt;FlexCRFs&lt;/a&gt; (Phan, Nguyen &amp;amp; Nguyen), &lt;a class="reference external" href="http://alias-i.com/lingpipe/index.html"&gt;LingPipe&lt;/a&gt; (Carpenter &amp;amp;al.), &lt;a class="reference external" href="http://mallet.cs.umass.edu/"&gt;MALLET&lt;/a&gt; (McCallum &amp;amp;al.), &lt;a class="reference external" href="http://www.umiacs.umd.edu/~hal/megam/"&gt;MEGAM&lt;/a&gt; (Daume III), [Apache] &lt;a class="reference external" href="http://opennlp.apache.org/"&gt;OpenNLP&lt;/a&gt; (Kottmann &amp;amp;al.), &lt;a class="reference external" href="http://nlp.stanford.edu/software/tagger.shtml"&gt;Stanford Tagger&lt;/a&gt; (Manning, Jurafsky &amp;amp; Liang), &lt;a class="reference external" href="http://www.lsi.upc.edu/~nlp/SVMTool/"&gt;SVM Tool&lt;/a&gt; (Giménez &amp;amp; Marquez), &lt;a class="reference external" href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"&gt;TreeTagger&lt;/a&gt; (Schmidt), and &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt; (Lavergne).
There are more options &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Conditional_random_field#Software"&gt;around&lt;/a&gt;, particular in C# and for the .Net platform, but as I do not have the money to pay for the Windows tax, I did not consider them.
If you know of a relevant, &lt;em&gt;generic&lt;/em&gt; sparse sequence tagger implementation I missed (see my filtering criteria below), please &lt;a class="reference external" href="http://fnl.es/pages/about.html"&gt;contact me&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I immediately discarded the Stanford Tagger and the SVM Tool, because they are both orders of magnitude &lt;a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf"&gt;slower&lt;/a&gt; than most other tools considered (and the same goes for MALLET, &lt;a class="reference external" href="http://www.chokkan.org/software/crfsuite/benchmark.html"&gt;too&lt;/a&gt;).
It is worth mentioning that the Stanford Tagger was one of the earliest tools with the software made available for research and a very high accuracy, and as such usually serves as the performance &amp;quot;baseline&amp;quot; for newcomers.
Second, CRF Suite claims to be the fastest first order CRF around and &lt;a class="reference external" href="http://www.chokkan.org/software/crfsuite/benchmark.html"&gt;demonstrates&lt;/a&gt; that CRF++ is significantly slower, which lead me to discard the latter.
That same benchmark claims that CRF Suite is faster than Wapiti, but not only has Lavergne developed several newer versions since then, the difference is far less pronounced, so that tool was not out of the race for me.
Being a free software advocate in the sense of all its aspects - cost, freedom of usage, modifiability, and open source - I feel very uncomfortable about using software with a license that tries to restrict my freedom, including its commercial application.
Therefore, I discarded FlexCRFs, LingPipe, MEGAM, and TreeTagger from the list because of their non-free nature (only being &amp;quot;free for research&amp;quot;, or GPL'ed).
While the GPL is not strictly out of my scope, it creates too many headaches for too many use-cases because it still poses usage restrictions (that I nonetheless support as a necessary evil given the overall copyright SNAFU).
Moreover, excluding the GPL only affects FlexCRFs, which anyways is very similar to CRF++ or CRF Suite.
Two of the already discarded tools would also not make it across this &amp;quot;free software barrier&amp;quot;, by the way (Stanford Tagger and SVM Tool).&lt;/p&gt;
&lt;p&gt;So this left me with CRF Suite, Factorie, OpenNLP, and Wapiti to compare against each other.
Given these harsh pre-filtering criteria, to be honest, I was astonished that I was left with not just one, but four viable and completely free &amp;quot;tools of the trade&amp;quot;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="implementation-considerations"&gt;
&lt;h2&gt;Implementation considerations&lt;/h2&gt;
&lt;p&gt;CRF Suite and Wapiti are both written in C, while Factorie is being coded in Scala, and OpenNLP is based on Java.
So this makes for yet another classical &amp;quot;binary, platform-specific code versus the Java Virtual Machine&amp;quot; comparison!
(Spoiler alert: it does not matter - as you should know already...)
But there a real, noteworthy differences between the taggers; starting with the implemented graphical models and optimization procedures:
Factorie's PoS tagger implementation only makes use of a forward learning procedure, while the other three use the more common (and more expensive) &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm"&gt;forward-backward optimization&lt;/a&gt; approach during training.
So this difference makes up for an interesting test: will forward learning alone be good enough in terms of accuracy, and if so, how much faster will training be?
(You could also read a &lt;a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf"&gt;paper&lt;/a&gt; about this...)
Furthermore, Factorie is a library that allows you to design &lt;em&gt;any kind&lt;/em&gt; of graphical models from basic factor classes (let that sink in if you know what I mean...), so it actually can represent any model you want (including non-linear models).
For the PoS tagging, Factorie uses this forward-only learning approach to maximize a first-order linear-chain CRF.
Next up, Wapiti allows you to choose between a (non-dynamic, pure) MaxEnt model, a MEMM or a CRF model.
Similar to the above &lt;a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf"&gt;paper&lt;/a&gt; linked to, Wapiti can even do dynamic model selection, falling back on simpler models where feasible in the sequence
(Note that Factorie's PoS tagger does not use dynamic model selection.)
Finally, OpenNLP only provides a MEMM implementation, while CRF Suite only provides a CRF.
These implementation details alone might be enough to make your decision:
If you want more than a first-order linear-chain model (say, second-order, or a non-linear graph), your only choice is Factorie.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="software-state-and-documentation"&gt;
&lt;h2&gt;Software state and documentation&lt;/h2&gt;
&lt;p&gt;First, a quick look at the code, implementation, the documentation, and each tool's multi-processor capabilities.
Two remarkable things about &lt;strong&gt;Wapiti&lt;/strong&gt; are how simple and lean the interface is, and its capability of running in multi-threaded mode.
While code is the typical, long spaghetti-code of C, it is clean and very well documented.
The only main downside is that the documentation is a bit sparse; Everything is in there, but they could have done a bit better detailing some of the capabilities, and/or providing examples.
I had to figure out myself that you always need to use both the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-c&lt;/span&gt; &lt;span class="pre"&gt;-s&lt;/span&gt;&lt;/tt&gt; switches when doing (feature-sparse) text labeling and it took me some time to understand how to do feature extraction (designing &amp;quot;patterns&amp;quot;).
The Wapiti authors do not provide a default set of feature pattern templates, only a few pre-trained PoS models for English, German, and Arabic newswire text.&lt;/p&gt;
&lt;p&gt;Similarly, &lt;strong&gt;Factorie&lt;/strong&gt; is able to run in multi-threaded mode and claims to be scalable across machines for hyper-parameter search (which I have not tried).
To use Factorie, I often have to refer back to the code-base, because few of the specifics of Factorie are entirely documented for now.
This means, to use Factorie, you should better know some Scala, as it might be tough having to go through the code otherwise.
In my opinion, some parts of the codebase could have been coded just as well in Java, but that only affects few regions of the library.
One should also note that this opinion is purely subjective and might be of little relevance, so you might be better of judging for yourself.
Finally, the documentation certainly assumes you are an expert for graphical models with plenty of background knowledge in that domain.
Given its state and direction, in comparison to the other tools here, it is probably safe to judge that this library is targeted at the probabilistic programming crack with a background on graphical models, not someone looking for a quick and dirty sequence tagging solution.
The main up-side is that, together with OpenNLP, this is the only library offering a full NLP pipeline (segmentation, tokenization, tagging, and parsing).
But again, except for PoS tagging, the other NLP functionality has to be deduced from the code, as there is not much more documentation on the NLP pipeline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRF Suite&lt;/strong&gt; comes with a nice interface, and although it does not support multi-threading, the C code is well written and very clear.
The functions are short and precise, so it is a nice example how C code can look if you put some effort into it.
The only code-wise downside I detected was the accompanying Python code for preparing and pruning data and benchmarking.
It is clearly written by a C expert with little knowledge of idiomatic Python, no offense (see the performance issues for feature extraction below).
However, I think this is a minor issue given the good documentation and high-quality C code, which in the end is the part that matters.
The worst issue with CRF Suite, however, seems to be that the original author has stopped maintaining the code.
The repository on GitHub has a handful of very good pull requests from serious developers that fix things like a minor memory leak and two or three other issues, but the author has never accepted the requests.
Neither have there been any updates to the library.
To me this means that CRF Suite development seems dead and it would have to be significantly better than the other tools here to make it worth using it.&lt;/p&gt;
&lt;p&gt;Finally, &lt;strong&gt;OpenNLP&lt;/strong&gt; has the expected high quality code found in Apache projects, and it is well documented, too.
The only downsides worthwhile mentioning are that there is no built-in parallel processing support and that it only comes with a MaxEnt Markov model.
As mentioned already, it is important to realize that OpenNLP offers a full NLP pipeline, unlike CRF Suite and Wapiti.&lt;/p&gt;
&lt;p&gt;To summarize, unique capabilities of Wapiti are its built-in template-based feature extraction mechanism and its ability to quickly choose either CRF or MEMM as the target model (more on this below).
A unique capability of Factorie is that it provides you with the necessary base classes to quickly code your own graphical models of any Markov order, both linear and non-linear.
However, admittedly, both Wapiti and Factorie are behind CRF Suite and OpenNLP in terms of documentation and in my opinion, Factorie is not consistently using idiomatic Scala, probably due to its many different developers.
Finally, both OpenNLP and Factorie include a full, documented and undocumented (respectively) NLP pipeline.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="feature-modeling"&gt;
&lt;h2&gt;Feature modeling&lt;/h2&gt;
&lt;p&gt;This leads to the next important consideration: modeling features; for example, via templates (or &amp;quot;patterns&amp;quot;) that define the features used by a model's binary indicator functions.
For example, an indicator function for assigning the PoS tag &amp;quot;VBZ&amp;quot; might be triggered when observing the bigram &amp;quot;I went&amp;quot; and having already assigned the PoS label &amp;quot;PRP&amp;quot; to the token &amp;quot;I&amp;quot;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
f(last_tag, current_tag, states, pos) =
    last_tag == &amp;quot;PRP&amp;quot; &amp;amp;&amp;amp; current_tag == &amp;quot;VBZ&amp;quot; &amp;amp;&amp;amp;
    states[pos-1] == &amp;quot;I&amp;quot; &amp;amp;&amp;amp; states[pos] == &amp;quot;went&amp;quot;
&lt;/pre&gt;
&lt;p&gt;This example is called a &amp;quot;combined (bigram) transition/label and state/token feature&amp;quot;
(And would be minimally encoded as &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;b:%x[-1,0]/%x[0,0]&lt;/span&gt;&lt;/tt&gt; in Wapiti's pattern template language.)
It is a rather &amp;quot;expensive&amp;quot; feature template, as it can easily lead to millions of individual indicator functions:
The number of indicator functions created from this template will be the squared number of PoS tags (label transitions or &amp;quot;bigrams&amp;quot;) times the number of unique token bigrams in the whole training data.
In general, for any template - even a constant one - there will be at least as many indicator functions as there are different tags.&lt;/p&gt;
&lt;p&gt;Except for Wapiti, all taggers come with a pre-defined set of feature templates for common NLP tasks.
Depending on your requirements, this might be either very practical or practically useless, particularly for domain-specific language (tweets, for example) and/or NER tagging.
For NER tagging, your entities might have unique morphological or orthographic properties;
For example, gene names might be used not just as nouns, but as adjectives, too (as in &amp;quot;p53-activated DNA repair&amp;quot;) and contain Roman or Arab numbers, Greek symbols, non-standard dashes and a few other orthographic surprises.
In addition, the entity tag might depend on &amp;quot;knowing the future&amp;quot;, such as the up-coming head token of the noun phrase currently being tagged (e.g., the head &amp;quot;gene&amp;quot; in &amp;quot;the ABC transporter gene&amp;quot; when looking at the token &amp;quot;ABC&amp;quot;).&lt;/p&gt;
&lt;p&gt;The predefined &lt;strong&gt;Factorie&lt;/strong&gt; features are, however, pretty good - so rich indeed, that they are more complete than any other set of features I used or provided myself in the experiments here (see &lt;tt class="docutils literal"&gt;FowardPosTagger.scala&lt;/tt&gt;, &lt;a class="reference external" href="https://github.com/factorie/factorie/blob/master/src/main/scala/cc/factorie/app/nlp/pos/ForwardPosTagger.scala"&gt;features&lt;/a&gt;, for PoS tagging).
That means training could be slow for Factorie, because it needs to optimize over a much larger indicator (feature) function space (turns out it is not, as we will see).
As with all systems except for Wapiti, this means you need to do some coding of your own to adapt the features for NER, while it might or might not be necessary for PoS tagging and phrase chunking tasks.
The real issue with Factorie is figuring out how to define your own NER tagger, as the documentation so far only covers PoS taggers.
More generally speaking, the documentation on generating features and models for your taggers is rather thin in terms of &amp;quot;applied&amp;quot; examples (see User Guide - Learning).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OpenNLP&lt;/strong&gt;, too, comes with a pre-selected list of feature templates for standard NLP tasks.
To change this list, you either have to write your own Java code or, at least for NER tagging, the documentation states that you can conjure up XML configuration files to extract different features.
As I am allergic to any use of XML other than its intended use-case - providing structure to unstructured data - and particularly against the use of XML as a vehicle for configuration files (hello Java/Maven world!), I did not even try this path.
In other words, for OpenNLP I will be using the predefined features and have not experimented with the &amp;quot;XML feature configuration&amp;quot; option, so I cannot tell how well it works or how easy it is to use.
As for PoS tagging, OpenNLP uses pretty much the &lt;em&gt;de facto&lt;/em&gt; standard features (prefixes, suffixes, orthographic features, and a window size of 5 [-2,+2] for the n-grams; see the &lt;tt class="docutils literal"&gt;DefaultPOSContextGenerator&lt;/tt&gt; &lt;a class="reference external" href="http://svn.apache.org/viewvc/opennlp/trunk/opennlp-tools/src/main/java/opennlp/tools/postag/DefaultPOSContextGenerator.java?view=markup"&gt;class&lt;/a&gt;), so that seemed good enough for this test.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRF Suite&lt;/strong&gt; comes with a set of Python scripts to convert simple OWPL files (one word [state] per line, with sentences [sequences] separated by an extra empty line, such as the CoNLL format) into the &amp;quot;per-label feature list files&amp;quot; CRF Suite uses as input.
To create different features, you modify the &amp;quot;template&amp;quot; defined inside the relevant Python script.
For most cases, I think the predefined templates do a pretty good job at generating features for standard NLP tagging tasks.
Additional features uniquely generated by CRF Suite and not OpenNLP or Factorie (for PoS tagging) are quadrigrams, pentagrams, and &amp;quot;long-range interactions&amp;quot;.
The latter are bigrams created from the current word and a word at position +/- 2 to 9 from the current word.
If you commonly work with Python, you might even easily assimilate the Python feature generation process, adapting it to your own needs.
CRF Suite's feature handling has an important shortcoming, however:
It is impossible to work with combined &amp;quot;label bigrams&amp;quot; (1st order Markov transition features) together with other (state) features from the token stream to form more advanced indicator functions.
That is, CRF Suite only models either label transition probabilities or the features from the current state, but does not allow you to create &amp;quot;mixed&amp;quot; indicator functions as described in the beginning of this section (&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;b:%s[-1,0]/%x[0,0]&lt;/span&gt;&lt;/tt&gt;).
This is an important conceptual shortcoming, because it is not possible to define features that condition on both the previous label (i.e., the transition) and the current token.
However, as opposed to the other tools here (that only work with discrete features), CRF Suite provides support for continuos features.
For example, when using &lt;a class="reference external" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"&gt;word embedding&lt;/a&gt; techniques, you might want to directly include the numeric word vectors, which you can simply pass on to CRF Suite.
While this shortcoming is commonly is circumvented by discretizing the real-valued features if continuos feature support is unavailable, it is a noteworthy difference.&lt;/p&gt;
&lt;p&gt;As for &lt;strong&gt;Wapiti&lt;/strong&gt;, you have to figure out how to generate features for each task on your own;
The authors do not provide any predefined templates for &amp;quot;standard&amp;quot; NLP tasks.
But Wapiti provides you with a mechanism to define &amp;quot;patterns&amp;quot;, much like CRF++' feature templates.
Once you fully understand the mechanism, this is indeed quite powerful and it felt like I &amp;quot;missed&amp;quot; it in the other tools.
Particularly, this means that feature extraction is done in C, so it will beat a script-based extraction process, while not requiring any C programming knowledge.
To provide an even playing field, at first I defined the same features &amp;quot;patterns&amp;quot; as CRF Suite does via its Python feature generation scripts.
The problem is that Wapiti uses all possible label and state combinations for its initial training matrix, not just all combinations present in the data.
In other words, it is the only tool that does no feature space reduction &lt;em&gt;prior&lt;/em&gt; to going into training.
For example, if you define a pattern such as the current token (&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;u:%[0,0]&lt;/span&gt;&lt;/tt&gt;), it creates one feature for each label in you training set times the number of unique tokens in your data, no matter if a token is observed with that label in the data or not.
So for token n-grams or label bigrams, the training matrix can quickly grow to extraordinary sizes.
It is worth noting that you can use your own feature extraction method and just feed Wapiti with extracted features directly (i.e., strings that start with &amp;quot;a&amp;quot; or &amp;quot;b&amp;quot;, depending on whether you want to use only the current state or integrate the transition, too).
The advantage - I assume, at least - is that the optimizer might decide to transfer some probability mass to those zero observations.
However, it is not clear to me if or how much performance Wapiti gains from such transfers, particularly when contrasting this unique feature with the greatly increased space penalty:
While Wapiti does compacting and supports sparse matrices during training, as it initially starts of with all features, training becomes rather sluggish when very large feature spaces are defined.
By using the same feature templates as CRF Suite, I ended up with an initial matrix containing a few hundred million features.
This simply was too much for my weak dual-core i5 processor to handle in realistic time.
In the end, I decided to cut down on the number of PoS feature templates with respect to CRF Suite or Factorie.
In particular, I removed the feature templates only CRF Suite has otherwise, thereby reducing the initial setup to 44 million &amp;quot;features&amp;quot; (indicator functions).
After compacting, Wapiti's final model contained 1.8 million indicator functions (&amp;quot;features functions&amp;quot;, or worse, sometimes just called &amp;quot;features&amp;quot;) for the PoS tagging trials (see below).
As should be noted, that reduced set was enough to out-compete all but Factorie in terms of accuracy while using each tool's default parameter settings.&lt;/p&gt;
&lt;p&gt;In summary, with OpenNLP, Factorie, and CRF Suite you will need to work with their respective feature generation API (in Java, Scala, and Python, respectively) to model features beyond anything but newswire PoS tagging, phrase chunking, and some basic NER.
CRF Suite, similar to Factorie, has rich, pre-defined feature templates and can handle them, because unused indicator functions are dropped before the actual training starts, thereby keeping the initial feature weight matrix manageable.
In addition, it is the only tool in this review that can handle real-valued features.
Pre-training feature (space) &lt;em&gt;reduction&lt;/em&gt; is done by all tools except Wapiti.
Feature &lt;em&gt;compaction&lt;/em&gt; after training is done by all tools except OpenNLP, where I could not confirm if any compaction had occurred.
Wapiti provides a very powerful pattern language to define feature templates, including mixed state and transition label templates, which are otherwise only possible to generate with Factorie.
While Wapiti's template (pattern) language lends to a great flexibility when modeling features, it has to be used with care unless training times are not an issue due to the maximal (non-reduced) feature matrix used during the first training cycles.
At the end of the day, in terms of feature generation, once you learn how to use Wapiti's pattern &amp;quot;language&amp;quot;, it will be very efficient and spares you from writing code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="training-time"&gt;
&lt;h2&gt;Training time&lt;/h2&gt;
&lt;p&gt;Next, I looked into the training run-times to see how long each tool takes to create a PoS model.
To make an equal, but simple comparison, I used the &lt;a class="reference external" href="http://www.cnts.ua.ac.be/conll2000/chunking/"&gt;CoNLL 2000&lt;/a&gt; PoS tags to train the models using the default feature templates as discussed in the last section.
Both Factorie and OpenNLP needed slight, but simple modifications to the downloaded CoNLL files.
For Factorie, the reversed parenthesis tags in the CoNLL files had to be fixed.
The main observation here is that Factorie is not very helpful in terms of error messages to understand the problem;
It just throws some obscure exception at you.
This means you will have to figure out what went wrong when you get errors on your own.
OpenNLP's problem was simpler to identify: as documented, it expects one sentence per line, with token-tag pairs separated by underscores instead of the de facto standard OWPL format.&lt;/p&gt;
&lt;p&gt;To train the the taggers for &lt;em&gt;Part-of-Speech&lt;/em&gt;, the commands I used were:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# CRF Suite
crfsuite learn -m pos.model train.txt

# Factorie
java -cp factorie-1.1-SNAPSHOT-nlp-jar-with-dependencies.jar \
     cc.factorie.app.nlp.pos.ForwardPosTrainer -Xmx2g \
     --owpl --train-file=train.txt --test-file=empty.txt \
     --model=pos.model --save-model=true

# OpenNLP
opennlp POSTaggerTrainer -type maxent -model pos.model -data train.txt

# Wapiti
wapiti train -c -s -p patterns.txt train.txt pos.model
&lt;/pre&gt;
&lt;p&gt;The input data is provided in &lt;tt class="docutils literal"&gt;train.txt&lt;/tt&gt;, and the models are saved to &lt;tt class="docutils literal"&gt;pos.model&lt;/tt&gt;.
To measure the training times, I prefixed each command with &lt;tt class="docutils literal"&gt;time&lt;/tt&gt; and used &lt;a class="reference external" href="http://stackoverflow.com/a/556411"&gt;the sum of user+sys&lt;/a&gt; as the measured, total time it took each process to complete.
This means, the measurement includes all relevant CPU time (i.e., over all processor cores) that was consumed by the run.
This might seem unfair to multi-threaded code, which might have an actual runtime lower than the result.
However, this is entirely depended on your machine and its cores, so a direct &amp;quot;total CPU time&amp;quot; comparison seemed fair to me.
In my case, it also is a rather minor issue, because I anyway only have two (hyper-threaded) cores on my laptop.
To be fair, this most significantly only affects Wapiti runtime, so I report total time there, too.
There are other opinions about performance measurements, e.g., that one should only measure post-warm-up training time (minus JVM, input data reading, etc.) or only a single training cycle/iteration should be measured.
I think it is more practical to measure and compare whatever the &amp;quot;out-of-the-box&amp;quot; performance of each tool is.
Each training process is run thrice and the shortest measured time is the one I report here.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="28%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="28%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="40%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Training Time&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;CRF Suite&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;3.63 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;10m 22s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Factorie&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;0.34 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;02m 18s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;OpenNLP&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;???? M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;02m 03s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Wapiti&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;1.56 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;19m 06s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Wapiti-4*&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;1.56 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;10m 09s*&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;[* absolute runtime (&amp;quot;real&amp;quot;) in multi-threaded mode using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-t&lt;/span&gt; 4&lt;/tt&gt; switch to make full use of my hyper-threaded dual-core i5 processor]&lt;/p&gt;
&lt;p&gt;As mentioned earlier, with respect to initial feature template richness, CRF Suite and Factorie are taking the lead, with Wapiti and OpenNLP using less templates.
The models' feature sizes shown here are as reported by each tool &lt;em&gt;after&lt;/em&gt; all feature pruning steps (compaction;
For Wapiti that number is calculated from &amp;quot;initial features&amp;quot; minus &amp;quot;removed features&amp;quot;.)
While OpenNLP does not report final feature set sizes, I &lt;em&gt;assume&lt;/em&gt; it to be in a similar range (somewhere around a million features).
So in terms of feature compaction, Factorie has a clear edge over the competition.&lt;/p&gt;
&lt;p&gt;In terms of training times, Factorie and OpenNLP easily outpace both CRF Suite and Wapiti, but this should not be entirely surprising:
OpenNLP uses a simpler model (MEMM), so it clearly must be faster.
One noteworthy point is the training speed of Factorie - probably due to forward-only learning, it achieves similar training times for its CRF as OpenNLP on a MEMM.
On the other end, as expected, Wapiti is by far the most resource-hungry tagger.
As Wapiti's &lt;em&gt;learning&lt;/em&gt; procedure can easily make use of multiple CPU cores with a simple switch, it gained significantly in terms of absolute (&amp;quot;real&amp;quot;) training time from running in multi-threaded mode on my dual core machine, at least.
This is important, because while tagging is what is called &amp;quot;embarrassingly parallel&amp;quot;, learning/optimization is not.
Still, this means the top model training implementation is provided by Factorie, as OpenNLP has a much simpler model to train.
The remaining question in this respect will be if OpenNLP and Factorie can keep up with the accuracy of the other two CRFs and how fast they all perform their tagging.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="tagging-quality"&gt;
&lt;h2&gt;Tagging quality&lt;/h2&gt;
&lt;p&gt;This section will resolve the final remaining question:
Which implementation can provide you with the most efficient tagger?
I have a SATA-3-attached SSD drive where the data is read from (but a slow i5 CPU...) and took the CoNLL 2000 test set sequences to measure accuracy on the 47,377 tokens using the models I had trained in the last step.
So while my measurements do not include writing the tagged tokens back to the device and reading data should not be an issue with a SSD, my CPU isn't exactly &amp;quot;Speedy Gonzales...&amp;quot;
I timed each system while tagging 100 times those 47,377 tokens in a single row, read from one file (i.e., about 200,000 sentences or roughly 30,000 scientific abstracts) to make a fair comparison of each system's token throughput, marginalizing any warm-up &amp;quot;penalties&amp;quot;.&lt;/p&gt;
&lt;p&gt;To run the the taggers on the generated models, the commands I used were:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# CRF Suite
cat test.txt | pos.py &amp;gt; features.txt
crfsuite tag -m pos.model -tq features.txt

# Factorie
java -cp factorie-1.1-SNAPSHOT-nlp-jar-with-dependencies.jar \
     cc.factorie.app.nlp.pos.ForwardPosTester -Xmx2g \
     --owpl --model=pos.model --test-file=test.txt

# OpenNLP
opennlp POSTaggerEvaluator -model pos.model -data test.txt

# Wapiti
wapiti label -m pos.model -c test.txt &amp;gt; /dev/null
&lt;/pre&gt;
&lt;p&gt;And here are the results, in terms of error rate (&lt;tt class="docutils literal"&gt;1 - accuracy&lt;/tt&gt; over 47k tokens) and throughput (on 201k sentences with 4.7M tokens, as measured with &lt;tt class="docutils literal"&gt;time&lt;/tt&gt;, sys+user):&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="27%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="31%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="38%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Error Rate&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Tokens/Second&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;CRF Suite&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;3.00 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;30,000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Factorie&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;2.19 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;23,800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;OpenNLP&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;2.88 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;19,500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Wapiti&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;2.20 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;21,200&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note that for the CRF Suite, I pre-generated the features from the Python script.
If not, the numbers would look quite bad, as the whole Python-based feature extraction process using the &lt;tt class="docutils literal"&gt;pos.py&lt;/tt&gt; script is several times slower than the tagger itself!
(A good indicator that the underlying Python code could arguably use some polish...)
In terms of tagging throughput, rather to my astonishment, it seems fair to say that the libraries perform roughly equal and there are by and large no noteworthy differences.
It seems that CRF Suite is faster, but then we actually cheated, because we pre-generated the label features.
So at best there is a minor chance that the CRF Suite could be faster than the others, if it had a very fast feature extraction mechanism.
Another remark maybe is that Factorie automatically detects the available cores and equally distributes the tagging load among them
(Note that the throughput calculation is based on CPU time (sys+user), so in absolute numbers on my CPU, Factorie tags at about 47k tokens/second.)
While the tagging process is an &amp;quot;embarrassingly parallel&amp;quot; problem (you could just split up the input between as many cores as you have and run your tagger with GNU &lt;a class="reference external" href="https://www.gnu.org/software/parallel/"&gt;parallel&lt;/a&gt;), it is a nice little extra thrown into the mix.
Overall, in terms of raw tagging throughput, there might be no real &amp;quot;winner&amp;quot;.&lt;/p&gt;
&lt;p&gt;Regarding accuracy, you might want to know that a baseline tagger (using the majority PoS tag and tagging all unseen words as noun) already achieves an accuracy of 90% (or, an error rate of 10%) in standard PoS scenarios.
Probably due to the inability to use mixed features and because it does its feature compaction &lt;em&gt;prior&lt;/em&gt; to the training, the CRF Suite has the worst performance in terms of accuracy, closely followed by OpenNLP.
OpenNLP's shortcoming most likely can be attributed to its model choice, which only really starts to shine in cross-domain experiments.
So in terms of high quality tagging, at least if you have training data specifically for that domain, you will be better off with Factorie or Wapiti - not all that unexpected, given our feature modeling and model implementation insights.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusions"&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;If you only need to do standard PoS tagging, chunking, and/or NER, and don't mind the tagging quality or performance too much, just go with the tool in your favorite language: OpenNLP for Java developers, Factorie for Scala hackers, and Wapiti for C/C++ or Python programmers
(There is a &lt;a class="reference external" href="https://github.com/adsva/python-wapiti"&gt;Python wrapper&lt;/a&gt; for Wapiti available, if Python (for both 2 and 3) is your deal.)
The trade-offs are simply not big enough to make a huge (order-of-magnitude) difference.
But then, if that were your case, you probably would not have read until here...&lt;/p&gt;
&lt;p&gt;Because there is no conceivable advantage in terms of training times, tagging throughput, or accuracy, no support for mixed transition/state features, and, particularly, as the code seems unmaintained, I would not recommend the use of CRF Suite. The missing ability to scale training across computing cores is similar to OpenNLP, and another possible issue. Nonetheless, the tool (and the wrapper) has gotten some love from Mikhail Korobov, who has a commercial interest in (maintaining) it, too. As just hinted, there is a Python wrapper for CRF Suite &lt;a class="reference external" href="https://github.com/tpeng/python-crfsuite"&gt;available&lt;/a&gt;, too, much like Wapiti. So if you need to work with real-valued features (e.g., for adding word-vector representations) and/or do not mind the mentioned issues, CRF Suite might still be an interesting option for you. (Have a look at the discussion linked below.)&lt;/p&gt;
&lt;p&gt;At the end of the day, if you are interested in generating high-performance quality annotations while using mixture (state + transition) features, there really is only the choice between Wapiti and Factorie:
Foremost, they are the only two tools ready for the multi-core world of today.
An error rate reduction of about 25% on the &amp;quot;solved&amp;quot; PoS tagging problem at no cost in throughput is not to be underestimated.
Wapiti definitely is the most attractive tagger in terms of off-the-shelf usability: feature generation is simple with the patterns with no need for doing any coding, and the overall implementation complexity vs. usability balance is excellent.
The only tradeoff are possibly longer training times (in single-core mode or vs. Factorie), so you will need to develop and combine your feature templates with care.
Finally, if you also are looking for true flexibility and the full power of graphical models, or want to venture into higher order Markov space, I see no way around Factorie.
In this case, it might be worth living with the sparse documentation and having to study Scala source code.
However, the team around McCallum are very actively working on this library, and the documentation is certainly getting better and more extensive every other time I come back and a few months have passed.
Another advantage is the fact that Factorie offers a (largely undocumented) full NLP pipeline, starting from sentence segmentation all the way to dependency parsing.&lt;/p&gt;
&lt;p&gt;Nonetheless, if you'd ask me to declare a &lt;em&gt;global&lt;/em&gt; &amp;quot;winner&amp;quot;, unless you are a probabilistic programming or at least Scala expert, I'd say that honor right now still goes to &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt;.
But once Factorie fixes the mentioned issues and makes the library more accessible to a &amp;quot;general public&amp;quot;, that might change, as it certainly already takes the lead in terms of model flexibility and implementation performance.&lt;/p&gt;
&lt;p&gt;If you feel like &lt;strong&gt;discussing&lt;/strong&gt; what is written here, I've posted a link to this article on &lt;a class="reference external" href="http://www.reddit.com/r/LanguageTechnology/comments/2i7fe1/a_review_of_free_sparse_sequence_taggers_for_nlp/"&gt;Reddit&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="text mining"></category><category term="nlp"></category><category term="probabilistic programming"></category></entry><entry><title>MEDLINE Kung-Fu</title><link href="http://fnl.es/medline-kung-fu.html" rel="alternate"></link><updated>2014-09-11T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2014-09-11:medline-kung-fu.html</id><summary type="html">&lt;p&gt;If you are a computational linguist, data analyst, or bioinformatician working with biological text corpora (on medicine, neuroscience, molecular biology, etc.), you will rather sooner than later need access to &lt;a class="reference external" href="http://www.nlm.nih.gov/bsd/pmresources.html"&gt;MEDLINE&lt;/a&gt;.
Right now, the MEDLINE &lt;a class="reference external" href="http://www.nlm.nih.gov/pubs/factsheets/dif_med_pub.html"&gt;subset&lt;/a&gt; (&amp;quot;baseline&amp;quot;) of PubMed contains nearly &lt;a class="reference external" href="http://www.nlm.nih.gov/bsd/licensee/baselinestats.html"&gt;23 million records&lt;/a&gt;, all with titles, author names, etc..
The majority of those records (officially called &lt;strong&gt;citations&lt;/strong&gt;) also have an abstract (on average about 5-8 sentences long).
This means you are looking at a significantly sized text collection with plenty of metadata (links, author names, MeSH terms, chemicals, etc.) you will need to handle if you want to make use of all this information your own data mining application.&lt;/p&gt;
&lt;p&gt;In my now about eight years of BioNLP (natural language processing for biology) work, I have not been able to locate a simple, up-to-date set of command-line tools to manage a MEDLINE DB mirror with all its metadata.
As I am an innately lazy guy, I have worked out a number of useful shell scripts I regularly use to work with MEDLINE data that I am documenting here.
To make this all work locally, and during a less lazy week, I wrote a tool called &lt;a class="reference external" href="https://pypi.python.org/pypi/medic"&gt;medic&lt;/a&gt; to create and manage an up-to-date (i.e., running daily, automatic updates) mirror of MEDLINE with the option of storing the citations either in a &lt;a class="reference external" href="http://www.postgresql.org/"&gt;Postgres&lt;/a&gt; or &lt;a class="reference external" href="http://sqlite.org/"&gt;SQLite&lt;/a&gt; database.&lt;/p&gt;
&lt;div class="section" id="synchronizing-the-medline-archives"&gt;
&lt;h2&gt;Synchronizing the MEDLINE archives&lt;/h2&gt;
&lt;p&gt;First you will most likely need to actually download the MEDLINE archives, given that your institute has a (&lt;a class="reference external" href="http://www.nlm.nih.gov/databases/journal.html"&gt;free&lt;/a&gt;) subscription to the archives.
If you are on a Mac, you are already provided with the necessary tools:
&lt;tt class="docutils literal"&gt;mount_ftp&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;rsync&lt;/tt&gt; should be installed on every Mac.
If you are on Linux, the &lt;tt class="docutils literal"&gt;curlftpfs&lt;/tt&gt; package simulates a file system for a FTP site accessed with with the cURL library, doing the same thing as &lt;tt class="docutils literal"&gt;mount_ftp&lt;/tt&gt; on a Mac.
In other words, we will mount the MEDLINE baseline and updates directories from the FTP site as a file system and then synchronize them with our local copy of each directory.
To do the synchronization step, we will be using &lt;tt class="docutils literal"&gt;rsync&lt;/tt&gt;.
On a GNU/Linux machine, you can use your package manager to install &lt;tt class="docutils literal"&gt;curlftpfs&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;rsync&lt;/tt&gt;, if they are not already present.
Once you have the packages installed, create a directory that acts as mount point, e.g., &lt;tt class="docutils literal"&gt;ftpfs&lt;/tt&gt;, as well as the &lt;tt class="docutils literal"&gt;baseline&lt;/tt&gt; directory, and the &lt;tt class="docutils literal"&gt;updates&lt;/tt&gt; directory.
Then, the relevant commands are:&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;To synchronize the baseline directory (replace &lt;tt class="docutils literal"&gt;curlftpfs&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;mount_ftp&lt;/tt&gt; on a Mac):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
curlftpfs ftp://ftp.nlm.nih.gov/nlmdata/.medleasebaseline/gz/ ftp_mount/
rsync -r -t -v --progress ftp_mount/* baseline/
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;To synchronize the updates directory (replace &lt;tt class="docutils literal"&gt;curlftpfs&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;mount_ftp&lt;/tt&gt; on a Mac):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
curlftpfs ftp://ftp.nlm.nih.gov/nlmdata/.medlease/gz/ ftp_mount/
rsync -r -t -v --progress ftp_mount/* updates/
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To unmount the FTP directories once rsync is done, you can use &lt;tt class="docutils literal"&gt;fusermount &lt;span class="pre"&gt;-u&lt;/span&gt; ftp_mount&lt;/tt&gt; on Linux and &lt;tt class="docutils literal"&gt;umount ftp_mount&lt;/tt&gt; on a Mac.
If you want to do the latter process regularly (MEDLINE sports daily updates), you might consider placing the update command series into a script and add it to your daily cron jobs.&lt;/p&gt;
&lt;p&gt;With this, you now have created the foundations to easily maintain a 24-hourly updated copy of all of MEDLINE on your site.
And because of using rsync, you do not have to worry about broken connections or communication errors - if the process breaks halfway, rsync will restart exactly where it left off.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="creating-a-local-medline-database-from-the-archives"&gt;
&lt;h2&gt;Creating a local MEDLINE database from the archives&lt;/h2&gt;
&lt;p&gt;Once the MEDLINE files are installed, it would be possible to parse (or grep...) the XML and manually extract whatever you need from them.
However, working this way will become cumbersome very fast, while placing the data into a well-structured database schema would help immensely.
To do this on the fly, I have created &lt;a class="reference external" href="https://pypi.python.org/pypi/medic"&gt;medic&lt;/a&gt;, a Python command-line tool to bootstrap and manage a local MEDLINE repository.
All you need to have installed to get this tool to work is &lt;a class="reference external" href="https://www.python.org/downloads/"&gt;Python&lt;/a&gt; (3.x); You can install &lt;a class="reference external" href="https://pypi.python.org/pypi/medic"&gt;medic&lt;/a&gt; with Python's own package manager: &lt;tt class="docutils literal"&gt;pip install medic&lt;/tt&gt; (possibly with the option &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--user&lt;/span&gt;&lt;/tt&gt; if you are not allowed to administer the machine you are working on).
Second, you need to decide which database you want to use for MEDLINE.
You can use either &lt;a class="reference external" href="http://www.postgresql.org/"&gt;Postgres&lt;/a&gt; or &lt;a class="reference external" href="http://sqlite.org/"&gt;SQLite&lt;/a&gt; as the back-end for medic (medic uses SQL Alchemy as its ORM, so in theory at least, it should be possible to use medic with other DBs, too.)&lt;/p&gt;
&lt;p&gt;As soon as you have the database installed and running (and CREATEd a DATABASE with UTF-8 text encoding, in the case of Postgres), you are ready to load the baseline files.
As loading all of MEDLINE through the ORM can be very slow for Postgres, it is better to parse the data into text files and then load them in one go:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;medic parse baseline/medline1?n*.xml.gz

&lt;span class="k"&gt;for &lt;/span&gt;table in citations abstracts authors chemicals databases descriptors &lt;span class="se"&gt;\&lt;/span&gt;
             identifiers keywords publication_types qualifiers sections;
  &lt;span class="k"&gt;do &lt;/span&gt;psql medline -c &lt;span class="s2"&gt;&amp;quot;COPY $table FROM &amp;#39;`pwd`/${table}.tab&amp;#39;;&amp;quot;&lt;/span&gt;;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you are loading the files into SQLite, you can load the data directly with &lt;tt class="docutils literal"&gt;medic insert&lt;/tt&gt;, although it will be considerably slower than the Postgres parse-and-dump method:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
medic --url sqlite:///MEDLINE.db insert baseline/medline1?n*.xml.gz
&lt;/pre&gt;
&lt;p&gt;Finally, to update the Postgres database to the latest state of MEDLINE, you can parse the &lt;tt class="docutils literal"&gt;updates&lt;/tt&gt; directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;for &lt;/span&gt;file in updates/medline1?n*.xml.gz;
  &lt;span class="k"&gt;do &lt;/span&gt;medic --update parse &lt;span class="nv"&gt;$file&lt;/span&gt;;
  &lt;span class="c"&gt;# NB: the above command created the file &amp;quot;delete.txt&amp;quot; (a list of PMIDs to delete)&lt;/span&gt;
  medic delete delete.txt

  &lt;span class="k"&gt;for &lt;/span&gt;table in citations abstracts authors chemicals databases descriptors &lt;span class="se"&gt;\&lt;/span&gt;
               identifiers keywords publication_types qualifiers sections;
    &lt;span class="k"&gt;do &lt;/span&gt;psql medline -c &lt;span class="s2"&gt;&amp;quot;COPY $table FROM &amp;#39;`pwd`/${table}.tab&amp;#39;;&amp;quot;&lt;/span&gt;;
  &lt;span class="k"&gt;done&lt;/span&gt;;
&lt;span class="k"&gt;done&lt;/span&gt;;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With SQLite you can again take the direct, but slower &lt;tt class="docutils literal"&gt;medic update&lt;/tt&gt; route:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
medic --url sqlite:///MEDLINE.db update updates/medline1?n*.xml.gz
&lt;/pre&gt;
&lt;p&gt;In theory, the simpler insert/update commands can be used for Postgres, too, but that is only recommended if you are loading citations in the thousands, not millions.
If you whish to cron-job this, you should only &lt;tt class="docutils literal"&gt;medic update&lt;/tt&gt; the latest file(s) - no need to parse-and-dump for a single file, not even for Postgres. In other words, make sure you are not working through all the files in the &lt;tt class="docutils literal"&gt;updates&lt;/tt&gt; directory every day...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="quickly-bootstrapping-a-subset-of-medline"&gt;
&lt;h2&gt;Quickly bootstrapping a subset of MEDLINE&lt;/h2&gt;
&lt;p&gt;Often I find myself only needing a tiny subset of MEDLINE that I am interested in analyzing.
In this case, you do not want to actually download, parse, and/or load all of PubMed into a heavy-weight Postgres DB, but rather have a small, single-file SQLite DB with the relevant citations.
To do this, medic provides an interface to effortlessly download and bootstrap a database right into the current directory.
Assuming you have the list of PubMed IDs (PMIDs) in a file called &lt;tt class="docutils literal"&gt;pmid_list.txt&lt;/tt&gt; (one ID per line),
and you want to bootstrap a SQLite DB file in the current directory called &lt;tt class="docutils literal"&gt;MEDLINE.db&lt;/tt&gt;,
you call medic like this:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
medic --url sqlite:///MEDLINE.db --pmid-lists insert pmid_list.txt
&lt;/pre&gt;
&lt;p&gt;With this, you have now quickly sampled that subset of MEDLINE citaitons relevant to your work, but still have them properly structured, stored in a single file, and easy to access as we will see next.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="extracting-medline-citations-with-medic"&gt;
&lt;h2&gt;Extracting MEDLINE citations with medic&lt;/h2&gt;
&lt;p&gt;Right now, medic has no interface to query the abstracts.
You can add a Postgres full-text index, but according to my own experience that is not particularly efficient if you have millions of records to index, as in the case of MEDLINE.
The right way would be to index the abstracts with a &amp;quot;real&amp;quot; search engine, for example, Lucene, but so far I have not gotten around to write an indexer for medic.
The best way right now is to query eUtils directly, using the standard PubMed query syntax, which is pretty powerful, anyways;
Note that eSearch queries to the eUtils API are capped, at most 100,000 IDs can be returned at once.
To fetch more, you need to page results with &lt;tt class="docutils literal"&gt;retmax&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;retmin&lt;/tt&gt;; Also by default (without setting &lt;tt class="docutils literal"&gt;retmax&lt;/tt&gt;) only the first 20 results are returned by eUtils:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
QUERY=&amp;quot;p53+AND+cancer&amp;quot;
URL=&amp;quot;http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi&amp;quot;

wget &amp;quot;$URL?db=PubMed&amp;amp;retmax=99&amp;amp;term=$QUERY&amp;quot; -O - 2&amp;gt; /dev/null \
| grep &amp;quot;^&amp;lt;Id&amp;gt;&amp;quot; \
| sed -E 's|&amp;lt;/?Id&amp;gt;||g' \
| cut -f3 \
&amp;gt; pmids.txt
&lt;/pre&gt;
&lt;p&gt;Again, if you do this often, you might want to stick this into a little script, for example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/usr/bin/env bash&lt;/span&gt;
&lt;span class="c"&gt;# for a given query (one term per argument), retrieve (up to retmax) matching PMIDs&lt;/span&gt;

&lt;span class="nv"&gt;QUERY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$@&amp;quot;&lt;/span&gt; | tr &lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt; +&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="nv"&gt;URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi&amp;quot;&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$QUERY&amp;quot;&lt;/span&gt; 1&amp;gt;&amp;amp;2

wget &lt;span class="s2"&gt;&amp;quot;$URL?db=PubMed&amp;amp;retmax=99999&amp;amp;term=$QUERY&amp;quot;&lt;/span&gt; -O - 2&amp;gt; /dev/null &lt;span class="se"&gt;\&lt;/span&gt;
| grep &lt;span class="s2"&gt;&amp;quot;^&amp;lt;Id&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
| sed -E &lt;span class="s1"&gt;&amp;#39;s|&amp;lt;/?Id&amp;gt;||g&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
| cut -f3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The query argument to this script now can contain space characters which are replaced with &amp;quot;+&amp;quot; characters, e.g., &amp;quot;&lt;tt class="docutils literal"&gt;search_pubmed.sh p53 AND cancer&lt;/tt&gt;&amp;quot; produces the same output as before (with far more PMIDs, however, so please do not try this particular query too often; and using &lt;tt class="docutils literal"&gt;search_pubmed.sh&lt;/tt&gt; as the name of the above script).&lt;/p&gt;
&lt;p&gt;Given one or a list of PMIDs, however, medic allows you to quickly pull the citations in a number of formats:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;medline&lt;/tt&gt;: write the citations, one per file, in the official MEDLINE format.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;tiab&lt;/tt&gt;: only put the title and abstract, one pair per citations, into the files.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;html&lt;/tt&gt;: write all citations into one large HTML file (&amp;quot;corpus&amp;quot;).&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;tsv&lt;/tt&gt;: write the PMID, title, and abstract into one large TSV file, one citation per line.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example: &lt;tt class="docutils literal"&gt;medic &lt;span class="pre"&gt;--format&lt;/span&gt; tiab &lt;span class="pre"&gt;--pmid-lists&lt;/span&gt; selected_pmids.txt&lt;/tt&gt;, where &lt;tt class="docutils literal"&gt;selected_pmids.txt&lt;/tt&gt; is a file with one PMID per line, will create one file per citation, named &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;&amp;lt;PMID&amp;gt;.txt&lt;/span&gt;&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;If you do not care too much about the actual IDs and just need a few random citations to work with, here is an easy way to select 999 random PMIDs from MEDLINE; on a Mac or FreeBSD machine:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
jot 999 100000 25000000 &amp;gt; pmids.rnd_test.txt
&lt;/pre&gt;
&lt;p&gt;And when running a Linux or Cygwin OS:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
shuf -i 100000-25000000 | head -999 &amp;gt; pmids.rnd_test.txt
&lt;/pre&gt;
&lt;p&gt;However, this approach is a bit like cheating: if the PMID does not exist, you have a non-existing ID in your list.
From a mathematical perspective, if the PMIDs are not evenly distributed over the range you are drawing integers from, you will not have the &lt;em&gt;perfect&lt;/em&gt; random sample.
Ideally, you should select random IDs from your collection, not the whole numeric range.&lt;/p&gt;
&lt;p&gt;Note that I chose 999 PMIDs not just by chance - SQLite has 999 set as a hard limit for the number of arguments for a &amp;quot;prepared statement&amp;quot;.
This means that if you want to fetch more than 999 PMIDs from a SQLite database, you will have to do that in several rounds.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="converting-dois-to-pmids"&gt;
&lt;h2&gt;Converting DOIs to PMIDs&lt;/h2&gt;
&lt;p&gt;To finish, here is a nifty little command-line to convert a list of DOIs into a list of PMIDs by using the NCBI eUtils web service:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;for &lt;/span&gt;doi in &lt;span class="sb"&gt;`&lt;/span&gt;cat dois.txt&lt;span class="sb"&gt;`&lt;/span&gt;;
  &lt;span class="k"&gt;do &lt;/span&gt;&lt;span class="nv"&gt;pmid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;wget &lt;span class="s2"&gt;&amp;quot;$URL?db=PubMed&amp;amp;retmode=xml&amp;amp;term=$doi&amp;quot;&lt;/span&gt; -O - 2&amp;gt; /dev/null &lt;span class="se"&gt;\&lt;/span&gt;
  | grep &lt;span class="s2"&gt;&amp;quot;&amp;lt;Id&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  | sed -E &lt;span class="s1"&gt;&amp;#39;s|&amp;lt;/?Id&amp;gt;||g&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  | cut -f3&lt;span class="sb"&gt;`&lt;/span&gt;;
  &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$doi&lt;/span&gt; &lt;span class="nv"&gt;$pmid&lt;/span&gt; &amp;gt;&amp;gt; doi2pmid.txt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you use this much, you might even want to put that into a little script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/usr/bin/env sh&lt;/span&gt;
&lt;span class="c"&gt;# for a argument list of DOIs, print each DOI and matching PubMed ID&lt;/span&gt;

&lt;span class="nv"&gt;URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;for &lt;/span&gt;doi in &lt;span class="nv"&gt;$@&lt;/span&gt;; &lt;span class="k"&gt;do&lt;/span&gt;
&lt;span class="k"&gt;  &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$doi&lt;/span&gt; &lt;span class="sb"&gt;`&lt;/span&gt;wget &lt;span class="s2"&gt;&amp;quot;$URL?db=PubMed&amp;amp;retmode=xml&amp;amp;term=$doi&amp;quot;&lt;/span&gt; -O - 2&amp;gt; /dev/null &lt;span class="se"&gt;\&lt;/span&gt;
  | grep &lt;span class="s2"&gt;&amp;quot;&amp;lt;Id&amp;gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  | sed -E &lt;span class="s1"&gt;&amp;#39;s|&amp;lt;/?Id&amp;gt;||g&amp;#39;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  | cut -f3&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Alternatively, you might want the script to take a file with DOIs as input.
But that is a trivial case to handle with this script: just use the file as an argument of &lt;tt class="docutils literal"&gt;xargs&lt;/tt&gt; and pipe the result into this script.&lt;/p&gt;
&lt;p&gt;Unluckily enough, converting PMIDs to DOIs is a lot more trickier: First, the download of MEDLINE from the FTP site does not contain all PubMed mappings of PMIDs to DOIs that the NLM has access to (why not is a mystery to me...). Second, there are still scores of PMIDs that even the NLM did not receive the correct DOI mapping from the publisher. So overall, no matter what you do, there will be holes if trying to go the direct PMID-to-DOI way. The best method right now is probably to query with the title and authors and see if you find an exact, unique match to a DOI on &lt;a class="reference external" href="http://www.crossref.org/"&gt;CrossRef&lt;/a&gt;, but that API is commercial and you need to pay for any serious query volumes.&lt;/p&gt;
&lt;p&gt;Overall, this collection of tools should give you everything you need to quickly and efficiently work with MEDLINE's PubMed citations. If you have not done so already, you can check out the &amp;quot;full capabilities&amp;quot; of &lt;a class="reference external" href="https://pypi.python.org/pypi/medic"&gt;medic&lt;/a&gt; and decide for yourself if my approach is suitable for you, too.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="text mining"></category><category term="bionlp"></category><category term="pubmed"></category></entry><entry><title>An Introduction to Statistical Text Mining</title><link href="http://fnl.es/an-introduction-to-statistical-text-mining.html" rel="alternate"></link><updated>2014-07-07T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2014-07-07:an-introduction-to-statistical-text-mining.html</id><summary type="html">&lt;p&gt;Last week we had a really great time at the first hands-on text mining workshop in the context of the &lt;a class="reference external" href="http://www.dia.fi.upm.es/ASDM"&gt;Advanced Statistics and Data Mining&lt;/a&gt; Summer School. This one-week course is an introduction to text mining from the &amp;quot;bottom up&amp;quot; to a bunch of motivated summer students, with the practical parts based on Python and the &lt;a class="reference external" href="http://www.nltk.org/"&gt;NLTK&lt;/a&gt;. The presentation was part of the 9th iteration of the Summer School that is located in the sunniest capital of Europe: Madrid (well, &lt;em&gt;ex aequo&lt;/em&gt; with Athens, at least). In its context, I presented 15 hours worth of practical and theoretical background on machine learning for text mining to fourteen participants from all over the world. With this post I am sharing the slides and tutorial files (&lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt; Notebooks) with the world for free (see the Creative Commons &lt;a class="reference external" href="https://creativecommons.org/licenses/by-sa/3.0/"&gt;BY-SA&lt;/a&gt; licensing details). While much of the material should speak for itself, it might not &amp;quot;save&amp;quot; you from visiting a text mining class (maybe mine, next summer?).&lt;/p&gt;
&lt;div class="section" id="overview"&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;The theory mostly focuses on explaining the methods and statistics behind machine learning &lt;em&gt;for text mining&lt;/em&gt; - without requiring a particular background other than sharp high-school maths. The practicals on the other hand make use of Python, particularly online &lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt; Notebooks. Therefore, prior contact with Python would be useful to profit the most from the practicals, but it is not required to follow the course. IPython's Notebooks act very much like MATLAB Notebooks, but run online in any modern browser, are based on open source software that requires no license fee, and provide facilities to place documentation and mathematical formulas between the code snippets, evaluation results, and graphical plots. Therefore, these notebooks serve as take home material for the students to study and play around with, even well after the course. To illustrate the power of IPython, learning how to work in this environment immediately enables you to run distributed, high-performance computations &amp;quot;off the shelf&amp;quot; (e.g., via &lt;a class="reference external" href="http://star.mit.edu/cluster/"&gt;StarCluster&lt;/a&gt; on Amazon EC2 Clusters).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-1-introduction"&gt;
&lt;h2&gt;Day 1 - Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/statistical-text-mining-introduction-florian-leitner"&gt;Lecture 1&lt;/a&gt; starts with a light-weight introduction to text mining, natural language understanding and generation, and a statistics approach to artificial intelligence in general. It provides a taste of both what is to come in the course and what might be of interest to look into afterwards. It closes with a brief reprise of elementary Bayesian statistics and conditional probability, using the Monty Hall problem as a practical example of &lt;em&gt;Bayes' Rule&lt;/em&gt;. The &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/IPython%20Notebook%20Introduction.ipynb"&gt;practical #1&lt;/a&gt; introduces the use of &lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt; and its online Notebook, as well as some aspects of &lt;a class="reference external" href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt; (particularly, contrasting NumPy/SciPy to &lt;a class="reference external" href="http://www.r-project.org/"&gt;R&lt;/a&gt;). The most important aspects of the &lt;strong&gt;Natural Language ToolKit&lt;/strong&gt; (&lt;a class="reference external" href="http://www.nltk.org/"&gt;NLTK&lt;/a&gt;) Python library are presented during the &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Introduction%20to%20NumPy%20and%20NLTK.ipynb"&gt;second part&lt;/a&gt; of the practical; The NTLK is frequently used during the course. As an exercise to become familiar with Python and the NLTK, participants are encouraged to implement a function to let two NLTK chat-bots converse between each other.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-2-language-modeling"&gt;
&lt;h2&gt;Day 2 - Language Modeling&lt;/h2&gt;
&lt;p&gt;On the second day, &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-25-language-modeling-florian-leitner"&gt;lecture 2&lt;/a&gt; introduces the chain rule of conditional probability and builds on that to take you from the &lt;em&gt;Markov property&lt;/em&gt; over &lt;em&gt;language modeling&lt;/em&gt; to &lt;em&gt;smoothing techniques&lt;/em&gt;. In class, you learn how to operate on n-gram frequency tables and we work out the conditional probability distributions of bi- and trigram models. This should ensure  students obtain well-grounded foundations of statistical language processing. After a &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Building%20a%20Language%20Model%20in%207%20Steps.ipynb"&gt;quick overview&lt;/a&gt;, the accompanying &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Language%20Modelling%20with%20NLTK.ipynb"&gt;exercises #2&lt;/a&gt; demonstrate how to build language models with the NLTK and participants are tasked with generating their own models using advanced smoothing techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-3-string-processing"&gt;
&lt;h2&gt;Day 3 - String Processing&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-35-string-processing"&gt;third day lecture&lt;/a&gt; focuses on &lt;em&gt;string processing&lt;/em&gt; and the algorithmic aspects of text mining. The slides contain an overview of state machines, like regular expressions, PATRICIA tries, and &lt;em&gt;Minimal Acyclic Deterministic Finite [State] Automata&lt;/em&gt; (MADFA). Particular attention is payed to string metrics and similarity measures. The last theoretical part is a thorough introduction to &lt;em&gt;Locality Sensitive Hashing&lt;/em&gt; (LSH), and its use as a tool to efficiently cluster millions of documents or implement approximate string matching over very large-scale term dictionaries is discussed. During the &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Locality%20Sensitive%20Hashing.ipynb"&gt;3rd exercises&lt;/a&gt; students apply LSH to a toy problem: improving the speed of &lt;a class="reference external" href="http://norvig.com/spell-correct.html"&gt;Peter Norvig&lt;/a&gt;-style &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Spelling%20Correction%20using%20LSH.ipynb"&gt;spelling correction&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-4-text-classification"&gt;
&lt;h2&gt;Day 4 - Text Classification&lt;/h2&gt;
&lt;p&gt;After the more algorithmic topics, the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-45-text-classification"&gt;lecture of day four&lt;/a&gt; takes the participants into the realm of &amp;quot;real&amp;quot; machine learning. For starters, ways to calculate syntactic and semantic document similarity are presented, culminating in &lt;em&gt;Latent Semantic Analysis&lt;/em&gt;. The slides introduce the Naive Bayes classifier as an example to prepare the audience for the &lt;em&gt;Maximum Entropy classifier&lt;/em&gt; (Multinomial Logistic Regression), which forms the central topic of this session. To &amp;quot;cool down&amp;quot; a bit after a math-heavy section, &lt;em&gt;sentiment analysis&lt;/em&gt; is discussed as a popular domain for text classification. Finally, the important topic of evaluating set-based classifier performance via Accuracy, F-measure, and MCC Score are discussed. The &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Twitter%20Sentiment%20Analysis.ipynb"&gt;fourth practical&lt;/a&gt; then walks the participants through a Maximum Entropy sentiment classifier and encourages the audience to improve its performance by designing better features and making clever use of all the gained knowledge in the course so far.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-5-graphical-models"&gt;
&lt;h2&gt;Day 5 - Graphical Models&lt;/h2&gt;
&lt;p&gt;On the final day, &lt;em&gt;probabilistic graphical models&lt;/em&gt; dominate the lecture. With the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-55-information-extraction"&gt;slides for day #5&lt;/a&gt;, first the main differences between Bayesian Networks and Markov Random Fields are introduced. Connected to yesterday's text classification topics, &lt;em&gt;Latent Dirichlet Allocation&lt;/em&gt; is presented as the first graphical model. Then, for the remainder of the talk, we move into &lt;em&gt;dynamic&lt;/em&gt; (temporal) models and their applications. We revisit the Markov chain and go from &lt;em&gt;Hidden Markov Models&lt;/em&gt; all the way to &lt;em&gt;Conditional Random Fields&lt;/em&gt;. To gear participants up for practical text mining, we look into actual applications of the presented models, for example, (semantic) &lt;em&gt;Word Representations&lt;/em&gt; or &lt;em&gt;Named Entity Recognition&lt;/em&gt;. The participants are introduced to the prospect of using the assigned labels for more advanced tasks. For example, &lt;em&gt;Relationship Extraction&lt;/em&gt; now becomes feasible by combining the shallow parse output as features for classifiers presented in other sessions of the Advanced Statistics and Data Mining Summer School. The rank-based performance measures of ROC and PR curves are discussed as way to evaluate the labeled results. During the &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Shallow%20Parsing%20with%20NLTK.ipynb"&gt;last practical&lt;/a&gt;, implementing a &lt;strong&gt;shallow parser&lt;/strong&gt; using NLTK and the &lt;a class="reference external" href="http://nlp.stanford.edu/software/tagger.shtml"&gt;Stanford Taggers&lt;/a&gt; is demonstrated in class. Overall, the practicals should provide the students with the basic software tools to embark on their own text mining adventures right after the course.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="remarks"&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;You can &lt;a class="reference external" href="mailto:florian.leitner&amp;#64;gmail.com"&gt;contact me&lt;/a&gt; if you would like to discuss having this tutorial in the context of your own venue or have questions and comments about the slides' content. Other than that, I hope to have awakened your interest in statistical text mining and/or to have provided you with useful material, both as a student or teacher.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="text mining"></category><category term="python"></category><category term="nltk"></category></entry><entry><title>Getting started with a "virtual" Go environment</title><link href="http://fnl.es/getting-started-with-a-virtual-go-environment.html" rel="alternate"></link><updated>2013-11-29T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2013-11-29:getting-started-with-a-virtual-go-environment.html</id><summary type="html">&lt;p&gt;Given how easy it is to write highly concurrent code in Go (aka &amp;quot;&lt;a class="reference external" href="http://golang.org"&gt;golang&lt;/a&gt;&amp;quot;), it is probably worth learning this language.
Personally, I believe Go is not yet mature enough for &amp;quot;production&amp;quot; projects other than servers maybe (and I am sure there are people who will not agree with my belief...).
But for doing science, juggling huge ammounts of data (no, I will not say the &amp;quot;B&amp;quot;-word...), and your typical scripting pipelines, Go is really great.
On top of that, Go is easy to learn because it has a simple syntax and a very &amp;quot;bare bones&amp;quot; approach in so many aspects.
So if you feel like giving it a try, here is a quick recipe to bootstrap a Go development environment within moments:&lt;/p&gt;
&lt;p&gt;First, &lt;a class="reference external" href="http://golang.org/doc/install"&gt;install Go&lt;/a&gt; itself using your package manager; For example, on OSX, you might use &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt;, and on Ubuntu or Debian you'd probably use &lt;tt class="docutils literal"&gt;apt&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew install go --cross-compile-common
apt-get install golang
...
&lt;/pre&gt;
&lt;p&gt;Notice that if you are on a LTS version of Ubuntu (Precise/12.04 right now) or Debian, you might actually want to install a more &lt;a class="reference external" href="http://code.google.com/p/go/downloads/list"&gt;up-to-date binary&lt;/a&gt;.
Next, you need to set the global environment variable &lt;tt class="docutils literal"&gt;GOROOT&lt;/tt&gt;.
It has to point to the directory you installed Go in;
When using Homebrew on OSX, this can be tricky, so here is a snippet that will create the correct path for you:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
export GOROOT=$(brew --prefix)/Cellar/go/$(go \
version | cut -f3 -d' ' | sed 's/go//')/libexec
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Protip&lt;/em&gt;: If during the next steps you see a lot of errors of the form:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
imports PKGNAME: unrecognized import path &amp;quot;PATH&amp;quot;
&lt;/pre&gt;
&lt;p&gt;They probably occur while running &lt;tt class="docutils literal"&gt;go get SOME_PKG&lt;/tt&gt;, and it most likely just means that your &lt;tt class="docutils literal"&gt;GOROOT&lt;/tt&gt; is wrong or unset.
Finally, you want to quickly bootstrap a &amp;quot;virtual&amp;quot; development environment for Go.
For this, I use a simple shell script (that I call &lt;tt class="docutils literal"&gt;goinit&lt;/tt&gt;) to set up the directory structure and an &amp;quot;&lt;tt class="docutils literal"&gt;activate&lt;/tt&gt;-able&amp;quot; environment [UPDATE 2013-12-17: additional go get package lines that are extremely useful for development]&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="c"&gt;# setup a directory structure for programming in go&lt;/span&gt;
&lt;span class="nv"&gt;VCS_HUB&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;github.com/username
&lt;span class="nv"&gt;PROJECT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;basename &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;

mkdir -p &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt;
mkdir -p &lt;span class="s2"&gt;&amp;quot;src/$VCS_HUB/$PROJECT&amp;quot;&lt;/span&gt;
mkdir bin
mkdir pkg
&lt;span class="nv"&gt;GOPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
go get github.com/nsf/gocode
go get github.com/jstemmer/gotags
go get github.com/davecheney/godoc2md
go get github.com/grobins2/gobrew
cat &lt;span class="s"&gt;&amp;lt;&amp;lt; ACTIVATE &amp;gt; bin/activate&lt;/span&gt;
&lt;span class="s"&gt;export GOPATH=&amp;quot;`pwd`&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;export PATH=&amp;quot;\$GOPATH/bin:\$PATH&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;export PS1=&amp;quot;($PROJECT)\$PS1&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;ACTIVATE&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Put this script somewhere on you &lt;tt class="docutils literal"&gt;PATH&lt;/tt&gt;, replace with your own GitHub &lt;tt class="docutils literal"&gt;username&lt;/tt&gt; (or any other version control system you use), make it executable (&lt;tt class="docutils literal"&gt;chmod 755 goinit&lt;/tt&gt;), and ensure you have the &lt;tt class="docutils literal"&gt;GOROOT&lt;/tt&gt; set in your environment and &lt;tt class="docutils literal"&gt;goinit&lt;/tt&gt; on your path.
For example, let's assume you want to start with the Go Tour to learn about the language itself (&lt;em&gt;highly&lt;/em&gt; recommendable!);
With this setup, bootstrapping your next Go project (simply called &amp;quot;&lt;tt class="docutils literal"&gt;project&lt;/tt&gt;&amp;quot; here) now is as simple as:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
goinit path/to/project
cd path/to/project
source bin/activate

# if you are new to Go, you might want to try this:
go get code.google.com/p/go-tour/gotour
gotour
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;gotour&lt;/tt&gt; should have opened in your browser.
Happy Go coding!&lt;/p&gt;
</summary><category term="golang"></category><category term="posix"></category></entry><entry><title>Concurrent Node.js</title><link href="http://fnl.es/concurrent-nodejs.html" rel="alternate"></link><updated>2013-10-08T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2013-10-08:concurrent-nodejs.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, a &lt;a class="reference external" href="https://twitter.com/SoftActiva"&gt;colleague&lt;/a&gt; of mine asked me to introduce the most important concepts of &lt;a class="reference external" href="http://nodejs.org/"&gt;Node&lt;/a&gt; programming to a flock of interested people in our &lt;a class="reference external" href="http://www.cnio.es/es/grupos/plantillas/presentacion.asp?grupo=50004294"&gt;research group&lt;/a&gt;.
Initially, I declined, considering the &lt;a class="reference external" href="http://howtonode.org/"&gt;vast&lt;/a&gt; &lt;a class="reference external" href="http://docs.nodejitsu.com/"&gt;number&lt;/a&gt; of &lt;a class="reference external" href="http://www.nodebeginner.org/"&gt;tutorials&lt;/a&gt; and &lt;a class="reference external" href="https://duckduckgo.com/?q=node.js+book"&gt;books&lt;/a&gt;, but then thought it might be quite an interesting challenge:
Is there any aspect of Node development that is not easily understood by Node beginners and that is poorly covered by the existing posts?
Taking this into account, my main goals for this tutorial are:
Which part of developing Node programs is the hardest to grasp for programmers proficient in imperative languages (Java, JavaScript, Objective-C, PHP, Python, Ruby, etc.)?
In my opinion, the biggest issue is writing asynchronous, concurrent Node applications.
At the same time, this seems to be the least covered aspect of all existing introductory tutorials.
Last, I wanted to present relevant issues &lt;em&gt;without&lt;/em&gt; getting too far ahead of the status quo (e.g., &lt;a class="reference external" href="http://wiki.ecmascript.org/doku.php?id=harmony:generators"&gt;generators&lt;/a&gt;, which are still considerd experimental right now) or anything else that you would not want to use in production.&lt;/p&gt;
&lt;p&gt;Overall, this tutorial should teach you to &lt;em&gt;code&lt;/em&gt; &lt;a class="reference external" href="http://www.reactivemanifesto.org/"&gt;reactive&lt;/a&gt; programs that are both maintainable and concise.
Nonetheless, I will assume you have a basic idea about Node and JavaScript, so I do expect you have read at least one of those excellent &lt;a class="reference external" href="https://github.com/maxogden/art-of-node#the-art-of-node"&gt;introductions&lt;/a&gt;.
In particular, you should have a basic idea of how Node uses &lt;a class="reference external" href="http://docs.nodejitsu.com/articles/getting-started/control-flow/what-are-callbacks"&gt;callbacks&lt;/a&gt; to handle &lt;a class="reference external" href="http://docs.nodejitsu.com/articles/getting-started/control-flow/what-are-event-emitters"&gt;events&lt;/a&gt;, given that they are the core concept of Node apps.
And although I will cover this aspect in more detail, Nodejitsu also has a short section on writing &lt;a class="reference external" href="http://docs.nodejitsu.com/articles/getting-started/control-flow/how-to-write-asynchronous-code"&gt;asynchronous&lt;/a&gt; JavaScript and spawning &lt;a class="reference external" href="http://docs.nodejitsu.com/articles/child-processes/how-to-spawn-a-child-process"&gt;child processes&lt;/a&gt; in Node.&lt;/p&gt;
&lt;p&gt;To make this discussion more valuable for real-world projects, I restrict myself to use “battle-tested” packages;
After checking with &lt;a class="reference external" href="https://npmjs.org/"&gt;npm&lt;/a&gt; and GitHub, I decided to only use &lt;strong&gt;async&lt;/strong&gt; and &lt;strong&gt;Q&lt;/strong&gt;, plus some &lt;strong&gt;underscore&lt;/strong&gt; magic (only &lt;a class="reference external" href="http://underscorejs.org/#partial"&gt;partial&lt;/a&gt; function application).
The tutorial should explain to the reader how, using a limited set of tools, a Node developer can write highly asynchronous Node apps without ending up in &amp;quot;concurrency hell&amp;quot;.
This introduction will take the reader familiar with Node's basic callback mechanism (&lt;a class="reference external" href="http://www.webdesignerdepot.com/2012/09/jquery-the-good-the-bad-and-the-ugly/"&gt;evil tongues&lt;/a&gt; would say &amp;quot;callback spaghetti&amp;quot;) via asynchronous control structures to a concurrent, functional programming style that uses &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Futures_and_promises"&gt;promises&lt;/a&gt; - also know as &lt;strong&gt;futures&lt;/strong&gt; - to delegate the logical flow from the JavaScript code to the V8 engine.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-quick-refresher"&gt;
&lt;h2&gt;A Quick Refresher&lt;/h2&gt;
&lt;p&gt;Lets look at a static HTTP file server, using the basic Node modules only.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;url&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;path&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;fs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;fs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;

&lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;createServer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;request&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;uri&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;url&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;request&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;url&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;pathname&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;path&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;cwd&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nx"&gt;uri&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;path&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nx"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;404&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Content-Type&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;text/plain&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;404 Not Found\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;statSync&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;isDirectory&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="nx"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/index.html&amp;#39;&lt;/span&gt;

      &lt;span class="nx"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;readFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;binary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Content-Type&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;text/plain&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;binary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;}).&lt;/span&gt;&lt;span class="nx"&gt;listen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;parseInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;port&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;file server running at http://localhost:&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This highlights basic Node concepts; Upon receiving a request, the server makes an asynchronous check via the OS to ensure the file exists.
The asynchronicity is indicated by the callback that gets sent to &lt;tt class="docutils literal"&gt;path.exists()&lt;/tt&gt;.
Then, again asynchronously (note the callback sent into &lt;tt class="docutils literal"&gt;fs.readFile()&lt;/tt&gt;), Node reads the file, and if this job completes successfully, sends the file (data) to the response handler.
Because both checks are done asynchronously, this leaves the server process free to handle other requests while the OS is doing the file lookups (i.e., &amp;quot;non-blocking&amp;quot;).
However, this demonstrates a problem with this callback coding style: the functions are stacked one on top of another, making this code quite uncomfortable to read and maintain.&lt;/p&gt;
&lt;p&gt;Before we begin, it is also worth to refresh the &lt;strong&gt;six commandments&lt;/strong&gt; (taken from Caolan McMahon's excellent post on Node &lt;a class="reference external" href="http://caolanmcmahon.com/posts/nodejs_style_and_structure/"&gt;style and structure&lt;/a&gt;) you should follow when coding any concurrent program:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Never mix sync and async style: return a value &lt;em&gt;or&lt;/em&gt; a callback, but not either.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Return&lt;/em&gt; on a callback: write &lt;tt class="docutils literal"&gt;return callback(null, result)&lt;/tt&gt; to prevent execution continuing beyond the CB.&lt;/li&gt;
&lt;li&gt;Always check errors in callbacks: make sure the CB has a way to deal with error states, as in &lt;tt class="docutils literal"&gt;if (err) { ... }&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Avoid mutable state - it leads to &amp;quot;debugging hell&amp;quot; in concurrent programs (e.g., &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Heisenbug"&gt;Heisenbugs&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Only use try, throw &amp;amp; catch within imperative code sections (you cannot throw &amp;quot;across&amp;quot; a CB).&lt;/li&gt;
&lt;li&gt;Write tiny functions: maybe 3 statements, and no more than 5.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="multiprocessing-in-node-js"&gt;
&lt;h2&gt;Multiprocessing in Node.js&lt;/h2&gt;
&lt;p&gt;Node is process-oriented, i.e., unlike some languages where you create multiple threads in your process, in node you need to &amp;quot;open&amp;quot; multiple OS processes (using &lt;tt class="docutils literal"&gt;popen(3)&lt;/tt&gt;).
The default tool to run multiple processes in Node is the &lt;a class="reference external" href="http://nodejs.org/api/child_process.html"&gt;child_process&lt;/a&gt; module, and, as an experimental add-on since Node v9, &lt;a class="reference external" href="http://nodejs.org/api/cluster.html"&gt;cluster&lt;/a&gt; can run multiple Node processes in parallel that share the same port or socket.
When forking/spawning/executing child process, be aware that segmentation faults and other nasty errors in a child process have the potential to bring down your whole stack.
In general, there are three calls that are relevant for running parallel processes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://nodejs.org/api/child_process.html#child_process_child_process_spawn_command_args_options"&gt;spawn&lt;/a&gt; provides a &lt;strong&gt;streaming&lt;/strong&gt; process that you communicate with through Unix pipes. Think of it as in shell syntax &amp;quot;&lt;tt class="docutils literal"&gt;process &amp;lt; instream &amp;gt; outstream &amp;amp;&lt;/tt&gt;&amp;quot;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://nodejs.org/api/child_process.html#child_process_child_process_exec_command_options_callback"&gt;exec&lt;/a&gt; runs a single shell &lt;strong&gt;command&lt;/strong&gt; and returns the exit status of the computation. Think of it as in shell syntax &amp;quot;&lt;tt class="docutils literal"&gt;exec input; echo $?&lt;/tt&gt;&amp;quot;. Note that while exec does have an output buffer, it is tiny and you should use the streaming interface of spawn if you expect to receive output from the process you are running.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://nodejs.org/api/child_process.html#child_process_child_process_fork_modulepath_args_options"&gt;fork&lt;/a&gt; runs a &lt;strong&gt;worker&lt;/strong&gt; script you can communicate with via a &lt;em&gt;channel&lt;/em&gt;. Think of it as a bidirectional version of the shell pipe &amp;quot;&lt;tt class="docutils literal"&gt;server | worker &amp;amp;&lt;/tt&gt;&amp;quot;. Fork is also useful to isolate blocking calls from your otherwise non-blocking Node program.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that &lt;tt class="docutils literal"&gt;spawn&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;fork&lt;/tt&gt; are much like background daemons, while &lt;tt class="docutils literal"&gt;exec&lt;/tt&gt; runs some binary to completion.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-case-a-service-broker"&gt;
&lt;h2&gt;Example Case: A Service Broker&lt;/h2&gt;
&lt;p&gt;Throughout this tutorial, we will use a theoretical service broker that might query a DB or use XHR to communicate with another server.
It also could be some worker writing or reading data, or anything else that is somehow &amp;quot;IO-bound&amp;quot;.
Whatever you prefer, the point is that this is a blocking (&amp;quot;side-effecting&amp;quot;) IO call in your program that you want to handle asynchronously.
In other words, this means you will need to use &lt;strong&gt;callbacks&lt;/strong&gt; (CB) to be notified when the IO event finished and with which result and error state.&lt;/p&gt;
&lt;p&gt;First, we create a directory where we can put our tutorial code, and for the sake of popularity, use the &lt;a class="reference external" href="http://expressjs.com/"&gt;Express&lt;/a&gt; web app framework:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ mkdir reactive-node-tutorial
$ cd reactive-node-tutorial
$ npm install express
$ node_modules/express/bin/express
yes
$ npm install
&lt;/pre&gt;
&lt;p&gt;We will use the following &lt;tt class="docutils literal"&gt;broker.js&lt;/tt&gt; script, placed in this same directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cm"&gt;/* a &amp;quot;blocking&amp;quot; query */&lt;/span&gt;
&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nb"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;start&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;: query started&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nb"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;start&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
  &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nb"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; query finished&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;


&lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="c1"&gt;// normally, here would probably be:&lt;/span&gt;
  &lt;span class="c1"&gt;// process.send(query(m))&lt;/span&gt;

  &lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;900&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;send&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Obviously, this is quite a a toy example, because our worker receives and returns only one single message before shutting down.
Similarly, our server API for this broker in &lt;tt class="docutils literal"&gt;app.js&lt;/tt&gt; will be just as simple:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;broker&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;child_process&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;channel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;broker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;fork&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;__dirname&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/db.js&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;send&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="nx"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;error&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Go ahead and replace everything in &lt;tt class="docutils literal"&gt;app.js&lt;/tt&gt; with this code.
In a real-world program, you would use a pool of workers and, each time you start a &lt;tt class="docutils literal"&gt;query&lt;/tt&gt;, use another channel from your pool.
As a matter of fact, most Node libraries will provide you with non-blocking versions of the API they wrap, so you might need no workers at all.
Rather, you are more likely to start scaling Node across multiple cores and even machines using workers and IPC events.
However, this will be enough to run our queries in parallel and demonstrate idioms that can help you write your code to be highly concurrent and easy to scale.
For demonstration purposes this setting will do, so let's start coding our &amp;quot;reactive&amp;quot; app!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-simple-json-rest-service"&gt;
&lt;h2&gt;A Simple JSON-REST Service&lt;/h2&gt;
&lt;p&gt;Let's assume we want to provide a JSON-REST web service connecting a query for a blogger ID against the broker that in turn interfaces to some form of external API or DB.
Upon receiving the user's ID, the server should query the broker for the user's data, and then, in a second step can use that data to fetch any posts the user wrote and any comments she made.
In other words, we have a serial flow of fetching user and the an parallel step of fetching post and comment data.
Once all this data is collected, the server should respond with a JSON object containing this data.&lt;/p&gt;
&lt;p&gt;In the simplest case, you would string a few calls to your query API together and return the results, maybe like this (&lt;em&gt;anti-pattern&lt;/em&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;express&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)()&lt;/span&gt;

&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/sync/:userId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;

  &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;users&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;poster&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;comments&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;commenter&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="p"&gt;})&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;


&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;listen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(Again, you can append this to the app.js you created earlier, just as all following code snippets.)
The above example gives rise to the (wrong) argument why callbacks are a Bad Idea (you will learn the true reason why callbacks are evil later):
By placing one CB inside the former, or returning (think &amp;quot;break&amp;quot;) early if an error occurred, we end up in &lt;a class="reference external" href="http://callbackhell.com/"&gt;callback hell&lt;/a&gt;.
In other words, code that is no longer maintainable and hard to unit test, as we will see.
Worse, we just lost the advantage of using Node... this code is actually being run synchronously, one broker query after the next!
If you run this app in the shell and query the URL (e.g., &lt;tt class="docutils literal"&gt;curl &lt;span class="pre"&gt;http://localhost:8000/sync/example&lt;/span&gt;&lt;/tt&gt;), you will get something like this output:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ node app.js
1381334176812: query started
1381334177793 query finished
1381334177858: query started
1381334178830 query finished
1381334178897: query started
1381334179868 query finished
&lt;/pre&gt;
&lt;p&gt;As can be seen, despite our best intentions, all queries are run one after the next instead of running the last two in parallel.
So we need to dispatch our last two &lt;tt class="docutils literal"&gt;query&lt;/tt&gt; calls simultaneously once we have the &lt;tt class="docutils literal"&gt;user&lt;/tt&gt; data and use the callbacks to collect the results.
In addition, this will allow us to &amp;quot;unstring&amp;quot; this callback chain a bit.
This takes us straight into the realm of asynchronous programming.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="home-brew-asynchronicity"&gt;
&lt;h2&gt;Home-brew Asynchronicity&lt;/h2&gt;
&lt;p&gt;While this certainly is not the cleanest code, here is a quick shot at solving the problem of concurrently running these three queries:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/brew/:userId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;cbCounter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;gotError&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;

  &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;checkError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;gotError&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;gotError&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
      &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="nx"&gt;cbCounte&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;users&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;checkError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;

      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;poster&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;checkError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;posts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;
          &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cbCounter&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;

      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;comments&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;commenter&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;checkError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;comments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;
          &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cbCounter&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;})&lt;/span&gt;
      &lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="p"&gt;})&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;While this version now runs the last two queries concurrently, and, if you have more than one CPU core at least, in parallel, the code has become quite bloated.
You need to ensure that all three calls have completed before returning a valid result.
You need to ensure to only return the error once.
And you need to make sure you have collected the intermediate results.
Last, if you add another query, you might forget to update the counter checks, use a wrong counter value, or forget to store the intermediate result.
Luckily, these issues have been solved with the second most popular node module used by fellow Node coders, &lt;a class="reference external" href="https://github.com/caolan/async"&gt;async&lt;/a&gt; (the most popular dependency being ... you guessed it, &lt;a class="reference external" href="http://underscorejs.org/"&gt;underscore&lt;/a&gt;, which we will use in a bit, too).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="controlling-data-flow"&gt;
&lt;h2&gt;Controlling Data Flow&lt;/h2&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;$ npm install async&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;We wil use the &lt;a class="reference external" href="https://github.com/caolan/async#auto"&gt;auto&lt;/a&gt; method of &lt;a class="reference external" href="https://github.com/caolan/async"&gt;async&lt;/a&gt; to make the code more readable, leaving all the mentioned issues up to the library to take care of.
&lt;tt class="docutils literal"&gt;auto&lt;/tt&gt; takes a &amp;quot;tasks object&amp;quot; (an object where each property is a function considered a task) and returns an equally shaped object with the results if all tasks were run without errors.
In addition to the tasks, you can list any other tasks you handed to auto that have to be completed before that specific task is run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;async&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;async&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/async/:userId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;

  &lt;span class="nx"&gt;async&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;auto&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;users&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;poster&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}],&lt;/span&gt;
    &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;commenter&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="nx"&gt;cb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}]&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As shown, we send the auto function an object with three tasks, &lt;tt class="docutils literal"&gt;user&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;posts&lt;/tt&gt;, and &lt;tt class="docutils literal"&gt;comments&lt;/tt&gt;.
Both the &lt;tt class="docutils literal"&gt;posts&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;comments&lt;/tt&gt; tasks require that the &lt;tt class="docutils literal"&gt;user&lt;/tt&gt; task has been run successfully before them, as indicated by the array value they map to.
The tasks (functions) have to accept a standard Node callback (&lt;tt class="docutils literal"&gt;function(err, result)&lt;/tt&gt;) and should delegate that to some asynchronous task.
In the case of dependent functions, they can also accept the result of the prior task(s).
The overall outcome then is pushed into another callback that is the last argument of &lt;tt class="docutils literal"&gt;auto&lt;/tt&gt;;
This CB will receive the error status or, if no error occurred, the result of each individual tasks, in the same &amp;quot;shape&amp;quot; as the object sent to auto.&lt;/p&gt;
&lt;p&gt;We have now solved our issues and have achieved our goal of a clean coding of our asynchronous tasks.
However, if you later need to write more complex code, the control to structure defined by your callbacks still gets back to you.
Because you are sending the data as well as the callbacks along your function chain, you are mixing two things you should keep separate:
In a nutshell, you are coding the &lt;strong&gt;control flow&lt;/strong&gt; of your program, while instead you should be describing what is known as &lt;strong&gt;data dependencies&lt;/strong&gt; and let the Node engine figure out when to run which task (callback).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-promised-land"&gt;
&lt;h2&gt;The Promised Land&lt;/h2&gt;
&lt;p&gt;As a matter of fact, with &lt;a class="reference external" href="https://github.com/caolan/async"&gt;async&lt;/a&gt; you will be able to write concurrent Node.js apps perfectly fine.
This last section will introduce you to concepts that will help you get rid of the problems described at the end of the former section.&lt;/p&gt;
&lt;p&gt;To avoid the use of control flow statements, declarative languages instead have developed the concept of the &lt;strong&gt;future&lt;/strong&gt;, also known as a &lt;a class="reference external" href="http://promises-aplus.github.io/promises-spec/"&gt;promise&lt;/a&gt;.
So far, we were using callbacks, that is, functions that do not return a value. This means they are hard to use when &lt;em&gt;composing&lt;/em&gt; new functions from them and, as they do not return a value, are exclusively executed for their &lt;em&gt;side-effects&lt;/em&gt;.
This means that both &lt;em&gt;function composition&lt;/em&gt; and &lt;em&gt;unit testing&lt;/em&gt; of these callbacks is rather a pain.
And it means that you have to manually control the &amp;quot;flow&amp;quot; of your data through these callbacks.
For example, to read a file in Node, the idiomatic structure is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;underscore&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;finish&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;sendData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;finish&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;reportError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;writeHead&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;finish&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;onResult&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;reportError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="nx"&gt;sendData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;callback&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;onResult&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;readFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;callback&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead, if the &lt;tt class="docutils literal"&gt;readFile&lt;/tt&gt; function were to return a value, this could be expressed in a much cleaner way, without having to nest (structure) the tasks into callback chains.
However, as we know, at the time of calling &lt;tt class="docutils literal"&gt;readFile&lt;/tt&gt;, no such value exists - as a matter of fact, we do not even know if we will be able to read the file at all.
So the only thing we can return is a &amp;quot;future&amp;quot; value, or a &amp;quot;promise&amp;quot; to return such a value (hence the name of these data structures).
For this reason, such a data type would have to provide three methods:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;reject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;reason&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;onFulfilled&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;result&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nx"&gt;onRejected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;error&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The called function that returned the promise will, in the future, decide to resolve or reject the promise.
And that new promise will &lt;tt class="docutils literal"&gt;then&lt;/tt&gt; have to describe what the program should do if the promise has been &lt;em&gt;fulfilled&lt;/em&gt; or &lt;em&gt;rejected&lt;/em&gt;.
The most important concept is that promises are &lt;strong&gt;propagating&lt;/strong&gt;:
The return value of &lt;tt class="docutils literal"&gt;then&lt;/tt&gt; (&lt;tt class="docutils literal"&gt;result&lt;/tt&gt; in the above API description) is yet another promise (the &amp;quot;output promise&amp;quot;):&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;If you return a value from any of the two handlers (fulfilled, rejected), the output promise will get fulfilled, too.&lt;/li&gt;
&lt;li&gt;If you throw an exception in any of the two handlers, the output promise will get rejected.&lt;/li&gt;
&lt;li&gt;If you return yet another promise from any of the two handlers, the &lt;tt class="docutils literal"&gt;result&lt;/tt&gt; will &lt;em&gt;become&lt;/em&gt; that promise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means you can comfortably chain promises, while you can rely on &lt;em&gt;return values&lt;/em&gt; (instead of side effects).
With this, we now can separate the concerns and instead describe the asynchronous reading of a file using the promise:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;Promise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;q&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;// wrap fs.readFile as a &amp;quot;promise-returning function&amp;quot;:&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;readFile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;Promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nfbind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;readFile&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;setHeader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;header&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setHeader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;header&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;set200&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;setHeader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;set500&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;setHeader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;readFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;set500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;send&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;set200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;send&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;write&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;fin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;end&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Promises therefore make it possible to resolve function values in a &lt;em&gt;time-independent&lt;/em&gt; manner:
We can call the &lt;tt class="docutils literal"&gt;then&lt;/tt&gt; method after or &lt;em&gt;before&lt;/em&gt; either &lt;tt class="docutils literal"&gt;resolve&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;reject&lt;/tt&gt; have been executed.
Furthermore, libraries that implement promises must guarantee that no matter how often we check the promise' state (call &lt;tt class="docutils literal"&gt;then&lt;/tt&gt;), that promise always will be resolved or rejected the same way.
We have also eliminated the linear dependencies of each function on the next, thus our functions become much more convenient to reuse.
Last, this version is actually safer than the callback implementation:
If setting the header fails for any reason, this gets logged, and the parameter-less &lt;tt class="docutils literal"&gt;fin&lt;/tt&gt; method is &lt;strong&gt;always&lt;/strong&gt; executed with &lt;tt class="docutils literal"&gt;res.end()&lt;/tt&gt;, ensuring all responses will be closed.&lt;/p&gt;
&lt;p&gt;This makes promises ideal to resolve I/O bound (blocking) tasks.
(Do not use promises for CPU intensive tasks!
Protip: If reading blogs about comparing promise libraries, make sure the tests are using the right kind of task.)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="from-control-flow-to-data-dependencies"&gt;
&lt;h2&gt;From Control Flow to Data Dependencies&lt;/h2&gt;
&lt;p&gt;As we have seen so far, the elegance of using promises instead of callbacks is that blocking functions now can &lt;em&gt;return values&lt;/em&gt; (promises) instead of requiring you to &lt;em&gt;send functions&lt;/em&gt; (callbacks) to them.
In other words, promise-returning functions behave like any other function.
Furthermore, the return value of &lt;tt class="docutils literal"&gt;then&lt;/tt&gt; will always be another (&amp;quot;output&amp;quot;) promise, possibly even the return value from or exception thrown in either handler sent to it.
Because of this behavior, you can keep chaining promises over your success (&lt;tt class="docutils literal"&gt;onFulfilled(result)&lt;/tt&gt;) and error functions (&lt;tt class="docutils literal"&gt;onRejected(error)&lt;/tt&gt;), and the final outcome will always be &lt;em&gt;the same&lt;/em&gt; result value or error.
As you return promises from your functions instead of having to send them continuation functions, you have separated the control flow from your code.&lt;/p&gt;
&lt;p&gt;If you want to learn the nitty-gritty theory behind the benefits of moving from callback-based control flow to data dependencies encoded as promises, James Coglan has an &lt;a class="reference external" href="http://blog.jcoglan.com/2013/03/30/callbacks-are-imperative-promises-are-functional-nodes-biggest-missed-opportunity/"&gt;excellent blog post&lt;/a&gt; on that matter.
And if you need more hands-on experience than what we have discussed so far, &lt;a class="reference external" href="http://strongloop.com/strongblog/promises-in-node-js-with-q-an-alternative-to-callbacks/"&gt;StrongLoop&lt;/a&gt; provides a very comprehensive tutorial of using promises.
We instead will directly get our hands dirty and use this knowledge to describe our three queries problem in terms of promises.
To do so, we will require the popular &lt;a class="reference external" href="http://documentup.com/kriskowal/q/"&gt;Q&lt;/a&gt; and &lt;a class="reference external" href="http://underscorejs.org/"&gt;underscore&lt;/a&gt; libraries:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ npm install q
$ npm install underscore
&lt;/pre&gt;
&lt;p&gt;Without much fanfare, this is how our JSON-REST service looks like using promises and some functional trickery:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;Q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;q&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;underscore&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;q_query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;Q&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nfbind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;db_query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;fetchPostsAndComments&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;Q&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;all&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nx"&gt;q_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;posts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;poster&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;}),&lt;/span&gt;
    &lt;span class="nx"&gt;q_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;comments&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;commenter&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
  &lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;respond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;json&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;user&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;posts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;comments&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/promise/:userId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;
  &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;respond&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nx"&gt;q_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;users&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fetchPostsAndComments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;spread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So what has changed?
Admittedly, except for the main function itself being a bit shorter, this code is longer than the asynchronous version.
Nonetheless, after introducing you to the benefits of promises, you should be able to see the elegance of this final solution.
In particular, because of the separation of concerns, your code has become much easier to unit test.
And because each function is clear and expressive, the code is easy to understand.
Last, we have reached our goal of three statements per function, too.&lt;/p&gt;
&lt;p&gt;Well, I hope to have enlightened you in a way or another and maybe made you a better Node developer!&lt;/p&gt;
&lt;/div&gt;
</summary><category term="javascript"></category><category term="node.js"></category></entry><entry><title>A new "home" for fnl.es</title><link href="http://fnl.es/a-new-home-for-fnles.html" rel="alternate"></link><updated>2013-07-01T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2013-07-01:a-new-home-for-fnles.html</id><summary type="html">&lt;p&gt;After months of looking for a loophole in my calendar to migrate my rather outdated and mostly untended Blogger site to something more modern, I have found the ideal timing: The birth and arrival of my son, Alexander LV (&lt;a class="reference external" href="https://twitter.com/AlexThe55th"&gt;&amp;#64;AlexThe55th&lt;/a&gt;)!
(Because here in Spain even daddies get two to three weeks off.)&lt;/p&gt;
&lt;img alt="Alexander taking a sun bath." src="http://fnl.es/images/13-07-01_initial-alexander.jpg" style="width: 75%;" /&gt;
&lt;p&gt;I will reduce the amount of personal entries and augment the articles about (mostly biological) data mininig issues, in particular related to text mining and software engineering.
Now and then I intend to throw in some issues about privacy, human rights, travelling and all those other things that interest me and make life worth living.&lt;/p&gt;
&lt;p&gt;As blogging framework I have decided to move on to &lt;a class="reference external" href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;, because it is &lt;a class="reference external" href="http://www.python.org/"&gt;Python&lt;/a&gt;-based, generating extremely light-weight static pages from &lt;a class="reference external" href="http://sphinx-doc.org/rest.html"&gt;ReST&lt;/a&gt; files.
This allows me to easily maintain the blog from any terminal and its pythonic origins allow me to hack the framework if need be.&lt;/p&gt;
</summary><category term="blog"></category><category term="alex"></category></entry><entry><title>Installing a full stack Python data analysis environment on OSX</title><link href="http://fnl.es/installing-a-full-stack-python-data-analysis-environment-on-osx.html" rel="alternate"></link><updated>2013-02-11T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2013-02-11:installing-a-full-stack-python-data-analysis-environment-on-osx.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: Installing the Scientific Python stack from &amp;quot;source&amp;quot; has become a lot simpler recently and this tutorial was updated accordingly in November 2013 to use with OSX Mavericks and, in particular, &lt;strong&gt;Python 3&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Installing a full-stack scientific data analysis environment on Mac OSX for Python 3 and making sure the correct, underlying Fortran and C libraries are used is (was?) not trivial.
Thanks to Apple, parts of the required libraries are already on your box when you install XCode (code-named the &amp;quot;&lt;a class="reference external" href="https://developer.apple.com/library/ios/documentation/Accelerate/Reference/AccelerateFWRef/_index.html"&gt;Accelerate&lt;/a&gt; Framework&amp;quot;), and the remaining pieces can easily be installed due to the great &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; project.
In other words, for the &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms"&gt;BLAS&lt;/a&gt; optimizations this setup will use Apple's pre-installed &lt;a class="reference external" href="https://developer.apple.com/library/ios/documentation/Accelerate/Reference/AccelerateFWRef/_index.html"&gt;Accelerate&lt;/a&gt; framework and you can choose to add the &lt;a class="reference external" href="http://www.cise.ufl.edu/research/sparse/SuiteSparse/"&gt;SuiteSparse&lt;/a&gt; and &lt;a class="reference external" href="http://www.fftw.org/"&gt;FFTW&lt;/a&gt; libraries via Homebrew for some extra speed when factorizing sparse matrices and doing Fourier transforms.
This guide will describe how to properly install the following software stack on Mac OSX from their sources and ensuring all the relevant C/Fortran &amp;quot;acceleration&amp;quot; is available:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.scipy.org/scipylib"&gt;SciPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this stack, it is a breeze to add other cool data analysis tools such as &lt;a class="reference external" href="http://scikit-learn.org/stable"&gt;scikit-learn&lt;/a&gt;, &lt;a class="reference external" href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt;, &lt;a class="reference external" href="http://sympy.org/en/index.html"&gt;SymPy&lt;/a&gt;, or &lt;a class="reference external" href="http://github.com/pymc-devs/pymc"&gt;PyMC&lt;/a&gt; in your &lt;a class="reference external" href="http://www.virtualenv.org/"&gt;VirtualEnv&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="preparatory-setup"&gt;
&lt;h2&gt;Preparatory Setup&lt;/h2&gt;
&lt;p&gt;First, you need to make sure you have &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; installed and running without any issues:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew doctor
&lt;/pre&gt;
&lt;p&gt;If that produces any other output than:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Your system is ready to brew.
&lt;/pre&gt;
&lt;p&gt;you need to stop &lt;em&gt;right now&lt;/em&gt; and fix the issues or install &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; first.
Note that if you &lt;em&gt;upgraded&lt;/em&gt; to OSX Mavericks, you also need to upgrade your XCode command line tools (or download them if you have not installed them) by executing:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
xcode-select --install
&lt;/pre&gt;
&lt;p&gt;(And this means that you will have to re-install/compile most brew libraries, too, because of a change of XCode libraries...)
Once you have a clean version of Homebrew up and running, you can proceed to install the actual requirements.&lt;/p&gt;
&lt;p&gt;First, you need to install a Fortran compiler and &lt;a class="reference external" href="http://docs.python.org/3"&gt;Python3&lt;/a&gt; itself:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew tag homebrew/science
brew install gfortran
brew install python3
&lt;/pre&gt;
&lt;p&gt;All of these commands should work nicely and you should encounter no issues.&lt;/p&gt;
&lt;p&gt;Second, it is obviously necessary to set up a minimal Python environment.
This tutorial will be using &lt;strong&gt;distribute&lt;/strong&gt; and &lt;strong&gt;pip&lt;/strong&gt; to install Python packages:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
curl -O http://python-distribute.org/distribute_setup.py
python3 distribute_setup.py
curl -O https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py
python3 get-pip.py
&lt;/pre&gt;
&lt;p&gt;Note that you &lt;em&gt;do not&lt;/em&gt; need to prefix &lt;tt class="docutils literal"&gt;sudo&lt;/tt&gt; to any of this - because you installed Python 3 using &lt;a class="reference external" href="http://brew.sh/"&gt;Homebrew&lt;/a&gt;, you are relieved from having to &amp;quot;root&amp;quot; everything.
And you should consider using &lt;a class="reference external" href="http://www.virtualenv.org/"&gt;VirtualEnv&lt;/a&gt; and &lt;a class="reference external" href="http://nose.readthedocs.org/en/latest"&gt;nose&lt;/a&gt; for your Python development, too:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip3 install virtualenv
pip3 install nose
&lt;/pre&gt;
&lt;p&gt;With this setup, you have Homebrew plus Python 3000 with &lt;strong&gt;pip&lt;/strong&gt;,  &lt;strong&gt;nosetests&lt;/strong&gt;, and &lt;strong&gt;virtualenv&lt;/strong&gt; all set up.
This is a great start for any kind of Python development;
Normally, it is suggested to &amp;quot;stop&amp;quot; here and install all further Python packages only in each &amp;quot;virtual environment&amp;quot;.
However, this scientific stack you are building is quite a lot of work to set up (compile-wise), so it is a time-saver to have this stack installed globally and then make use of it via &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--system-site-packages&lt;/span&gt;&lt;/tt&gt; when creating a new virtual environment instead of having to install it each time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="numpy"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;First, download the latest stable &lt;a class="reference external" href="http://sourceforge.net/projects/numpy/files/NumPy/"&gt;NumPy sources from SourceForge&lt;/a&gt;.
By installing from source, NumPy will automatically detect that you are using OSX and therefore configure itself to use the &lt;em&gt;Accelerate&lt;/em&gt; framework for the BLAS/LAPACK optimizations:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 setup.py config
&lt;/pre&gt;
&lt;p&gt;Below &lt;tt class="docutils literal"&gt;atlas_info&lt;/tt&gt;, at the end the config output, you should see the following message:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
FOUND:
  extra_link_args = ['-Wl,-framework', '-Wl,Accelerate']
  extra_compile_args = ['-msse3']
  define_macros = [('NO_ATLAS_INFO', 3)]
&lt;/pre&gt;
&lt;p&gt;As NumPy recognized Accelerate, you can proceed with the installation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 setup.py build
python3 setup.py install
&lt;/pre&gt;
&lt;p&gt;If you installed &lt;a class="reference external" href="http://nose.readthedocs.org/en/latest"&gt;nose&lt;/a&gt; (as advised), you also can test that your installation is working correctly (note that you must &lt;em&gt;move to another directory&lt;/em&gt; than where you build NumPy before running the tests):&lt;/p&gt;
&lt;blockquote&gt;
python3 -c &amp;quot;import numpy; numpy.test('full')&amp;quot;&lt;/blockquote&gt;
&lt;p&gt;All tests should pass without errors or issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="scipy"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://www.scipy.org/scipylib"&gt;SciPy&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To use SciPy, you need to install &lt;a class="reference external" href="http://cython.org/"&gt;Cython&lt;/a&gt; and &lt;a class="reference external" href="http://swig.org/"&gt;SWIG&lt;/a&gt; first:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip3 install Cython
brew install swig
&lt;/pre&gt;
&lt;p&gt;Optionally, you can also install &lt;a class="reference external" href="http://www.openblas.net/"&gt;OpenBLAS&lt;/a&gt;, &lt;a class="reference external" href="http://www.fftw.org/"&gt;FFTW&lt;/a&gt; and &lt;a class="reference external" href="http://www.cise.ufl.edu/research/sparse/SuiteSparse/"&gt;SuiteSparse&lt;/a&gt; (for the AMD and UMFPACK libraries) for some extra speedups on Fourier Transform and sparse asymmetric matrix factorizations:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew install openblas
brew install fftw --with-fortran
brew install suite-sparse --with-openblas
&lt;/pre&gt;
&lt;p&gt;This step is probably recommended (although it is entirely optional).&lt;/p&gt;
&lt;p&gt;Next you now can fetch the &lt;a class="reference external" href="http://sourceforge.net/projects/scipy/files/scipy/"&gt;SciPy sources from SourceForge&lt;/a&gt; and build them:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 setup.py config
python3 setup.py build
python3 setup.py install
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;config&lt;/tt&gt; step is only there so you can make sure SciPy found the Accelerate framework and the UMFPACK/AMD SuiteSparse libraries.
The &lt;a class="reference external" href="http://www.fftw.org/"&gt;FFTW&lt;/a&gt; library you installed earlier with Homebrew is not listed in this output, but will be used during the build, too.&lt;/p&gt;
&lt;p&gt;As with NumPy, you can run some tests to ensure our installation is working properly after moving to another directory:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 -c &amp;quot;import scipy; scipy.test()&amp;quot;
&lt;/pre&gt;
&lt;p&gt;None of the tests should fail (except for KNOWNFAIL and SKIP tests, naturally).&lt;/p&gt;
&lt;p&gt;If you have come this far, congratulations! Everything from here on will be a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Cakewalk"&gt;cake-walk&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="matplotlib"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The next step is the installation of matplotlib:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip3 install matplotlib
&lt;/pre&gt;
&lt;p&gt;As it is trivial to install and only takes a few minutes, you might consider adding it to your virtual environments only.
However, the next packge that will be installed, IPython, makes use of matplotlib and is quite a hassle to install in every virtual environment.&lt;/p&gt;
&lt;p&gt;To ensure the plotting library is working, try this in an interpreter:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
&amp;gt;&amp;gt;&amp;gt; from pylab import \*; plot([1,2,3]); show()
&lt;/pre&gt;
&lt;p&gt;You should see a plot with a straight diagonal.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ipython"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Now it is time to install a great MATLAB-like interpreter and environment.
The first, optional, step is to install PyQt4 so you can use IPython's &lt;tt class="docutils literal"&gt;qtconsole&lt;/tt&gt;.
This is not required, but it is nice to render plots inline in a Qt terminal window, making the IPython &amp;quot;experience&amp;quot; more like MATLAB:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
brew install sip --with-python3
brew install qt --HEAD # currently, on Mavericks, the --HEAD option is required
&lt;/pre&gt;
&lt;p&gt;Finally, you need to &lt;a class="reference external" href="http://www.riverbankcomputing.com/software/pyqt/download"&gt;download&lt;/a&gt; and install PyQt4 using:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 configure-ng.py
make &amp;amp;&amp;amp; make install
&lt;/pre&gt;
&lt;p&gt;Apart from PyQt4, installing IPython itself is again straightforward:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip3 install ipython[zmq,qtconsole,notebook,test]
&lt;/pre&gt;
&lt;p&gt;To make sure the installation worked, execute the newly installed &lt;tt class="docutils literal"&gt;iptest3&lt;/tt&gt; script.
Again as before, there should be no failures.&lt;/p&gt;
&lt;p&gt;From now on, instead of &lt;tt class="docutils literal"&gt;python3&lt;/tt&gt;, you should be using &lt;tt class="docutils literal"&gt;ipython3&lt;/tt&gt; if you want to work in a Python interpreter and you have reached the &amp;quot;holy grail&amp;quot; of having set up a MATLAB-like scientific computing environment:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
ipython3 qtconsole --pylab=inline
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="additional-data-science-libraries"&gt;
&lt;h2&gt;Additional Data Science Libraries&lt;/h2&gt;
&lt;p&gt;Finally, here is a list of mature, interesting data science libraries that all will use the stack you just installed.
These could all go either into the global site-packages, or you can just add them to your projects in your virtual environments as needed.
In the latter case, do not forget to enable the globabl stack with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--system-site-packages&lt;/span&gt;&lt;/tt&gt; when creating a new &lt;a class="reference external" href="http://www.virtualenv.org/"&gt;VirtualEnv&lt;/a&gt;.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://scikit-learn.org/stable"&gt;scikit-learn&lt;/a&gt; machine learning library: &lt;tt class="docutils literal"&gt;pip3 install &lt;span class="pre"&gt;scikit-learn&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt; statistical data analysis: &lt;tt class="docutils literal"&gt;pip3 install pandas&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://sympy.org/en/index.html"&gt;SymPy&lt;/a&gt; symbolic computer algebra system: &lt;tt class="docutils literal"&gt;pip3 install sympy&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://github.com/pymc-devs/pymc"&gt;PyMC&lt;/a&gt; probabilistic programming environment (see this &lt;a class="reference external" href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"&gt;PyMC tutorial&lt;/a&gt;):
&lt;tt class="docutils literal"&gt;pip3 install pymc&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other noteworthy analytical tools include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.pytables.org/"&gt;PyTables&lt;/a&gt; large data management: &lt;tt class="docutils literal"&gt;pip3 install tables&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://rpy.sourceforge.net/"&gt;RPy2&lt;/a&gt; Python-R interface: &lt;tt class="docutils literal"&gt;pip3 install rpy2&lt;/tt&gt; (assuming you have R installed)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://github.com/pydata/patsy"&gt;patsy&lt;/a&gt; and &lt;a class="reference external" href="http://statsmodels.sourceforge.net/"&gt;StatsModels&lt;/a&gt; statistical models:
&lt;tt class="docutils literal"&gt;pip3 install patsy &amp;amp;&amp;amp; pip3 install statsmodels&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;E voilà&lt;/em&gt; - you now have a fully functioning environment for running
all kinds and sorts of statistical data analyses and developing machine
learning algorithms!&lt;/p&gt;
&lt;/div&gt;
</summary><category term="python"></category><category term="apple"></category><category term="data mining"></category></entry><entry><title>No "Smart"-Phone, Please</title><link href="http://fnl.es/no-smart-phone-please.html" rel="alternate"></link><updated>2011-08-06T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2011-08-06:no-smart-phone-please.html</id><summary type="html">&lt;p&gt;Out of my strong opinions about privacy, I avoid the total exposure of my private life to governmental organizations and trans-national cooperations that you (maybe unconciously) subdue yourself by owning a &amp;quot;smart&amp;quot; phone and storing your whole life's data in the Cloud.
And no, I am not being paranoid: There is hard evidence for the &lt;a class="reference external" href="http://www.zeit.de/digital/datenschutz/2011-03/data-protection-malte-spitz/seite-1"&gt;tracking&lt;/a&gt; (&lt;em&gt;and&lt;/em&gt; now [EDIT in 2013] the &lt;a class="reference external" href="http://www.guardian.co.uk/world/prism"&gt;sniffing&lt;/a&gt;) going on - &lt;a class="reference external" href="http://www.onread.com/book/Brave-New-World-191056/"&gt;Huxley&lt;/a&gt; and &lt;a class="reference external" href="http://www.george-orwell.org/1984"&gt;Orwell&lt;/a&gt; didn't even scratch the surface of what was to come…
So don't waste your time trying to &amp;quot;Whats-App&amp;quot; me or convince me to contribute &amp;quot;content&amp;quot; via my mobile to &amp;quot;people marketing&amp;quot; (social network) sites.
Yet another argument: my old-school phone cost me 20 Euros to buy, contract free, and costs be 9 Euros to run, per month - roughly, about one quarter the cost of &amp;quot;smart&amp;quot;-phones.
In other words, I am saving a small holiday every year, or about 500 Euros.
Don't tell me again you are saving money because you now have Whats-App...
On top of all that, it frightens me to see - in restaurants and bars, at dinner tables at home, or in any other place people normally would talk to each other - more and more people turn into &amp;quot;Matrix dummys&amp;quot; that rather stare at their gadget than actually interact with each other.&lt;/p&gt;
</summary><category term="privacy"></category></entry><entry><title>Rails: RSpec'ing controllers with declarative authorization AND AuthLogic</title><link href="http://fnl.es/rails-rspecing-controllers-with-declarative-authorization-and-authlogic.html" rel="alternate"></link><updated>2010-03-12T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2010-03-12:rails-rspecing-controllers-with-declarative-authorization-and-authlogic.html</id><summary type="html">&lt;p&gt;I just had a rough time figuring out how to bypass all the security
features of the Rails project I am developing to write decent controller
specs with RSpec. I am using AuthLogic as authentication module and
declarative authorization (DA) for exactly that. However, when I started
to write controller specs that would simulate (HTTP) GET requests, I ran
into a wall: I simply could not digg what the cleanest way would be to
bypass both AuthLogic and DA. Finally, after finding the right queries
in Google, I managed to get the necessary snippets. To avoid that the
same tedious task might befall you, here's what you need to add, e.g.,
to your spec_helpers directory - I called the file
&amp;quot;controller_helpers.rb&amp;quot;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;module&lt;/span&gt; &lt;span class="nn"&gt;SessionHelper&lt;/span&gt;
  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;current_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;
    &lt;span class="vi"&gt;@current_user&lt;/span&gt; &lt;span class="o"&gt;||=&lt;/span&gt; &lt;span class="n"&gt;mock_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;User&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;user_session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;user_stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;
    &lt;span class="vi"&gt;@current_user_session&lt;/span&gt; &lt;span class="o"&gt;||=&lt;/span&gt; &lt;span class="n"&gt;mock_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="no"&gt;UserSession&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="ss"&gt;:user&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;current_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_stubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;login&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;session_stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;user_stubs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;
    &lt;span class="no"&gt;UserSession&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stub!&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:find&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;and_return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;user_session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;session_stubs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_stubs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;logout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="vi"&gt;@user_session&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;nil&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;disable_authorization&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="no"&gt;Authorization&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ignore_access_control&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The trick is that, for AuthLogic, you can now &amp;quot;authenticate&amp;quot; the user
by the stubbed UserSession that returns a mocked User model. DA is less
complicated: the &lt;tt class="docutils literal"&gt;disable_authorization()&lt;/tt&gt; method is all that is
needed. Now, in your &amp;quot;spec_helper.rb&amp;quot;, you add this line to the top:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;require&lt;/span&gt; &lt;span class="no"&gt;File&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dirname&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;__FILE__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/spec_helpers/controller_helpers&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And this line somewhere in the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;Spec::Runner.configure&lt;/span&gt;&lt;/tt&gt; loop:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;SessionHelper&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, in your controller specs, it is more than trivial to disable
authorization and authentication at once - simply add the following
line, e.g., to your &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;before(:each)&lt;/span&gt;&lt;/tt&gt; definitions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;disable_authorization&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;login&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Voila - your GET requests pass; and you can even add stubs to your
User model, if needed, by adding them as key-value pairs to the
&lt;tt class="docutils literal"&gt;login()&lt;/tt&gt; call above! So now you can get back to make your specs
pass...&lt;/p&gt;
</summary><category term="rspec"></category><category term="rails"></category></entry><entry><title>MobileMe vs. SugarSync vs. DropBox</title><link href="http://fnl.es/mobileme-vs-sugarsync-vs-dropbox.html" rel="alternate"></link><updated>2009-05-26T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2009-05-26:mobileme-vs-sugarsync-vs-dropbox.html</id><summary type="html">&lt;p&gt;I now have tested MobileMe, SugarSync, and DropBox for quite a while to
decide which service to buy for syncing my “electronic life” between my
Macs (soon I’ll be managing two OSX Server blades, one Mini, and two
MBPs!). After this period, there is no doubt to me: I’m syncing my iCal
calendars and Address Book content via Google, my bookmarks with XMarks,
and everything else via DropBox.&lt;/p&gt;
&lt;p&gt;MobileMe’s iDisk is nothing more than a pain and a piece of junk, which
I honestly did not expect. After all the problems they had last year
launching Me.com, I thought they would have by now created a working
service. But the iDisk and syncing my PIM (Personal Information Manager
- I still use Yojimbo, as Evernote’s and Together’s handling of
encryption are pure patched add-ons) was just a [bad] joke: You even
need to buy extra software if you want to do file syncing, as iDisk’s
“offline” sync is so slow and error prone I could not believe Apple
dares to offer something like that. So you need to either use Lingon and
rsync to sync to your online mode iDisk which doesn’t win a medal for
simplicity, or buy something like ChronoSync - and that takes hours
(!!!) to ensure 10 GB of data in about 30-40k Files are synced, &lt;strong&gt;every
time&lt;/strong&gt;. All the more, ChronoSync may be the fastest and safest syncer in
the wild! What finally got me mad was the sync agent using 90% CPU all
of the time, at times virtually locking you out of your own machine,
while performing almost nothing. Finally, if you ever try to navigate
that online iDisk, get yourself a cup of tea, you will have plenty of
time to drink it up until that file is open…&lt;/p&gt;
&lt;p&gt;Compared to SugarSync, DropBox with its simplicity and real versioning
of files is significantly better performing than SugarSync, espcially if
we talk upload and real volume, and if you ever tried SugarSync, it is a
resource hugger (not as bad as iDisk, but it will stop your workflow).
So in the end the choice for me was based on “mutual exclusion”, there
is simply still no service that can hold the candle to DropBox - and
just got me Pro account. As I am writing this, I am syncing up dozens of
gigs of data to my 50 GB DropBox, and I hardly notice it happening!&lt;/p&gt;
&lt;p&gt;Oh, if you get yourself an account for DropBox, either the free 2 GB or
a full Pro account, I would appreciate if you register via &lt;a class="reference external" href="https://www.getdropbox.com/referrals/NTE0NTA0OTk"&gt;this link&lt;/a&gt;,
as it creates me some 500 MB extra space for referring you :o).&lt;/p&gt;
</summary><category term="apple"></category><category term="cloud storage"></category></entry><entry><title>My first visit to a volcano</title><link href="http://fnl.es/my-first-visit-to-a-volcano.html" rel="alternate"></link><updated>2009-05-01T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2009-05-01:my-first-visit-to-a-volcano.html</id><summary type="html">&lt;p&gt;Actually, I already considered myself very lucky this year for visiting
the jungle in the Amazons. I would have not thought I could repeat such
an experience that soon again, but was proven wrong shortly after. A few
weeks ago, my girlfriend spent some weeks in Spain and we used the last
long weekend we had together to go to Tenerife, Canary islands. There I
had the pleasure to visit yet another really strange and amazing place
in our world: a volcano! My girlfriend, who grew up in Mexico, thought
it was quite funny I had not seen a volcano before... Well, she has not
been on a glacier so far :-P. I can only recommend this place - if you
do not know where to go for a short trip for a few days, you just have
found it. We rented a very romantic small house on the north-eastern tip
of the island, far away from all the tourist places. The owner renting
it to us (Juan Ramón) is a very kind and friendly guy, and he even went
into the trouble to send me my keys and a whole load of money I forgot
in the house - without even taking a cent for the troubles, although I
kept insisting! (Anybody knowing me don't laugh! Okay, I know, typically
Flo...) The house itself is built into the cliffs, with a marvelous view
over the sea and the beach below, and a great terrace. Mayte, my
girlfriend, found it and I can only say this was a great choice. If you
feel like renting it, follow this &lt;a class="reference external" href="http://www.casas-turismo-rural.com/alojamiento.phtml?alojamiento=2740"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We spent five days there, which is just enough time to visit the island
in a hurry - time flew by like nothing. You have to ask permission to
access the volcano in the capital city, Santa Cruz, which will then be
granted to you for a following day at a certain hour. Naturally, we
managed to be there too late the next day, but arrived again the day
after and the friendly guys guarding the entrance let us up anyways. Oh,
and pleeease dress appropriately: you can't imagine the scores of stupid
(Russian) girls trying to walk a volcano in high heels and with almost
nothing on - freezing to death at 3.700 m ASL and not able to walk in the
rocky area... Well, at least this allowed us to be completely alone at the
top of the crater! Another great feat of the park is, as it's at 2000+ m
ASL, that even if it's cloudy down on the beach, it might be a gorgeous
day in the park!&lt;/p&gt;
&lt;p&gt;Finally, one last word of warning: do not take residence on the south
side of the island except if you like artificial touristic sprawls: the
whole area is one urban block of concrete and completely hideous. As far
as I can tell, the north-east tip is best, maybe the (north-)western
areas may have some nice hideaways from the touristic towns, too. As for
the climate: just expect 20-25 ˚C in winter, 20-30 ˚C in summer - great!
One final recommendation if you like driving on narrow, curvy mountain
roads: rent a powerful car - I haven't had so much fun riding a car in a
long time!&lt;/p&gt;
</summary><category term="mayte"></category><category term="spain"></category><category term="travel"></category></entry><entry><title>News, Swines &amp; Pigs</title><link href="http://fnl.es/news-swines-pigs.html" rel="alternate"></link><updated>2009-04-30T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2009-04-30:news-swines-pigs.html</id><summary type="html">&lt;p&gt;Usually, I prefer to steer free from the day-to-day mainstream news, yet
even I have to accept a low level of &amp;quot;noise&amp;quot; if I want to know at least
something about the most significant things going on. However, currently
I get the overwhelming feeling that the whole news world is grunting and
snorting like a pigsty. You guessed it, I am concerned about all this
&amp;quot;swine flu&amp;quot; reporting going on. As can be easily demonstrated, this
whole &amp;quot;pandemic alert&amp;quot; and panic-making has gone completely out of
proportion. It is yet another example of how ridiculous news agencies
exaggerate or even blur the facts, and this attitude might be much more
lethal than most biological illnesses if measured by its indirect
impact.&lt;/p&gt;
&lt;p&gt;First of, some flu virus classification: There are three types of
influenza virus: A, B, and C. B and C play only very minor roles as they
mutate slower, are less virulent, and affect far less species. Usually,
when talking about a flu virus, type A is implied. Type A influenza then
is further categorized by the HxNx nomenclature. The H refers to the
hemagglutinin (HA) lectin and the N to the neuroaminidase (NA)
glycoprotein. Both are found on the outside coat of the virus particle.
HA mediates the virus' binding to target cells, while NA is responsible
for the release of progeny virus from infected host cells. The numbers
denote the antibody response of the virus, ordered by historic discovery
- meaning, a virus with the same H/N number is identified by the same
&lt;em&gt;type&lt;/em&gt; of antibodies, which form part of your immune system/defense. HA
and NA are essential for the virulence (the relative ability to cause
disease) in terms of infectiveness and the epidemic capabilities of the
strain. H1N1 denotes the class of flu virus HA/NA proteins that is the
most commonly found form in human influenza. Our immune system defends
us by recognizing mainly those two proteins (the &amp;quot;antigenes&amp;quot;) through
our antibodies. This means, from a pure immuno-defense point of view,
this kind of strain is the most well known to the human body and immune
system. This is one of the reasons why the new Hong Kong avian flu, with
its H5N1 composition, is much more virulent than the current H1N1 swine
flu or any other &amp;quot;regular&amp;quot; flu.&lt;/p&gt;
&lt;p&gt;Endemic states and lethal properties: H1N1 most deadly appearance and
the worst pandemic in modern human history was what we now know as the
&amp;quot;Spanish Flu&amp;quot; in 1918; This specific influenza virus transformed its
endemic properties (the ability to propagate within one kind of species,
in this case birds) to a panzootic state (affecting animals including
humans - epizootic would be the intermediate state that does not affect
humans). Note that this pandemic occurred during WW I, largely
facilitating its spread. In general, any kind of virus capable of
overcoming the species barrier is potentially more dangerous, as it is
likely to carry genetic material and protein structures the newly
infected species has never seen before, therefor being much more
virulent and lethal than the existing endemic strains. However, the
Spanish Flu killed somewhere around 20 million people (taking
conservative, low estimates), and its symptoms were so strong it was
often misdiagnosed as some much more severe infection. Apart from the
HA/NA properties already mentioned, there is the actual RNA (viruses
commonly use RNA instead of DNA to carry their genetic information) that
a virus uses to encode its proteins and other functions that are
important to the survival and impact of a virus. Our immune defense
looks for RNA sequences those are different to any sequence found in our
body and &amp;quot;destroys&amp;quot; those foreign sequences by cleaving the strands into
non-functional pieces with the help of so-called RNases. For this to
work, the immune system therefor has to identify the RNA [as foreign].
But a strand of RNA coming from another species might actually contain
nucleotide base sequences our immune system does not recognize (because
it only recognizes already &lt;em&gt;known&lt;/em&gt; foreign sequences), making the virus
much more lethal. This process of changing the RNA and protein
configuration is amplified by two related mechanisms called &amp;quot;genetic
drift&amp;quot; and &amp;quot;antigenic shift&amp;quot;. In the case of the Spanish flu, the RNA
was very &amp;quot;new&amp;quot; to the human's immune system and had a very high mutation
rate: how often the RNA sequence changes, roughly the meaning of the
aforementioned genetic drift and antigenic shift. On the contrary, the
new &amp;quot;Mexican&amp;quot; swine flu virus has a very similar RNA composition to
regular virus strains found in humans, i.e. it does not appear to be
significantly more leathal than any other flu. The only known difference
to the regular flu circulating in humans is that it seems to affect more
younger than older people, possibly due to the fact that older people's
immune systems already have &amp;quot;seen&amp;quot; a similar version of this virus some
time ago and therefor have antibodies that recognize the HA/NA
glycoproteins.&lt;/p&gt;
&lt;p&gt;Now compare this swine flu against any regular flu by numbers: The
regular flu kills about 250,000 to &lt;em&gt;half a million&lt;/em&gt; people &lt;em&gt;per year&lt;/em&gt;.
This overhyped swine flu managed to kill &lt;em&gt;eight&lt;/em&gt; (8!) humans so far and
it has been confirmed to have infected about 150 people worldwide (WHO
data, 29th of April, 2009). I.e., on a daily average about a
&lt;strong&gt;thousand times more&lt;/strong&gt; people die from the regular flu than this new
strain. Regular flu is almost continuously spreading somewhere in the
world, i.e., if the WHO took this into account, we would be living in a
nearly constant influenza pandemic. The swine flu just now made it to
the last stage before &lt;em&gt;even being defined&lt;/em&gt; as a WHO &amp;quot;pandemic&amp;quot;: There
must be a few known infections in at least two countries. Recall this
definition and check the real numbers when somebody is talking about a
new &amp;quot;pandemic&amp;quot;. The infection with swine flu seems to be no more lethal
than with any other flu, so your chances of dying from the swine flu
outside of Mexico are so marginal it makes no sense to take them into
account, while if you do go to Mexico, all you seriously need to do is
make sure your current health state is good enough to survive any flu
anywhere, which is much more prevalent and 1,000 times more likely to
kill you by a global statistic&lt;em&gt;.&lt;/em&gt; But the main point is: there is
nothing dangerous or wrong about going to Mexico, at least concerning
the flu. I would be much more worried about drug gangsters and hijackers
there if I were you: They managed to kill several thousands of people
this year alone already. In other words, the WHO rulings and
suggestions, that are close to ridiculous given the circumstances,
combined with the media hype are about to isolate Mexico from the world,
which leads me to my final and most important point.&lt;/p&gt;
&lt;p&gt;In general, this &amp;quot;pig-hyped&amp;quot; flu without any review of its background,
no factual content, and exclusively based on beliefs and propaganda has
only one really worrisome influence: it is weakening and isolating
Mexico, both socially and economically. Our ignorance to real facts are
estimated to cost Mexico &lt;em&gt;City&lt;/em&gt; (&amp;quot;D.F.&amp;quot;) alone around $88 million &lt;em&gt;per
day&lt;/em&gt;, and this figure will need huge updates for the crash Mexico's main
(legal) economic sector, tourism, will suffer, plus the costs incurred
on the country as a whole. This number will be similar to or more likely
even exceed the daily cost of the U.S. oil war in Irak (estimated to
about $250 million, in case you didn't know) - the only good news being
that instead of about 100 (direct, not counting the indirect toll, which
is estimated to be around five times higher) deaths per day, the swine
flu's daily death toll is still below one. In other words, the combined
direct and indirect negative impact of this insubstantial media hype on
a close to imaginary &amp;quot;Mexican Flu&amp;quot; will cost and destroy much more lives
than the virus itself most likely ever will have been capable of. Media
propaganda crusades against a country nowadays have the same
socioeconomic impact as the largest &amp;quot;real&amp;quot; war in decades if measured by
the daily cost. Keep this in mind the next time you read news about
swines from pigs.&lt;/p&gt;
</summary><category term="politics"></category></entry><entry><title>Why I love Python 3.0: Unicode + UTF-8</title><link href="http://fnl.es/why-i-love-python-30-unicode-utf-8.html" rel="alternate"></link><updated>2009-04-27T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2009-04-27:why-i-love-python-30-unicode-utf-8.html</id><summary type="html">&lt;div class="section" id="tl-dr-summary"&gt;
&lt;h2&gt;tl;dr summary&lt;/h2&gt;
&lt;table&gt; &lt;tbody&gt;
&lt;tr&gt; &lt;th align="left"&gt; &lt;strong&gt;Python pre-3.0&lt;/strong&gt; &lt;/th&gt;
     &lt;th align="left"&gt; &lt;strong&gt;Python post-3.0&lt;/strong&gt; &lt;/th&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; str.encode &lt;/td&gt; &lt;td&gt; bytes.translate or (new) str.encode &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; str.decode &lt;/td&gt; &lt;td&gt; bytes.decode &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; unicode &lt;/td&gt; &lt;td&gt; str &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; unicode.encode &lt;/td&gt; &lt;td&gt; str.encode &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; unicode.decode &lt;/td&gt; &lt;td&gt; *n/a* &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt; str("x") == unicode("x") &amp;nbsp;&amp;nbsp; &lt;/td&gt; &lt;td&gt; bytes("x") != str("x") &lt;/td&gt; &lt;/tr&gt;
&lt;/tbody&gt; &lt;/table&gt;&lt;p&gt;This change in Python 3.0 might be more than useful for
anybody intending to write programs that use more than the ASCII
characters (A-z, 0-9, and some symbols), which, given how i18n'ed most
applications are today, is rather the norm than the exception. I also
hope to encourage my fellow Pythoneers to update to 3.0 as soon as
humanly possible, not only because of this change, but because of the
general advantages of Python 3.0 (aka &amp;quot;no-where near 3000&amp;quot;...).&lt;/p&gt;
&lt;p&gt;In case you do not understand the difference between Unicode and String
arrays, here is a short paragraph to get you started. A String (str in
pre-3.0 Python, bytes/bytearray in Python 3.x+) is a byte-array already
&lt;em&gt;bound&lt;/em&gt; to a specific character-lookup table (e.g. ASCII, Latin-1,
UTF-8, etc.) to find the correct representation for that String. Note
that this is not the &lt;em&gt;glyph&lt;/em&gt; itself you see on-screen, as this depends
on, e.g., what font you are using, and is handled by the GUI toolkit or
the terminal. A Unicode array (unicode in pre-3.0, str in 3.x+) on the
other hand is an array of &amp;quot;universal&amp;quot; bytes, so-called &lt;strong&gt;code-points&lt;/strong&gt;
usually managed as two-byte arrays, but has no native representation.
Therefore, to create something readable from an Unicode object, you have
to &lt;em&gt;encode&lt;/em&gt; its bytes by using a codetable, such as ASCII or UTF-16, to
the correct String representation (&amp;quot;&lt;em&gt;bind&lt;/em&gt; the Unicode array to a code
table&amp;quot;). On the contrary, to create a Unicode array from a String array,
you need to &lt;em&gt;decode&lt;/em&gt; (&amp;quot;unbind&amp;quot;) the String's coding to get the
&amp;quot;universal&amp;quot; (in quotation marks as not all programming langues have to
use base 16 integers (aka hex, or two bytes)) Unicode. If you are not
used to thinking in these terms, a general tip for pre-3.0 Python: your
program should, when handling String input (SAX parsers for example
already do the conversion for you), convert it to Unicode (decode the
Strings), and when outputing your Unicode arrays, convert them back to
the desired String representation (encode them) - while working with
Unicode internally to avoid bugs and possible exploits. A (rather
stupid, but you can interpolate the danger, I hope) snippet from
Python's Unicode HOWTO might exemplify this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;/&amp;#39;&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;u&amp;quot;&amp;#39;/&amp;#39; not allowed in filename&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looks good at first, but what about sending that function a String not
in any standard encoding? For example, the UTF-7 encoding for
u&amp;quot;/etc/passwd&amp;quot; is &amp;quot;+AC8-etc+AC8-passwd&amp;quot; - a nasty mistake if that file
is presented to a user... (the work-around in this trivial example is
obvious: just decode before the if-clause - or, even better, when the
string enters your program - and compare to u'/'). To summarize, in
Python (not so in C, for example!) a Unicode array consists of two-byte
elements (base 16 integers) called code-points, Strings are arrays of
bytes which are bound to a codetable that helps the Python interpreter
look up the bytes' character representations and send them to your
terminal or GUI. Unicode to String conversion is called encoding
(&amp;quot;binding&amp;quot;), String to Unicode conversion is decoding (&amp;quot;unbinding&amp;quot;). The
fact that, when using the Python shell, you see &amp;quot;real&amp;quot; characters for a
String or Unicode object is pure convenience and should not distract you
from how they truly work internally.&lt;/p&gt;
&lt;p&gt;After this lengthy Unicode vs. String intro, the best news first: if you
can allow yourself the luxury to program with any Python version and are
not dependent on external libraries, Python 3.0 is just made for you:
The new native String object is always a Unicode representation, and the
default encoding chosen for representing your strings is UTF-8. In other
words, if you use Python 3.0 and are happy with UTF-8, you no longer
have to worry about decoding your (byte) strings to Unicode arrays or
binding your Unicode code-points to the right (byte-) string
representations. While this might seem like something that should have
been done long ago, for historic reasons older programming languages
(plus Python pre-3.0) use ASCII as the default encoding, meaning you had
to look after de-/encoding the whole time when working with input/output
functionality of your programs and using most other languages other than
English - and even there you might want to have special characters
(don't be so naïve...). Sad side to this: what I am talking about here
is standard in Java...&lt;/p&gt;
&lt;p&gt;However, you no longer need to worry with 3.0: First, the totally
useless old String object (str) has been removed (to be exact, it could
be said it is now &amp;quot;integrated&amp;quot; into the bytes and bytearray objects),
including the even more ridiculous &amp;quot;encode&amp;quot; method for old str objects:
bytes and bytearray only support a &amp;quot;decode&amp;quot; message (to the new Unicode
str objects), while the intended use of str.encode, transforming Byte
objects that were represented as str objects in pre-3.0, like zip or
base64, now has to be done through a new method called &amp;quot;translate&amp;quot; on
the new bytes and bytearray objects in 3.0, or via encode on the new str
object. This was a dangerous duck typing strategy to have str.encode in
pre-3.0: as Unicode objects can and should have this method, too, but as
you could not tell if you were calling encode on a Unicode object or a
String object (without something like writing:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
assert isinstance(my_obj, unicode)
&lt;/pre&gt;
&lt;p&gt;before every call to encode, at least), you could have been decoding
Unicode and encoding Strings - and because Python was (yes, was (!) -
see below) as &amp;quot;nice&amp;quot; as to do auto-coercion for you, without very
thorough testing libraries such a bug could go unnoticed for a long time
in pre-3.0. So, my praise to whomever was responsible for that decision!&lt;/p&gt;
&lt;p&gt;On the other hand, the unicode object is now the new str object, sans
the even more useless and dangerous &amp;quot;decode&amp;quot; functionality: the new
(Unicode) str object only supports str.encode (for cases where you want
something else than UTF-8), while str.decode is finally dropped from the
Python Standarad Library. Obviously, you might have a system that does
not want UTF-8, and encoding your Unicode str to whatever schema you
need with str.encode the whole time would be a pain; To define a
different encoding globally, Python uses your &amp;quot;coding&amp;quot; declaration in
the first lines of your program as the default encoding schema for all
your new, shiny Unicode str objects. I.e., writing:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# -*- coding: funny-arab-dialect -*-
&lt;/pre&gt;
&lt;p&gt;will be enough if you have some strange language sporting glyphs that
require characters not found in the Unicode consortium's codetables, or
you might want to set it back to ASCII (the default in pre-3.0) if you
really need to ensure nothing other than good, old &amp;quot;7-bit&amp;quot; is output by
your program. On a side note: UTF-8 is compatible with ASCII, while
UTF-16 is not; i.e., an ASCII string encoded using the UTF-8 codetable
still gives the right characters, trying this with UTF-16 encoding does
not - and a good explanation why we have still not moved to UTF-16 in
general.&lt;/p&gt;
&lt;p&gt;Finally, the really dangerous auto-coercion of Python between Strings,
Unicode representations, and Byte arrays is gone for good. Your
message's argument types must now match the receiving object's type and
comparisons between the different types always evaluate to false. This
last change might sound drastic if seen from a purely rapid prototyping
view, but everybody with some intent on not going crazy while
programming will greatly appreciate this change. The bugs and exploits
stemming from wrong (en/de-) coding, or, let's say, too much duck typing
the str and unicode objects in pre-3.0 Python (yeah, I love to put the
fault on somebody else...) are finally gone! Also, as all Strings are
now represented as Unicode str objects, you no longer need to worry if,
while comparing two str objects, they are using the same encoding -
which was another fountain of bugs in pre-3.0 Python - as any String is
internally managed as universal Unicode.&lt;/p&gt;
&lt;p&gt;What is left to say? These changes are dramatic (even if they should
have been made already long ago with 2.0), and it will take a while
until Python 3.0 will have replaced 2.7 (the final, upcoming stable 2.x
release, which will warn you about code that will break with 3.0). But
the message should be clear: the effort of converting your libraries to
the next generation of Python is more than worth it, and the 2to3
converter should help if you had your encoding/decoding correct. If not,
converting to 3.0 might help you uncover some nasty bugs you were not
even aware of! Other reasons to &amp;quot;convert&amp;quot; would be:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;no more longs, which are now ints and unlimited in size (think of
what happend when reaching maxint before...),&lt;/li&gt;
&lt;li&gt;generator/views from most operations formerly returning lists (think:
time used for creating and garbage collecting those temporary lists),&lt;/li&gt;
&lt;li&gt;function annotations for metaclassing and advanced decorators,&lt;/li&gt;
&lt;li&gt;nonlocal scope (similar to LISPs lexical scope),&lt;/li&gt;
&lt;li&gt;dictionary comprehensions (&amp;quot;{k: v for k, v in my_dict}&amp;quot;) and set
literals (&amp;quot;my_set = {1, 2}&amp;quot;),&lt;/li&gt;
&lt;li&gt;and tons of streamlining the syntax and Standard Library.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</summary><category term="unicode"></category><category term="python"></category></entry><entry><title>Zeitgeist Addendum: The Venus Project</title><link href="http://fnl.es/zeitgeist-addendum-the-venus-project.html" rel="alternate"></link><updated>2008-10-05T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2008-10-05:zeitgeist-addendum-the-venus-project.html</id><summary type="html">&lt;p&gt;I just sent a mail to some of my friends of whom I hope they might be
interested in this subject. I am sure this will be a very hot topic in
certain circles very soon; So I was giving this some thought if I should
publish this entry on my blog or not. My final conclusion is that I give
a damn if anybody can, will, must, etc. associate me with the content of
this documentary I am describing and linking to here. I just think it is
to important for humanity - and much more than 10to100 if you allow me
this joke and understand it because you followed the news lately - than
that it should be a question of my personal integrity needing
protection, as the content is &lt;strong&gt;highly controversial&lt;/strong&gt; and more
anarchistic in its nature than anything I have encountered in my life so
far.&lt;/p&gt;
&lt;p&gt;However, before reading on, I want to remind you that you are a free
person and as such are free to stop reading this blog or watching the
documentary to which I will be linking you at any time - I think it is
awesome, but I can well imagine you may not. Also, I saw the first
Zeitgeist movie and thought it was rather extreme and not offering any
solutions, just another of these “everything is bad” documentaries we
have out there already, but no real content. Well, this second release
just starts out the same again, and in the first twenty minutes (sorry,
it will take two hours of your time, but if you like what I write here,
I promise you, it’s worth every second) I was very tempted to turn it
down had there not been the financial disaster in the US and my feeling
that this documentary might give me some answers. I promise you,
watching this movie will do way more than letting you understand what
just has happened the last two weeks. Enough bla-bla, let’s see what you
might be into and if you want to see it.&lt;/p&gt;
&lt;p&gt;First, Zeitgeist: Addendum (which I think should be called
Revelation…) will explain the current situation of the world and how
things tie up to the situation we are in. If you do not understand this
part, you might want to look at the first Zeitgeist, but it is actually
irrelevant to the cause. The second part speaks straight from my heart,
and what I, too, believe is true: we - as in humanity - now have the
technology to create a self-sustainable, free, and, most importantly,
equal society. It will also explain you why there is no long-term need
for the military, the police, the government, laws, economy, and many
other things we were trained/taught to believe are unrelenting
(de:unbeugsam, es:irreductible), essential, and necessary elements of
human society. This leads into the third part, showing you something I
have never seen before: a possible future, and - this is why I stated
that I have never seen this before - as a more beautiful vision than I
had imagined even in my wildest dreams. This is probably the most
crucial part of the whole movie: you can accept it or not, and I -
obviously - cannot promise you (for now) that it all will become reality
one day. But allow me to dream… Finally, and this is what makes this
movie so exceptional, it explains you something I was missing in all
these documentaries I have watched so far, even better than just a
possible future: it gives you five very simple tasks to accomplish which
will be more than sufficient to make this future our reality if we reach
a, what they call, “critical mass” - although I still believe it is not
a question of if we do it, just when (as in before we eradicate
ourselves from this universe). OK, enough from me, I think the movie
will either disgust you or whatever, or you will have the feeling I am
just now having: my eyes were never that wide open (and no, yours truly
is not on any drugs as in a similarly named movie ;-)).&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.zeitgeistmovie.com/"&gt;Zeitgeist Movie Site&lt;/a&gt; (download and/or contribute)&lt;/p&gt;
&lt;p&gt;The second and maybe much more important link is the one to the Venus
Project itself, which is the core element of this movie. You will much
better understand its implications after watching the movie, but feel
free to give it a look right away. Essentially, TVP is implementing a
post-scarcity economy where transparent, public technology and resource
exchanges have replaced the monetary system and the current market
economy while preserving productivity through cultural and social status
incentives.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.thevenusproject.com/"&gt;The Venus Project&lt;/a&gt;&lt;/p&gt;
</summary><category term="zeitgeist"></category></entry><entry><title>What were you up to this summer?</title><link href="http://fnl.es/what-were-you-up-to-this-summer.html" rel="alternate"></link><updated>2008-09-12T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2008-09-12:what-were-you-up-to-this-summer.html</id><summary type="html">&lt;p&gt;Obviously, from the huge gap between this and my last post you probably
already have imagined, mine was quite busy. To sum it up: I went to
Canada, then Mayte - that is my gorgeous girlfriend in case you still
don’t know! - was in Madrid for a whole month (happy me!!!) ad we went
to Tarragona, a city to the south of Barcelona, and to Bilbao. Finally,
I had to prepare for a talk in Turku, Finland, which was the last
“station” for me this summer.&lt;/p&gt;
&lt;p&gt;First things first, I’ll start with the Canada trip. This was more or
less all payed by my institute because I went to the world’s largest
bioinformatics conference, the ISMB in Toronto, to present my
&lt;a class="reference external" href="http://bcms.bioinfo.cnio.es"&gt;BioCreative MetaServer&lt;/a&gt; project the first time. Reception was quite
good, and my boss and I were both quite happy. The hotel we stayed at,
although expensive, was rather crappy: The second day we got back, a
colleague of mine found another guy in his room, just taking a shower!
After some inquiries at the desk, they could not “determine” how the
mistake happend and the manager went up with my friend to clear the
matters up. The guy there the even refused to change the room! So my
friend finally decided to do the only logic thing and moved to another
room… Nice hotel, especially considering we were paying almost Can$ 200
a night! The city of Toronto itself is very US-like, with skyscrapers
and a downtown that gets all crappy and full of homeless people during
the night. I also was kind of annoyed by the way people act there -
everyone is just a bit too friendly for my taste, it seems more like a
show than for real. Finally, the city surely is cosy in winter: all the
center is connected with tunnels so you do never have to go outside -
brr, that makes me freeze just thinking of it! Another funny thing:
while we were there, the largest bike theft ever was going on; later the
police caught a bike seller stealing most of them - they found a whole
2,800 (!!!) bikes in his home, his shop, and in various garages he had
to rent to store all those bikes!&lt;/p&gt;
&lt;p&gt;Mayte’s stay here without doubt was my personal highlight this summer,
and maybe even this year: she found a way to be here a whole month,
which we spent together almost every day (well, at least the nights…).
It will be a while before we can spend that much time together in a row
again! Anyways, apart from some good going out to “terrazas”, bars and
fancy night clubs here in Madrid, getting some even better dancing done
and listening to the latest electronic beats, we also did two trips to
the north of Spain and had a nice afternoon one weekend getting out of
the city sprawl and enjoying the great countryside - of which you can
ascertain yourself in the photos below. We went to Tarragona together
with Iris and Gonzalo to a Spa-Hotel with all sorts nice luxury like
Saunas, swimming pools, baths, massages, golf course, etc. and had a
very nice and relaxed weekend while celebrating Iris’ birthday. Actually
this is a really special occasion, as Iris usually loves to be with many
people and gives great parties with very many people (and nice Spanish
and Mexican girls … not that it would be any of my business nowadays!).
The second trip we made to Bilbao, enjoying the great landscape there,
making the obligatory - and worth while! - visit to the Guggenheim, as
well as having a heavy night out during the opening of the Semana
Grande, which had spectacular fireworks. While we were sitting in the
grass watching the fireworks, a group standing next to us with two guys
and a girl was watching just as delighted - when suddenly a really huge
explosion filled the whole sky above us in a startling view of little
stars falling down from the sky. The girl got rather frightened, jumped,
and ran a bit, when she finally just ducked “for cover” on the grass. I
admit it was not nice of us, but we had to laugh. Then her boyfriend
turned his head toward her and with a very nonchalant voice asked: “Que
tienes?” (“What’s up with you?” - “Was hast’n?”). The somewhat
frightened girlfriend returned to his side, while we keept giggling
every other time a huge rocket exploded and she started to make leeway…
Well, apart from that we had a great breakfast, too, with champagne and
caviar, of which I am sorry to say, I cannot give details. So, this city
is really worth a visit, especially if you have a person along who knows
her way around town, the bars, and the beaches. One day the time was
ripe for Mayte to go back, and I finally gave up my plans of taking here
hostage and let her go back - and now she is happy and far away from all
the bourgeois western world in South America… Man, do I miss her
company!&lt;/p&gt;
&lt;p&gt;The third and final trip this summer was going to Turku. What, you
really do not know what or where Turku is? No worries, I didn’t either.
To be honest, it took me some time to find an acceptable travel route to
that place… But every Turku inhabitant will quickly assure you that you
missed out on history: it was, until about a hundred years ago, the
capital of Finland, and is also the oldest town to be found in that part
of the world. So, you might be asking yourself quite rightfully,
what-the-hell was I doing in such a remote location? Well, I was
attending a text-mining conference there: the SMBM’08 (Semantic Mining
in Biomedicine) and was giving an almost one hour long presentation
about my platform. So you can imagine it took my quite some time to
prepare the talk and the data to present, and, as I never had given a
talk of that length, quite some cups of tea (thanks to a tip from
Mayte!) before the talk to keep me calm. As a result, I had the feeling
the presentation was quite some success and the response from the
audience more than good. Anyhow, the “real” success will have to be
measured when seeing how many new participants I could attract to the
platform with this presentation - time will tell! Apart from work, we
had a great boat tour through the hundreds of tiny islands along the
coast, went to a Finish smoke Sauna with heated baths - you cool
yourself of by jumping into the (very cold) sea! - and a great dinner.
This had a rather funny ending, when the organizers fooled around with
me: there was a quiz about Finish wildlife, which I apparently won. They
gave me a bottle filled with transparent liquid and some finish text
explaining the contents. Without much thought I assumed it was vodka and
immediately got some shot glasses and shared the bottle with the
conference attendants. Much to our surprise, it did not have the taste
of vodka, but was a special kind of water extracted from birch trees… I
guess, I have to rethink my attitude about alcohol! As for the Finish
people themselves, there are some quite notable things: first of, it
really is true, they are blond, almost all of them! Second, and I really
was quite astonished, I never met that many friendly - and really
friendly, not that artificial stuff you find in the US and Canada -
people in any country before. The only thing why I think they are
slightly crazy occurred to me when, while walking through town at about
15 ˚C, with jackets and long pants, we passed by an outdoor, public
swimming pool: it was full of Fins “simulating” that it still was
summer!&lt;/p&gt;
&lt;p&gt;Well, I am sure I forgot to cover some stuff, but then the post is
already long enough as it is. To all of you, have a nice autumn,
enjoying the last rays of sun - if you do not live in Spain!&lt;/p&gt;
</summary><category term="travel"></category><category term="mayte"></category><category term="biocreative"></category></entry><entry><title>Witches and Terrorists</title><link href="http://fnl.es/witches-and-terrorists.html" rel="alternate"></link><updated>2008-06-24T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2008-06-24:witches-and-terrorists.html</id><summary type="html">&lt;p&gt;The recent reports on renditions of terrorism suspects have reached an
alarming rate. Compare what happened during the middle ages and what is
occurring now again with slightly different circumstances:&lt;/p&gt;
&lt;p&gt;If you have one of these characteristics, you might be a
witch/&lt;em&gt;terrorist&lt;/em&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Red/&lt;em&gt;Dark&lt;/em&gt; hair,&lt;/li&gt;
&lt;li&gt;green/&lt;em&gt;dark&lt;/em&gt; eyes,&lt;/li&gt;
&lt;li&gt;female/&lt;em&gt;non-caucasian&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are considered a “target” by these traits, you can be
“rightfully” tortured by the inquisition/secret service.&lt;/p&gt;
&lt;p&gt;Governments worldwide, including most likely yours, are actively
ignoring our most basic rights in the name of “safety”. More and more
people are disappearing into “black sites”, tortured and ill-treated for
years and years - at levels where even the inquisition was not as bad as
compared to what is happening here and now, today. On the other hand, we
Japanese, EU, and US citizens have fought for so many centuries to
arrive at a status where everybody should understand the necessity for
human rights, free speech, etc.. It took two world wars and millions of
deaths to realize these trivial facts. Now governments are openly taking
away your rights from you again, trying to tell you they are
“protecting” you from a threat that virtually does not exist. And even
if it would exist, it does not give anybody the right to destroy the
life of any living person at their very foundations. If you do not act
now, you are either yourself approving that anybody, including you, may
be tortured for the sake of “security”, or are ignorant enough to
believe this will simply “go away” again some day.&lt;/p&gt;
&lt;p&gt;Please, do not let this continue; do not let humanity fall back into a
state that could be even worse than the middle ages. Any day you spend
not protesting against these inhuman and useless acts of random force
against humanity you are indirectly killing and torturing, too. Any day
this continues, your rights will deteriorate.&lt;/p&gt;
&lt;p&gt;If you do not protest or voice your opinion, you are indirectly
granting the governments of this world with the right to kill and
torture people at will. If you do not raise your voice, you, too, are
part of this machinery, killing and torturing because your silence is
set equal to agreeing to your government's actions in a democratic
world. Act now, and raise your voice against this madness - while we
still have the right of freedom of speech (or do we?).&lt;/p&gt;
&lt;p&gt;POST-EDIT: On an interesting side note, the only &lt;a class="reference external" href="http://www.petitiononline.com/stoper/petition.html"&gt;petition where anybody
worldwide can sign&lt;/a&gt;, has exactly 139 signees at the time of this
writing. How sad…&lt;/p&gt;
&lt;p&gt;POST-POST-EDIT: At least, there is an Amnesty International page against
Guantanamo Bay and illegal US torture &lt;a class="reference external" href="http://www.tearitdown.org/"&gt;here&lt;/a&gt;, which almost 140,000
people have signed so far - even though I think it should be dozens of
millions if humans were capable of using their head…&lt;/p&gt;
&lt;p&gt;POST-3-EDIT: And another very good video from AI published yesterday on
the subject: &lt;a class="reference external" href="http://www.amnesty.org/en/news-and-updates/video-and-audio/no-justification-for-torture-20080626"&gt;Torture can never be justified&lt;/a&gt;.&lt;/p&gt;
</summary><category term="human rights"></category></entry><entry><title>Amazonas 101</title><link href="http://fnl.es/amazonas-101.html" rel="alternate"></link><updated>2008-04-14T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2008-04-14:amazonas-101.html</id><summary type="html">&lt;p&gt;Colombia is one of the most magnificent countries I have been so far.
If you know a tiny bit of its history with all the bloody civil wars
which have been almost continuously tormenting the country since the
40s, it is more than astonishing to find that the people themselves are
very open and friendly. The lush green on the countryside and the
amazing beaches on the coasts, mixed with some really high mountain
peaks allow for a variety you will not find in many other countries.
Finally, as it is next to the equator, the climate is almost all year
round the same: warm and mostly sunny (well, Bogotá is at 2,600 ASL, so
bring a warm jacket for the capital...).&lt;/p&gt;
&lt;p&gt;The current president, Uribe, is sending military forces everywhere in
the country to protect the civilians and tourists from attacks from
guerilla and paramilitary groups. Although you might not agree that this
is the best solution to the problem, you have to admit that he so far is
probably the most successful president in terms of restoring relative
security - at the great price of sacrificing the peoples personal
freedom, obviously; Colombia probably never has been as safe in the past
few decades as it is now - and it has by far less hostage-takings per
year as most uninformed people would make you believe (by now, something
below 600 per year). In my travels, I never felt more unsafe than in any
place in Europe - at least if you learn to ignore that at every second
corner you see a small group of military personnel or police.&lt;/p&gt;
&lt;p&gt;Both major cities I visited, Bogotá and Cartagena, are really nice
cities - although the crazy traffic and taxi drivers in Bogotá can make
you feel a little ill to you stomach... Especially Cartagena is really
worth the visit, the old town being built in a style slightly similar to
the houses in Extremadura (Spain) - and not without reason probably,
as most Conquistadores came from this region of Spain. Yet, the bright
colors and plenty of flowers and green give the town a very unique and
lovely look. Last but not least, Cartagena is on the Caribbean coast and
therefore you have plenty of wonderful beaches to choose to spend your
day. On a general advice: usually, you take boats to get to those
beaches, which leave from the town center. Insist, really, to be taken
there by a tiny private boat (not more than 20 or so passengers) and do
not take one of the big ships: it cuts your beach stay in half, you are
dumped at a super-touristic aquarium nobody seemed interested in anyway,
and spend the boat time with hundreds of Colombian families (including
everything from grandmother to great-grand child). Not that it was my
worst experience ever, but there are more fun ways to waste your time...&lt;/p&gt;
&lt;p&gt;The major part of our travels - with my [now (*kiss*)] girlfriend
Mayte and two of her friends from Portugal and Spain, Xana and Ivan - we
spent in the Amazons. It is hard to describe the experience of visiting
the jungle, but if you have been to a real desert, on top of high
mountains or glaciers, or other really strange places, you know what I
am talking about. The biodiversity is so amazing you will never be able
to go back to a zoo or botanic garden without a knowing smile on your
face. I have no idea how many different plants I saw - including the
Victoria amazonica, the largest lotus in the world - but when one of our
guides was telling us about which plant heals what while in the jungle,
I got the impression of walking though a pharmacy... The animal life is
just as amazing: alligators, snakes, birds in all varieties and kinds,
spiders, sloths, insects of all sizes, piranhas (we even caught our own
- but as far as I can tell, it took us more chicken meat to catch them
than they yielded...), even dolphins (which get a pink tint when they
hunt because of their circulation - a really unique view) - plus another
few dozen or so I have missed to list here.&lt;/p&gt;
&lt;p&gt;Moving around the jungle might seem tough and dangerous at first - and
I make a bet it is, if you try to move cross-country straight through
the jungle for days. We were not intending to do that - we were moving
along the Amazonas river only - and always had at least a hut to sleep
in, with beds and mosquito nets in the most &amp;quot;extreme&amp;quot; cases. Most of the
time we spent in villages and towns (Leticia -&amp;gt; Puerto Nariño [a little
jewel in the Colombian Amazons and a must visit] -&amp;gt; Leticia/Tabatinga -&amp;gt;
and Manaus) in hotels, where you do not even need to use mosquito nets.
The greatest danger is you doing something stupid on your own. The
interested might want an advice on what to bring, so I'd suggest to
pack:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;lots of insect repellent (we used Relec extra fuerte, about 1 bottle
[100 mL each] per person and week),&lt;/li&gt;
&lt;li&gt;a light rain protection (if it rains really heavy - which it does
more or less every morning - just forget protection... this is more
against the wind and on boats),&lt;/li&gt;
&lt;li&gt;some long-sleeves and long pants (protection against sun and/or while
hiking in the jungle),&lt;/li&gt;
&lt;li&gt;decent (!) hiking shoes,&lt;/li&gt;
&lt;li&gt;very strong sun protection (factor 40 or more - I used 60!),&lt;/li&gt;
&lt;li&gt;a towel (no, this is no Hitchhikers Guide joke :-) - preferentially,
one of those micro-towels to save space and weight),&lt;/li&gt;
&lt;li&gt;a flashlight,&lt;/li&gt;
&lt;li&gt;a small first aid kit with antibiotics against diarrhea, pills to
dampen fever and Malaria effects (Ibuprofen 600, Malaron - just in
case - I strongly advise against taking those Malaria pills on the
trip as prevention if you do not go jungle-trekking for many days!
The side effects are rather ugly, and actually the pills only reduce
the effects of Malaria, they do not really help prevent it.),&lt;/li&gt;
&lt;li&gt;Suero (salt &amp;amp; electrolytes) powder, and a lots of it! This protects you
from dehydrating and diarrhea; My friends all laughed at me for
drinking a liter of it a day (it tastes rather ugly), but I was the
only of our group to not have to spend a day on the pot in the end...&lt;/li&gt;
&lt;li&gt;a very light (I used silk) sleeping bag, and&lt;/li&gt;
&lt;li&gt;something to pass time (books, card games, etc. - hey, it's Colombia,
no need to be in a hurry).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On a final shout-out: many thanks to my amazing travel companions (aka
&amp;quot;los pendejos/tottos&amp;quot;...) and some very special ones to the most
gorgeous girl in the world that I am more than just happy to have become
close with during this trip: You provided me with one of the most
exciting trips I ever have made in my life!&lt;/p&gt;
</summary><category term="travel"></category></entry><entry><title>GEO-4</title><link href="http://fnl.es/geo-4.html" rel="alternate"></link><updated>2007-10-28T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-10-28:geo-4.html</id><summary type="html">&lt;p&gt;If you don't know what &lt;a class="reference external" href="http://www.unep.org/geo/geo4"&gt;GEO-4&lt;/a&gt; (4th Global Environment Outlook) is, it is
about high time to get you informed. This is a recent publication by the
United Nations, explaining why this planet's ecosystem and the human
race is close to the point of no return, facing (unavoidable)
extinction. Let's hope this is what the authors (390 scientist backed by
another 1000 reviewers) intend it to be: the final &amp;quot;wake up call&amp;quot; for
&lt;strong&gt;all&lt;/strong&gt; of us.&lt;/p&gt;
&lt;p&gt;To provide you a few facts:&lt;/p&gt;
&lt;p&gt;We are currently, on a total, using 30 % more resources than earth can
provide for us; water contamination is the number 1 cause of death
worldwide; we are overfishing at a rate of 250 %; major regions of the
world's oceans having become oxygen dead zones (i.e. lifeless); in
Africa, food production over the past 20 years has declined by more than
10 %; somewhere between today and 2100 we will have reached a global
warming level which will lead to inevitable (&amp;quot;point of no return&amp;quot;)
extinction of the ecosystem (i.e. incl. humanity) - oh well, I could go
on... this report is approx. 400 pages! The only positive news I could
mine: at least the ozone layer will be recovering - although currently,
the Antarctic hole is still growing, being larger today than ever
before. The real message behind all this:
&lt;strong&gt;We have to act *now*!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One last warning: this might be the most depressing news you ever
encountered, yet also the most important if you were not aware of these
facts (I surely wasnt aware of that many....)&lt;/p&gt;
</summary><category term="environment"></category></entry><entry><title>Weekend Dance</title><link href="http://fnl.es/weekend-dance.html" rel="alternate"></link><updated>2007-09-17T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-09-17:weekend-dance.html</id><summary type="html">&lt;p&gt;As it is still summer time here in Madrid (well, at least for us
northern foreigners), I just enjoyed a all-night open air DJing event in
t-shirt and short pants this weekend. Remember this is Madrid, so
nothing ended before the sun chased us away! It also was the first time
I saw Massive Attack live, which turned out to be quite cool. They're
not so much about an energetic performance, but you could imagine them as
a mixture of Pink Floyd-ish showcasing and a De Phazz-like Mary Jane
atmosphere... Anyway, great concert with about 50k visitors in a full
arena. I even liked Sven Väth - but only only because I was standing
next to a wall for his performance, so his BPMs were doubled by the
wall's echo... Oh, and if you like slightly harder beats (they had tons
of good old hardcore samples in their mixes), put you ears on Vitalic, at
least live they rock! Another unknown tip and quite impressive was
Digitalism. Have phun listening!&lt;/p&gt;
</summary><category term="madrid"></category><category term="music"></category></entry><entry><title>TextMate Python and Django cheat sheet</title><link href="http://fnl.es/textmate-python-and-django-cheat-sheet.html" rel="alternate"></link><updated>2007-09-07T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-09-07:textmate-python-and-django-cheat-sheet.html</id><summary type="html">&lt;p&gt;After not finding anything appropriate, I decided to do my own reference
card (aka cheat sheet) for the pythonic and django commands you can use
in TextMate. If you want to have it, download it from &lt;a class="reference external" href="http://www.scribd.com/doc/7759743/TextMate-PythonDjango-Cheat-Sheet"&gt;here&lt;/a&gt;.&lt;/p&gt;
</summary><category term="python"></category><category term="django"></category></entry><entry><title>Travelling around Spain</title><link href="http://fnl.es/travelling-around-spain.html" rel="alternate"></link><updated>2007-08-19T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-08-19:travelling-around-spain.html</id><summary type="html">&lt;p&gt;In the last few weeks I had been to Cadiz and Almeria, both in
the south of Spain. Well, actually I was in none of those cities, but in
places close by.&lt;/p&gt;
&lt;p&gt;The first trip together with Gema was down to Cadiz, about 30 km south
of the city some of Gemas friends were renting a flat in &lt;a class="reference external" href="http://maps.google.es/maps?f=q&amp;amp;hl=en&amp;amp;geocode=&amp;amp;q=Conil,+Spain&amp;amp;amp;ie=UTF8&amp;amp;z=11&amp;amp;om=1"&gt;Conil&lt;/a&gt;, a
village nearby. South of Cadiz you basically find the probably most
beautiful beaches of Spain, at least that was my impression and what my
Spanish friends told me (and, on a side note, the most incredible amount
of beautiful girls, too...). The only downside is that it can be kind of
windy, but if you know it, just bring some very stable wind protection,
and you should be fine. Actually, the one day it was windy, I was quite
happy, because with the burning sun having a swim and then cooling in
the breeze is a real relief. We also visited Tarifa, one of the main
wind- and kite surfing sites of Europe. You never saw a village so small
just full of surf shops - if you ask me, more dense than the skiing
shops in the tourist-trap villages in Tirol. Also we spent some time in
Los Caños de Meca, right between Conil and Tarifa, which is a real hippy
town and has a cool bar which is made up a bit like a ship deck and is
above a cliff looking out over the Atlantic. Great place to have some
&amp;quot;copas&amp;quot; (drinks)! On our way back to Madrid we took the time to stop in
Cordoba, which might be, together with Sevilla, the hottest (in
temperature, man!) town of Spain. In the center you can visit an
enormous (like in really enormous) mosque, as Cordoba and Granade where
interchangeably the capitals of the Arabian empire during the medieval
ages. Now it houses several (!!!) churches inside and it is really worth
paying the 8 Euros entry. Gema was slightly angry about my cultural
blasphemy, as the first thing that came to my mind upon entering was
&amp;quot;and where is Indiana Jones?&amp;quot;...&lt;/p&gt;
&lt;p&gt;The second trip to Almeria in the southeast corner of Spain was
actually for a festival, the &lt;a class="reference external" href="http://www.creamfields-andalucia.com/"&gt;Creamfields&lt;/a&gt; in Spain. This festival is
exclusively for electronic music, and the lineup were bands and DJs like
Basement Jaxx, The Prodigy, or John Digweed. It's a 14 h festival
&amp;quot;only&amp;quot;, starting at 6 pm and lasting until about 8 am the next morning.
But believe me, that was by far enough for me, and it took me several
days to recover... The greatest thing about this location is that you
have the After Hour right smack on the beach, hanging out in the sun and
taking an occasional bath to cool down. Although it was great, I am not
sure to recommend it again: This year they had 50k visitors, and the
only access is a single little road, which meant we were jamming 2 1/2 h
in the car just to get in, and then another 1/2 h walk from the parking
place to the actual festival. Considering that they have more attendees
every year, I don't want to know how bad it will be next year. So if you
feel like going, I might recommend you come a day earlier and put up
tent there on the beach (actually, better in the wood at the side of the
beach - if you can't imagine why, just do as I recommend...).&lt;/p&gt;
&lt;p&gt;So, all in all, two really gorgeous trips around Spain, and I am
looking forward to some more in the days to come this year. The only
thing I could slap my ass for is that I on both occasions forgot to
bring my camera...&lt;/p&gt;
</summary><category term="spain"></category><category term="travel"></category><category term="music"></category></entry><entry><title>Good Programmers?</title><link href="http://fnl.es/good-programmers.html" rel="alternate"></link><updated>2007-05-08T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-05-08:good-programmers.html</id><summary type="html">&lt;p&gt;Just now again, I stumbled across one of these mind-numbing blog posts
telling you what makes a &amp;quot;good&amp;quot; - sometimes even &amp;quot;great&amp;quot; - programmer.
As always, it went on about programming before college, having
programmed in private, knowing all sorts of technologies, being witty
and intelligent, etc. etc. etc.. Google it, you'll find dozens of such
articles. On those standards I would be a great programmer, as I usually
fit almost any point on such ridiculous lists, just as most of us
programmers do to a certain extent, and for sure after some time in the
programming biz. First of, I do NOT consider myself a &amp;quot;great
programmer.&amp;quot; Why? Because I have actually worked alongside truly great
programmers. Also, I have worked with many a programmer, yet I still do
not even need all fingers of one hand to count up all &amp;quot;great ones.&amp;quot; So
what makes up a great programmer? Actually, you can sum it up to three
quintessential points, in increasing degree of difficulty: A good
programmer...&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;...knows his interface.&lt;/li&gt;
&lt;li&gt;...can communicate his work to anyone.&lt;/li&gt;
&lt;li&gt;...surpasses the time and space constraints of a project.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To make this a bit more clear: (1) pertains to all HCI
(human-computer-interface) related stuff like OS, IDE, or keyboard (!);
but also libraries, tools, and the knowledge of at least one programming
language close to the skill level of writing the API for it. There is no
need for somebody knowing several languages, if he can't handle at least
one really well. (2) Is about his ability to discuss the needs,
problems, requirements, etc. with his boss, a client, or a fellow mate
at just the right level of technical detail and conciseness necessary.
The third point finally is the most important of all, and usually never
even gets mentioned in most of these page-filling &amp;quot;what makes a good
programmer&amp;quot;-lists, much to my amusement (and is the main reason why I
just had to blog this...). This point implies, a great programmer is
capable of telling you how long his job will take (or at least will
conclude that he cannot make - for good reasons - a true estimate and
will know how to &amp;quot;bet on the safe side&amp;quot;), and he will then finish that
job by that estimate. Also, the way he has designed and implemented the
program will be such that others can follow as long as they have the
technical background (&amp;quot;best practices&amp;quot;, etc.), and will have all
interfaces and crucial nodes developed in a style which requires minimal
and necessary (i.e., optimal) documentation. Of these space and time
constraints (deadline, design, implementation, and documentation) all
will be fulfilled, and at least one of them will surpass what one was
expecting when the project started. I'd say it is usually the toughest
aspect making up great programmers.&lt;/p&gt;
&lt;p&gt;I know that almost all guys saying they're &amp;quot;good programmers&amp;quot; (just
because they're too many) fail at least two of these three requirements,
and I freely admit I have my weaknesses in some of them and therefore do
not consider myself in the top league (hey, I'm working on it ;)); but
at least I realize those are the guys pulling of the great stunts; not
the ones who started programming at age ten, do it sixteen hours a day,
and know &amp;quot;all&amp;quot; about twenty different IT technologies (believe me, you
just can't...). We all are somewhat like this at certain points in time,
but the great ones got the above three points right, and especially
those three; That's what makes them uniquely different.&lt;/p&gt;
</summary><category term="programming"></category></entry><entry><title>Must have apps for OSX</title><link href="http://fnl.es/must-have-apps-for-osx.html" rel="alternate"></link><updated>2007-04-06T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-04-06:must-have-apps-for-osx.html</id><summary type="html">&lt;p&gt;As everyone has one, I just thought I might as well also put together
my personal list of the most useful applications for your nifty Mac
running the one and only truly user-friendly, working windowed OS: X!
The lists are ordered by importance, some apps are non-free, but I
explain why I use them and why you might want to as well.&lt;/p&gt;
&lt;div class="section" id="system-tools"&gt;
&lt;h2&gt;System Tools&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://quicksilver.blacktree.com/"&gt;Quicksilver&lt;/a&gt; - application launcher, action and command window -
THE most important app to have on any Mac, just get it (and learn to
use it...).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://free.abracode.com/cmworkshop/"&gt;OnMyCommand&lt;/a&gt; - custom right click context menus. Great tool, I use
it for a tons of stuff from SVN repository interaction, file settings
and archiving, to things like &amp;quot;select a Madrid address to
automatically open the location in Google maps&amp;quot; or &amp;quot;translate the
selected text to English/Spanish.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://cyberduck.ch/"&gt;Cyberduck&lt;/a&gt; - is a great FTP/SFTP client, if you need one - and
definitely more elegant than using Finder...&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="editors"&gt;
&lt;h2&gt;Editors&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://aquamacs.org/"&gt;Aquamacs&lt;/a&gt; - is the one-in-all solution and my favorite emacs port
for the Mac. Although, I only use it for LisP and C programming now,
as I have TextMate (see below) for my Python and web development as
well as simple text editing.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.uoregon.edu/%7Ekoch/texshop/"&gt;TexShop&lt;/a&gt; - is the most comfortable WYSINWYG (what you see is NOT
what you get...) editor for LaTeX, which is the most reliable and
stable environment to write books and papers. Yet, you naturally just
can keep editing your TeX in emacs/Aquamacs...&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.macromates.com/"&gt;TextMate&lt;/a&gt; - editor is kinda cool in some senses, but has some
drawbacks: first, it ain't free - I have it because my research
institute bought a license for me without even asking. It doesn't
know LisP and C. Most features it has you can have with emacs or (if
things would come to worst) write them with ELisP for emacs instead
of learning the TM macro commands. Yet, some things are kinda cool,
especially the filebrowser and codebrowser I prefer over the emacs
Speedbar. It is great for Python, Ruby, XML, Perl, JavaScript and SVN
interaction (if you are damned to program Java, use Eclipse...). So
that, incredibly fast startup, and decent column selection (you can
have that in emacs with a plug, too!) are my reasons for using it. If
you are willing to spend the bucks, get it. If you want an all-in-one
solution stick with some emacs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="getting-things-done"&gt;
&lt;h2&gt;Getting Things Done&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.barebones.com/products/yojimbo/"&gt;Yojimbo&lt;/a&gt; - if you are only willing to spend money on just one
single application, this is it. This is sort of a multi-functional
archiver: it stores your serial numbers, passwords, credit card
details, invoices, interesting documents, webpages and pdfs, the
metro plan of your city - whatever, you name it. Stuff is properly
encrypted using AES for protection. You can sort, tag, file your
content in a very decent way and you actually get to immediately find
the information again. This thing should be standard in any OS X
distribution, makes life so much simpler.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://journler.com/"&gt;Journler&lt;/a&gt; - is a nice diary to keep track of what you are doing and
your ideas about it. I use it as my development diary (&amp;quot;lab journal&amp;quot;
would be the term, I guess...). You can add all sorts of media to
your entries (videos, sound (voice recordings!), images) and even
upload your entries to your blog. Cool!&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://bargiel.home.pl/iGTD/"&gt;iGTD&lt;/a&gt; - yet another GettingThingsDone client - if you are in a
situation like me and have 10k things in your head and another 10k
todos, this is what helps you keeping track of what's next.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.apple.com/iwork/keynote/"&gt;Keynote&lt;/a&gt; - from the iWork package; if you need to do presentations:
please do not try to infest your gorgeous Mac with crap software from
Redmond. Do your presentations with Keynote - their just better,
nicer and more aesthetic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="media"&gt;
&lt;h2&gt;Media&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.videolan.org/vlc/"&gt;VideoLANClient (VLC)&lt;/a&gt; - the media player for the Mac: no more
problems with missing audio/video codes, plays (almost) everything
and allows full screen for free (a two thumbs down on QuickTime for
this one!).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.getdemocracy.com/"&gt;Democracy&lt;/a&gt; - player, the most fun way to handle, download and
archive your video collection from YouTube, Yahoo! and Google Video,
Blogdigger, et al. as well as any clips, vids and movies you have
locally. Supports full screen viewing for free (as opposed to
QuickTime...), but ain't as versatile with codecs as VLC - but you
can teach Democracy to launch those vids in VLC.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.apple.com/itunes/download/"&gt;iTunes&lt;/a&gt; - is a must have if you own an iPod (or, eventually, an
iPhone...). If not, it is still the coolest way to keep track of your
podcasts, radio stations and browse your local MP3s (covers).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.apple.com/ilife/iphoto/"&gt;iPhoto&lt;/a&gt; - should come with your Mac anyway AFAIK. The most
practical way of staying on top of your ever-growing picture and
image collection while integrating nicely with other apps (like
Keynote, Journler and many others).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="safari-plugins"&gt;
&lt;h2&gt;Safari Plugins&lt;/h2&gt;
&lt;p&gt;If you are just such a Cocoa-fan as me, and prefer browsing with
Safari over Firefox (yes, OK, decent web development can only be done
with the fox, so you do need it anyway in this case...) here are some
plugs. There are some reasons to use Safari: integrates websites from
your Address Book into your Bookmarks, your bookmarks are system-wide
searchable, you can aggregate feeds very nicely, pushing URLs (weblocs)
to other apps is easy are just some. UPDATE: now back to the Fox again,
the plugins for it just rule! UPDATE (2008): and to Safari - high-speed,
nicest rendering of all browsers and WebKit rule even more!&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.kottke.org/05/02/saft-safari-plugin"&gt;Saft&lt;/a&gt; - is the only plugin I can recommend to buy. And if only for
the full screen mode. This is just so much cooler than even the
crappy IExploder 7 full screen mode, as it actually intelligently
hides what you do not want to see currently (status bar, booksmarks,
URL bar). Naturally, it has tons of other features, too - check it
out.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.inquisitorx.com/safari/"&gt;Inquisitor&lt;/a&gt; - is the coolest enhancement for the (Google) search
box and possibly the best free plugin. Just get it.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://pimpmysafari.com/plugins/safariblock-112"&gt;SafariBlock&lt;/a&gt; - is a good way to block images, popups and frames you
don't wanna see. Not as powerful as Pith or Saft, but a free
alternative.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</summary><category term="apple"></category></entry><entry><title>Skiing in Spain</title><link href="http://fnl.es/skiing-in-spain.html" rel="alternate"></link><updated>2007-03-27T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2007-03-27:skiing-in-spain.html</id><summary type="html">&lt;p&gt;Actually, when I decided to move to Spain I would never have thought of
skiing here. There are the Pyrenees, where you could expect some areas,
but I was not expecting anything anywhere else. Meanwhile, I have been
skiing in Asturias (north, center) and Sierra Nevada (south, with a view
of Africa!)... Especially the latter was quite convincing: a large skiing
area with a lot of people who can actually ski (for Tyrolean standards
quite hard to achieve!). Just thought it would be kind of crazy to be
skiing end of March in Spain, while having a glimpse at Africa and
enjoying almost 20 ˚C in the &amp;quot;valley&amp;quot; (that is, Granada).&lt;/p&gt;
&lt;p&gt;While I am at it: Granada is quite a beautiful town, and naturally there
is the Alhambra to visit (but I did not enter: you have to reserve at
least 2 months ahead if you want to get in...). There are more foreigners
here than in Madrid, naturally, and it seems to have slightly less extreme
temperatures than the capital. Nice place to live, I guess: 1 hour for
skiing, 1 hour to the beach!&lt;/p&gt;
&lt;p&gt;In summary, there are 3 decent areas for skiing in Spain: the Pyrenees,
Asturias and Granada/Sierra Nevada. I have not seen the first, but
everybody tells me it is the best place. Anyway, I'd say Granada is
definitely the fanciest and worth a visit, too. And then you can ski on
about any other mountain range here, too (e.g., Gredos, Guadarrama, etc.),
but for my taste, the slopes there are not quite as steep and long as I'd
like them.&lt;/p&gt;
&lt;/p&gt;</summary><category term="spain"></category><category term="skiing"></category></entry></feed>