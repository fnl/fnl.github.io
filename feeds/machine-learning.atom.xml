<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>fnl en España - Machine Learning</title><link href="http://fnl.es/" rel="alternate"></link><link href="http://fnl.es/feeds/machine-learning.atom.xml" rel="self"></link><id>http://fnl.es/</id><updated>2018-02-15T00:00:00+01:00</updated><entry><title>A quick reference for working with TensorFlow</title><link href="http://fnl.es/a-quick-reference-for-working-with-tensorflow.html" rel="alternate"></link><published>2018-02-15T00:00:00+01:00</published><updated>2018-02-15T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2018-02-15:/a-quick-reference-for-working-with-tensorflow.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Given the last few years of hype around Deep Learning, knowing one of those frameworks is probably no longer an option, at least if you are a professional Machine Learning engineer. Personally, I always favor free, open source solutions, so Apache MXNet would be the natural fit. However, I …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Given the last few years of hype around Deep Learning, knowing one of those frameworks is probably no longer an option, at least if you are a professional Machine Learning engineer. Personally, I always favor free, open source solutions, so Apache MXNet would be the natural fit. However, I must admit that for research &amp;amp; development, Facebook's PyTorch is probably the nicer API, and for operations &amp;amp; production, Google's TensorFlow is beyond doubt the best fit right now. And, as I've mostly moved out of academia/research lately, and am pretty dedicated to consulting by now (and loving it!), I am mostly interested in a tooling that I can put to good use in my day-to-day work. Hence, I have been - not without remorse - focusing on TensorFlow and want to share my personal notes with you, to use as a quick reference when setting up a new tensor graph.&lt;/p&gt;
&lt;p&gt;I mostly learned how to use TensorFlow from Aurélien Géron's book &amp;quot;&lt;a class="reference external" href="http://shop.oreilly.com/product/0636920052289.do"&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow&lt;/a&gt;&amp;quot;; So if you are familiar with it and my notes below remind you of it, that is not by accident: These are my modified excerpts, all taken from that book. In fact, if you really are interested in using TensorFlow, I cannot emphasize enough how much I recommend reading this book. And, if you are not familiar with the SciKit-Learn world, or simply want to learn how you can combine these two essential Machine Learning frameworks into&amp;quot;one ring to rule them all&amp;quot;, you need to read this book. Full-stop. In fact, if you are a serious Machine Learning practitioner, you either should have read it, or at least make sure you know the techniques presented in it already. Beyond just giving you tips, the book is chock-a-block full of up-to-date examples and highly relevant exercises. Aurélien even recently &lt;a class="reference external" href="https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb"&gt;released more example code&lt;/a&gt;, to produce G. E. Hinton's Capsule Network architecture (using dynamic routing) with TensorFlow, and generally ensures the practical material is very much up to date with the latest TensorFlow releases. (Disclaimer: I am in no way affiliated with Aurélien, O'Reilly, or would otherwise benefit from sales of this book!)&lt;/p&gt;
&lt;p&gt;That being said, let's dive right in!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="setup"&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;div class="section" id="miniconda"&gt;
&lt;h3&gt;Miniconda&lt;/h3&gt;
&lt;p&gt;I strongly recommend using the &lt;a class="reference external" href="https://www.anaconda.com/download/"&gt;Anaconda&lt;/a&gt; distribution to set up TensorFlow, and Python in general. And, for the more expert users, do go directly to &lt;a class="reference external" href="https://conda.io/miniconda.html"&gt;miniconda&lt;/a&gt; (&amp;quot;don't go over Anaconda, and don't collect 200MB of (irrelevant) bytes&amp;quot;). The packages you want to install (and that will also fetch all other, relevant dependencies, like Jupyter, SciPy, or NumPy) are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda install -c conda-forge tensorflow
conda install -c conda-forge scikit-learn
conda install -c conda-forge matplotlib
conda install -c conda-forge jupyter_contrib_nbextensions
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="graph-design"&gt;
&lt;h2&gt;Graph Design&lt;/h2&gt;
&lt;div class="section" id="back-propagation-basics"&gt;
&lt;h3&gt;Back-propagation Basics&lt;/h3&gt;
&lt;p&gt;Input is feed into TensorFlow's (static) computational graphs via &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/placeholder"&gt;tf.placeholder&lt;/a&gt; (&lt;strong&gt;placeholder&lt;/strong&gt;) nodes. Typically, those placeholders will be accepting a tensor &lt;tt class="docutils literal"&gt;X&lt;/tt&gt; and some array or matrix of labels &lt;tt class="docutils literal"&gt;y&lt;/tt&gt;. Input is part of the &lt;a class="reference external" href="https://www.tensorflow.org/api_guides/python/io_ops"&gt;io_ops&lt;/a&gt; package; Input values for placeholders must be provided (see &amp;quot;Feeding the Graph&amp;quot;) during graph evaluation, or cause exceptions if left unset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weights&lt;/strong&gt; &lt;tt class="docutils literal"&gt;W&lt;/tt&gt; and &lt;strong&gt;bias&lt;/strong&gt; &lt;tt class="docutils literal"&gt;B&lt;/tt&gt; tensors are represented by &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Variable"&gt;tf.Variable&lt;/a&gt; nodes, via the &lt;a class="reference external" href="https://www.tensorflow.org/api_guides/python/state_ops"&gt;state_ops&lt;/a&gt; package; &lt;strong&gt;Variables&lt;/strong&gt; are stateful, that is, they maintain their values across session runs.
&lt;strong&gt;Operations&lt;/strong&gt; are defined by either applying standard Python operators (+, -, /, *) to nodes, or by using the &lt;tt class="docutils literal"&gt;tf.add&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;tf.matmul&lt;/tt&gt;, etc. functions from the TF &lt;a class="reference external" href="https://www.tensorflow.org/api_guides/python/math_ops"&gt;math_ops&lt;/a&gt; packages.&lt;/p&gt;
&lt;p&gt;The final operation typically evaluates the error or cost between the predicted &lt;tt class="docutils literal"&gt;y_hat&lt;/tt&gt;, modeled as a state node, and [minus] the true &lt;tt class="docutils literal"&gt;y&lt;/tt&gt;, modeled as an io node, as shown above.
Common loss functions for this task are found in TensorFlow's &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/losses"&gt;losses&lt;/a&gt; package.
The outcome of this operation (by following Aurélien nomenclature, the &lt;tt class="docutils literal"&gt;cost_op&lt;/tt&gt; node) is the one you typically want to plot on your TensorBoard (more on that at the end of this post).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# n variables + 1 constant bias input&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# ... graph setup with tf.Variables ...&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="c1"&gt;# some last step...&lt;/span&gt;
&lt;span class="n"&gt;cost_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The final touch is to create and append the optimizer (e.g, &lt;tt class="docutils literal"&gt;tf.train.GradientDescentOptimizer&lt;/tt&gt;):
That creates the &lt;tt class="docutils literal"&gt;training_op&lt;/tt&gt; (for minimizing the cost/error), which typically will be the node that gets sent to evaluations of a TensorFlow graph via a &lt;strong&gt;Session&lt;/strong&gt; (&lt;tt class="docutils literal"&gt;sess.run&lt;/tt&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;training_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="placeholder-nodes"&gt;
&lt;h3&gt;Placeholder Nodes&lt;/h3&gt;
&lt;p&gt;As discussed already, to supply data to your TF graph you designed, you insert &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/placeholder"&gt;tf.placeholder&lt;/a&gt; nodes in the graph (e.g., in the bottom-most layer of your net). Placeholders don’t perform any computation, they just output any data you tell them to, during the graph evaluation (&amp;quot;execution&amp;quot;) phase. Optionally, you can also specify the node's &lt;tt class="docutils literal"&gt;shape&lt;/tt&gt; , if you want to enforce it: And, if you furthermore specify &lt;tt class="docutils literal"&gt;None&lt;/tt&gt; for any tensor dimension, that dimension will adapt to any size (according to the next node's input).&lt;/p&gt;
&lt;p&gt;In the example shown below, the placeholder &lt;tt class="docutils literal"&gt;A&lt;/tt&gt; must be of rank 2 (i.e., two-dimensional), and the tensor must have three columns, but it can have any number of rows (which typically will be the current batch' examples).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="reassignable-variables"&gt;
&lt;h3&gt;Reassignable Variables&lt;/h3&gt;
&lt;p&gt;Note that it is possible to feed values into variables, too, not just to placeholders, even if it is a tad unusual.
To set a variable to some value during graph evaluation, use the &lt;a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/assign"&gt;tf.assign&lt;/a&gt; state operator:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# sample a random variable:&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;minval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# or feed a variable:&lt;/span&gt;
&lt;span class="n"&gt;x_new_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x_assign&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_new_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# now, you can feed new values into X:&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;x_assign&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x_new_val&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="c1"&gt;# always 0.5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizing-the-graph"&gt;
&lt;h3&gt;Optimizing the Graph&lt;/h3&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/gradients"&gt;tf.gradients&lt;/a&gt; function takes a cost operator (e.g., to calculate the MSE) and a list of variables to optimize, and creates a list of ops (one per variable) to compute the gradients of each op with regard to each variable, returning the desired list of gradients (aka. performa &lt;em&gt;reverse-mode autodiff&lt;/em&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var1grad&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var2grad&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;var1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, TensorFlow provides a good number of &lt;a class="reference external" href="https://www.tensorflow.org/api_guides/python/train#Optimizers"&gt;optimizers&lt;/a&gt; right out of the box, for example, the Adam optimizer, saving you the need to explicitly calculate gradients or update variables yourself:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;training_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="adding-regularization"&gt;
&lt;h3&gt;Adding Regularization&lt;/h3&gt;
&lt;p&gt;Regularization (typically, L1 or L2) prevents overfitting and therefore allows you to train your model for more epochs. Simply add the appropriate operations to your graph, to get to a regularized cost (or &lt;tt class="docutils literal"&gt;loss&lt;/tt&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# construct the neural network&lt;/span&gt;
&lt;span class="n"&gt;base_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xentropy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;avg_xentropy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;reg_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# L1&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;reg_losses&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, if you have many layers, this approach quickly becomes inconvenient. Instead,
most TensorFlow functions in the &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers"&gt;tf.contrib.layers&lt;/a&gt; package that create variables accept a &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;..._regularizer&lt;/span&gt;&lt;/tt&gt; argument for their weights and biases.
Those arguments need to be functions that takes the weights as their argument, and return the regularization losses of that layer.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;weights_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l1_regularizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;hidden1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hidden1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;hidden2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hidden2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;out&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;TensorFlow automatically adds these nodes to a special collection containing all the regularization losses.
Then, when calculating the final loss, you add them up to find your overall, final loss:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;reg_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_collection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GraphKeys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;REGULARIZATION_LOSSES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_n&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;base_loss&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;reg_losses&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Max-norm regularization&lt;/strong&gt; (clipping the L2-norm at some threshold) has become quite popular, yet TensorFlow does not provide an off-the-shelf max-norm regularizer. The following code creates a node &lt;tt class="docutils literal"&gt;clip_weights&lt;/tt&gt; that will clip your &lt;tt class="docutils literal"&gt;weights&lt;/tt&gt; along their second axis ( &lt;tt class="docutils literal"&gt;axes=1&lt;/tt&gt; ), so that every resulting row vector will have a max-norm of 1.0:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="n"&gt;clipped_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip_by_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clip_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clip_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clipped_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The issue then becomes accessing the weights of a &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers"&gt;tf.contrib.layers&lt;/a&gt; module; A better solution therefore is to create a function equivalent to the &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/l1_regularizer"&gt;l1_regularizer&lt;/a&gt; found in the layers module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;max_norm_regularizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;max_norm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;max_norm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;max_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;clipped&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip_by_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clip_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;clip_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clipped&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_to_collection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clip_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;  &lt;span class="c1"&gt;# there is no regularization loss term&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;max_norm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, this regularization function can be used as an argument like any other regularizer would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hidden1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hidden1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                          &lt;span class="n"&gt;weights_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;max_norm_regularizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And to actually clip weights using max-norm during session evaluation, finally, add this to your execution phase:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;clip_all_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_collection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;max_norm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# note we used &amp;quot;max_norm&amp;quot; above&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batches&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clip_all_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="scopes-modules-and-shared-variables"&gt;
&lt;h3&gt;Scopes, Modules, and Shared Variables&lt;/h3&gt;
&lt;p&gt;Variables and nodes created within a named scope are prefixed with the scopes' name, and such scopes are collapsed into single nodes on the TensorBoard:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;mse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mse&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;mse&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;E.g., to define a ReLU within a named scope (just in case: normally, you would use &lt;tt class="docutils literal"&gt;tf.nn.relu&lt;/tt&gt; ):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;w_shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_shape&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w_shape&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;weights&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bias&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;z&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you want to share a variable between various components of your graph, (e.g., thresholds, biases, etc.), TensorFlow provides the &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_variable"&gt;tf.get_variable&lt;/a&gt; function, that creates a shared variable if it does not exist, or reuses it, if it does. The desired behavior (creating or reusing) is controlled by an attribute of the current &lt;a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/variable_scope"&gt;tf.variable_scope&lt;/a&gt;, &lt;tt class="docutils literal"&gt;reuse&lt;/tt&gt; . Note that &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_variable"&gt;tf.get_variable&lt;/a&gt; raises an exception if &lt;tt class="docutils literal"&gt;reuse&lt;/tt&gt; is &lt;tt class="docutils literal"&gt;False&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;scope.reuse_variables()&lt;/tt&gt; has not been set.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# as attribute:&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reuse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;threshold&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# as function:&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reuse_variables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;threshold&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="xavier-and-he-initialization-of-model-weights"&gt;
&lt;h3&gt;Xavier and He Initialization of Model Weights&lt;/h3&gt;
&lt;p&gt;You need to use an initializer to avoid the uniform initialization of weights to all the same values.&lt;/p&gt;
&lt;p&gt;By default, TensorFlow's layers are initialized using Xavier initialization: The fully connected layers in &lt;tt class="docutils literal"&gt;tf.contrib.layers&lt;/tt&gt; use Xavier initialization, with a normal dist. using µ=``0``, sigma=``sqrt[ 2 / (n_in + n_out) ]`` or a uniform dist. using &lt;tt class="docutils literal"&gt;+/- sqrt[ 6 / (n_in + n_out) ]&lt;/tt&gt; , where the &lt;tt class="docutils literal"&gt;n&lt;/tt&gt;'s are the sizes of the input/output connections.&lt;/p&gt;
&lt;p&gt;To use He initialization instead (which is mostly a matter of preference, but has been made popular with ResNet), you can use variance scaling initialization:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tf.contrib.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;variance_scaling_initializer&lt;/span&gt;
&lt;span class="n"&gt;initializer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variance_scaling_initializer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;FAN_AVG&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hidden1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights_initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;h1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="implementing-a-learning-rate-scheduler"&gt;
&lt;h3&gt;Implementing a Learning Rate Scheduler&lt;/h3&gt;
&lt;p&gt;Normally, it is not necessary to add a learning rate scheduler, because the AdaGrad, RMSProp, and Adam optimizers automatically reduce the learning rate for you during training. Yet, implementing a learning rate scheduler is fairly straightforward with TensorFlow; Typically, exponential decay is recommended, because it is easy to tune and will converge (slightly) faster than the optimal solution. Here, we adapt Momentum to use a dynamic learning rate: Note how the decay depends on the current global step that is set by the optimizer's minimization function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;initial_learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;span class="n"&gt;decay_steps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="n"&gt;decay_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;global_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exponential_decay&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial_learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                           &lt;span class="n"&gt;decay_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;decay_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MomentumOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;training_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;global_step&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="interactive-sessions"&gt;
&lt;h2&gt;Interactive Sessions&lt;/h2&gt;
&lt;p&gt;Before we get to the actual evaluation of TF graphs with sessions, let me add in a few tips that come in handy when working in interactive Python sessions.&lt;/p&gt;
&lt;div class="section" id="resetting-the-default-graph"&gt;
&lt;h3&gt;Resetting the Default Graph&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_default_graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In Jupyter (or if using TF in a Python shell), it is common to run the same commands more than once while you are experimenting. As a result, you may end up with a &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_default_graph"&gt;default graph&lt;/a&gt; containing many duplicate nodes. One solution is to restart the Jupyter kernel (or the Python shell), but a more convenient solution is to just reset the default graph by running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_default_graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In single-process TensorFlow, multiple sessions do not share any state, even if they reuse the same graph (and each session gets its own copy of every variable). Beware though, that in distributed TensorFlow, variable state is stored on the servers, not in the sessions, so multiple sessions can &lt;em&gt;share&lt;/em&gt; the same &lt;em&gt;variables&lt;/em&gt; (actually, that is a good, desired thing, obviously).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-tensorflow-s-interactive-sessions"&gt;
&lt;h3&gt;Using TensorFlow's [Interactive] Sessions&lt;/h3&gt;
&lt;p&gt;Use &lt;tt class="docutils literal"&gt;InteractiveSession&lt;/tt&gt; in notebooks to automatically set a default session, relieving you from the need of a &lt;tt class="docutils literal"&gt;with&lt;/tt&gt; block for the evaluation/execution phase. But do remember to close the session manually when you are done with it!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InteractiveSession&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# do graph setup&lt;/span&gt;
&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# do evaluation&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When running TensorFlow &lt;strong&gt;locally&lt;/strong&gt;, the sessions manage your variable values. So if you create a graph, then start two threads, and open a local session in either thread, both will use the same graph, yet each session will have its &lt;em&gt;own&lt;/em&gt; copy of the variables.&lt;/p&gt;
&lt;p&gt;However, in &lt;strong&gt;distributed&lt;/strong&gt; TensorFlow sessions, variable values are stored in containers managed by the TF cluster (see &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/get_variable"&gt;tf.get_variable&lt;/a&gt;). So if both sessions connect to the same cluster and use the same container, then they will share the same variable value for w.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="model-evaluation"&gt;
&lt;h2&gt;Model Evaluation&lt;/h2&gt;
&lt;div class="section" id="scaling-variables"&gt;
&lt;h3&gt;Scaling Variables&lt;/h3&gt;
&lt;p&gt;When using a Gradient Descent method, remember that it is important to &lt;strong&gt;normalize&lt;/strong&gt; the input feature vectors, or else training may progress much slower.
You can do this using TensorFlow, NumPy, Scikit-Learn’s StandardScaler, or any other solution you prefer. In fact, with NumPy arrays, this is pretty straightforward:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;scaled&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="running-the-graph"&gt;
&lt;h3&gt;Running the Graph&lt;/h3&gt;
&lt;p&gt;Once the graph has been designed (incl. a &lt;tt class="docutils literal"&gt;training_op&lt;/tt&gt; node) and the initializer (an &lt;tt class="docutils literal"&gt;init&lt;/tt&gt; node) has been set up (see Graph Design), a typical snippet for the (batched) executing of the training phase is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;iteration&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_examples&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="feeding-the-graph-with-data"&gt;
&lt;h3&gt;Feeding the Graph with Data&lt;/h3&gt;
&lt;p&gt;When you evaluate the graph, you pass a feed dictionary (&lt;tt class="docutils literal"&gt;feed_dict&lt;/tt&gt;) to the target (&amp;quot;output&amp;quot;) node's &lt;tt class="docutils literal"&gt;eval()&lt;/tt&gt; method. And you specify the value of the placeholder (input) node by using the node itself as the key of the feed dictionary.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# more graph setup, down to the training_op&lt;/span&gt;
&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;  &lt;span class="c1"&gt;# &amp;lt;- feeding&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that you can feed data into &lt;em&gt;any&lt;/em&gt; kind of node, not just placeholders. Note that when using other nodes, TensorFlow will not evaluate their operations; If fed to, TF uses the values you feed to that node, only (see Reassignable Variables in Graph Design for more info).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="mini-batching-with-tensorflow"&gt;
&lt;h3&gt;Mini-batching with TensorFlow&lt;/h3&gt;
&lt;p&gt;Instead of feeding all data at once, you typically will mini-batch your data as follows:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Create a session (&lt;tt class="docutils literal"&gt;with tf.Session() as sess&lt;/tt&gt;)&lt;/li&gt;
&lt;li&gt;Run the variable initializer (&lt;tt class="docutils literal"&gt;sess.run(init)&lt;/tt&gt;)&lt;/li&gt;
&lt;li&gt;Loop over the epochs and batches, feeding each mini-batch to the session (&lt;tt class="docutils literal"&gt;sess.run(training_op, &lt;span class="pre"&gt;feed_dict={X:&lt;/span&gt; X_batch, y: y_batch})&lt;/tt&gt;)&lt;/li&gt;
&lt;li&gt;Optionally: Write a summary every n mini-batches, to visualize the progress on your TensorBoard.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# somehow fetch data and labels (numpy arrays) to feed...&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="saving-and-restoring-models"&gt;
&lt;h3&gt;Saving and Restoring Models&lt;/h3&gt;
&lt;p&gt;Create a Saver node at the end of the construction phase (after all variable nodes are created); Then, during the execution phase, call the node's &lt;tt class="docutils literal"&gt;save()&lt;/tt&gt; method whenever you want to save the model, passing it the session and path of the &lt;strong&gt;checkpoint&lt;/strong&gt; file to create:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;saver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Saver&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;checkpoint_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/tmp/my_classifier.tfckpt&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;checkpoint_epoch_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;.tfepoch&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;final_model_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;./my_classifier.tfmodel&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;best_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;infty&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isfile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoint_epoch_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# if the checkpoint file exists, restore the model and load the epoch number&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoint_epoch_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;start_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Training was interrupted. Continuing at epoch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start_epoch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;start_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start_epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# checkpoint every 100 epochs&lt;/span&gt;
            &lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoint_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoint_epoch_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;wb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="n"&gt;loss_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;loss_val&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;best_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;final_model_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;best_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss_val&lt;/span&gt;
            &lt;span class="c1"&gt;# best_parameters = parameters.eval()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To use a trained model &lt;em&gt;in production&lt;/em&gt;, restoring a model is just as easy: You create a Saver node at the end of the graph, just like before, but then, when beginning the execution phase, instead of initializing the variables using the typical &lt;tt class="docutils literal"&gt;init&lt;/tt&gt; node, you call the &lt;tt class="docutils literal"&gt;restore()&lt;/tt&gt; method of the Saver object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;./my_model_final.ckpt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;X_unseen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# some unseen (scaled) data&lt;/span&gt;
    &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_unseen&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="monitoring-with-tensorboard"&gt;
&lt;h2&gt;Monitoring with TensorBoard&lt;/h2&gt;
&lt;p&gt;One of the biggest advantages of TensorFlow over many other frameworks is the TensorBoard. It allows you to visualize the progression of any variable in your graph.&lt;/p&gt;
&lt;div class="section" id="writing-session-summaries"&gt;
&lt;h3&gt;Writing Session Summaries&lt;/h3&gt;
&lt;p&gt;To provide TensorBoard with data, you need to write TF's graph definition and some training stats (like the cost/loss) to a log directory that TensorBoard reads from. You need to use a different log directory on every run, to avoid that TensorBoard will merge the output of different runs. The solution to this here will be to include a timestamp in the log directory name.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;

&lt;span class="n"&gt;root_logdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tf_logs&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utcnow&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;%Y%m&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;%H%M%S&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;logdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;{}/run-{}/&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root_logdir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, add a &lt;strong&gt;summary node&lt;/strong&gt; and attach a &lt;strong&gt;file writer&lt;/strong&gt; to the node you wish to visualize on your TensorBoard; The &lt;tt class="docutils literal"&gt;FileWriter&lt;/tt&gt; shown below will create any missing directories for you:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;mse_summary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MSE&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;file_writer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FileWriter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logdir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_default_graph&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The first line creates a node in the graph that will evaluate the MSE value and write it to a TensorBoard-compatible binary log string called a &lt;strong&gt;summary&lt;/strong&gt;. The second line creates a &lt;tt class="docutils literal"&gt;FileWriter&lt;/tt&gt; that you will use to write summaries into the log directory. The second (optional) parameter is the graph you want to visualize. Upon creation, the FileWriter creates the directory path if it does not exist, and writes the graph definition in a binary log file called an &lt;strong&gt;events&lt;/strong&gt; file.&lt;/p&gt;
&lt;p&gt;Next, you need to update the execution phase, to evaluate the summary node regularly during training, and you should not forget to close the writer after training:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;update_summary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;update_summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;summary_str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mse_summary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;batch_index&lt;/span&gt;
            &lt;span class="n"&gt;file_writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;summary_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_batch&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;file_writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, you now can visualize the stats you are recording by starting the TensorBoard server and pointing it at the log directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tensorboard --logdir tf_logs/
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="epilogue"&gt;
&lt;h2&gt;Epilogue&lt;/h2&gt;
&lt;p&gt;As I already advised in the beginning, if you want to learn more, I can warmly recommend you get Aurélien Géron's fantastic book &amp;quot;&lt;a class="reference external" href="http://shop.oreilly.com/product/0636920052289.do"&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow&lt;/a&gt;&amp;quot;; The more advanced topics covered (and that would explode this blog post...) are transfer learning, distributed training, designing Recurrent Networks and Auto-encoders, and even a &amp;quot;beginner's guide&amp;quot; to Deep Reinforcement Learning. Yet, I hope, this tiny taste of the book's contents, spiced up with a bit of my own &amp;quot;opinionated&amp;quot; modifications, will provide you with  a handy quick-reference when building and using TensorFlow graphs. (And, that I don't get sued by him or O'Reilly for plagiarism! :-) Please just contact me if this post is an issue -- I have no problem taking the post down again, if it is problematic.)&lt;/p&gt;
&lt;/div&gt;
</content><category term="tensorflow"></category><category term="machine learning"></category></entry><entry><title>What are the best books [for programmers] to get into Data Science?</title><link href="http://fnl.es/what-are-the-best-books-for-programmers-to-get-into-data-science.html" rel="alternate"></link><published>2015-08-07T00:00:00+02:00</published><updated>2015-08-07T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2015-08-07:/what-are-the-best-books-for-programmers-to-get-into-data-science.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is a question I get on a frequent basis by colleagues that are serious programmers or software developers and are planning to pick up data analytics. There are three fundamental topics in mathematics that you need to cover, assuming that you are an expert in software development and …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is a question I get on a frequent basis by colleagues that are serious programmers or software developers and are planning to pick up data analytics. There are three fundamental topics in mathematics that you need to cover, assuming that you are an expert in software development and informatics already: statistics, linear algebra, and machine learning. Therefore, I will recommend one book associated to each of these topics. Even I continue to consult those books as reference material.&lt;/p&gt;
&lt;p&gt;There might be a fourth foundation worth mentioning: calculus. I don't really have a recommendation for that, as it never has been essential to my work. As long as you can remember how to do differentiation and integration and solve an ODE, you are pretty much saddled. In other words, fresh high-school knowledge of calculus has always been good enough for me. And there is no need to make any proofs, unless you want to develop your own machine learning methods…&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-r-environment"&gt;
&lt;h2&gt;The R Environment&lt;/h2&gt;
&lt;p&gt;Before presenting the books, let me recommend one tool that you should pick up when doing the books' practical exercises: &lt;a class="reference external" href="https://www.r-project.org/"&gt;R&lt;/a&gt;. R is the de facto tool for data science today and, as I would predict, the &lt;a class="reference external" href="https://www.rstudio.com/"&gt;environment&lt;/a&gt; that will (is?) put (putting?) S, SPSS, SAS, and a number of other commercial tools into their digital grave. Even banks and insurances are beginning to embrace R, apparently (not sure that's a good thing, though...). R provides a huge ecosystem of packages that lets you quickly explore and test models, inspect your data, develop theories, and visualize results. Any of these points can be extremely tedious with other tools, but a few free and commercial statistics, math, and/or machine learning environments (e.g., MatLab or Octave) might be viable alternatives if you already know them. Python in particular is extremely well suited (maybe even better than R) if you are interested in language processing or neural networks. Overall, I recommend working through the exercises provided by the books with your preferred interactive environment. Just as with programming and software development, only reading about the relevant material will not provide you with the experience needed to actually “do” data science.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="statistics"&gt;
&lt;h2&gt;Statistics&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.statisticalsleuth.com/"&gt;The Statistical Sleuth&lt;/a&gt; by Fred Ramsey and Daniel W. Schafer.
While there might be “simpler” books around, this one has the most compact and best overview of applied statistics that I have read to date (t and F distributions, ANOVA and multivariate regression, etc.). If you can only read one book or only need to work the statistics angle, then this is “the” book. Most of the machine learning stuff can be understood easily after digesting this book, and I particularly like the very applied approach that Ramsey &amp;amp; Schafer take.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="linear-algebra"&gt;
&lt;h2&gt;Linear Algebra&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://math.mit.edu/~gs/linearalgebra/"&gt;Introduction to Linear Algebra&lt;/a&gt; by Gibert Strang.
Contrary to my statistics recommendation, on this topic there certainly are more in-depth introductory books around, but this one covers all I ever needed. It takes you from the dot and outer product over eigenspaces to Singular Value Decomposition. And, the author is exceptionally gifted at teaching algebraic thinking (The book can be &amp;quot;watched&amp;quot; for free in form of &lt;a class="reference external" href="http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/"&gt;videos&lt;/a&gt; of his classes.) As a bonus, the final chapters explain how this material ties in with advanced data science techniques and machine learning models.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="machine-learning"&gt;
&lt;h2&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www-bcf.usc.edu/~gareth/ISL/"&gt;An Introduction to Statistical Learning&lt;/a&gt; by James, Witten, Hastie, and Tibshirani.
While quite a bit slower moving than the first Hastie &amp;amp; Tibshirani book (The Elements of Statistical Learning), I think it is the better start, and it has a ton of exercises using R. Especially the first few chapters contain discussions about basic aspects that are critical to this line of work, like the bias-variance tradeoff issue to keep in mind when developing your machine learning models. This is an excellent read if the Sleuth above is no longer a problem, as it does assume a solid background on basic statistics, distributions &amp;amp; regression techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;After studying these three books, you will be ready to extend your knowledge with domain-specific techniques. That should be geared at what kind of data you want to process: Text mining, time series, bioinformatics, signal processing, etc. and what specific models you want to apply: Neural networks, graphical models, kernel machines, structured equations, etc. In other words, the above three books will not convert you into a data science expert, but they will provide you with an extremely solid foundation so that you can effortlessly study any advanced topics and techniques that will.&lt;/p&gt;
&lt;/div&gt;
</content><category term="data science"></category><category term="review"></category></entry><entry><title>An efficient online sequence tagger resource for GATE</title><link href="http://fnl.es/an-efficient-online-sequence-tagger-resource-for-gate.html" rel="alternate"></link><published>2015-04-06T00:00:00+02:00</published><updated>2015-04-06T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2015-04-06:/an-efficient-online-sequence-tagger-resource-for-gate.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; for a stressed out generation:
&lt;a class="reference external" href="https://gate.ac.uk"&gt;GATE&lt;/a&gt;'s &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch23.html#sec:parsers:taggerframework"&gt;Generic Tagger&lt;/a&gt; framework is a CREOLE plug-in that allows you to wrap any existing &lt;a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;sequence tagger&lt;/a&gt; and use it to create annotations in your pipeline, but it is a bit slow.
Therefore, I have created the &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework"&gt;Online Tagger&lt;/a&gt; GATE plug-in that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; for a stressed out generation:
&lt;a class="reference external" href="https://gate.ac.uk"&gt;GATE&lt;/a&gt;'s &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch23.html#sec:parsers:taggerframework"&gt;Generic Tagger&lt;/a&gt; framework is a CREOLE plug-in that allows you to wrap any existing &lt;a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;sequence tagger&lt;/a&gt; and use it to create annotations in your pipeline, but it is a bit slow.
Therefore, I have created the &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework"&gt;Online Tagger&lt;/a&gt; GATE plug-in that works similarly to the Generic Tagger framework, but does not do any disk I/O for inter-process communication or launch more than one &amp;quot;singleton&amp;quot; sub-process per Processing Resource instance.
This version can in some cases be several &lt;em&gt;orders&lt;/em&gt; of magnitude faster than the built-in framework.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://gate.ac.uk"&gt;GATE&lt;/a&gt; (General Architecture for Text Engineering) has crystallized itself as my preferred tool for teaching text mining and information extraction.
While anybody might argue that there are leaner and faster frameworks around, it has one pretty outstanding, unique quality: it is (mostly) GUI-based.&lt;/p&gt;
&lt;p&gt;During any text mining course I teach, the most frequent question I get is what text mining software is around that can be used &lt;em&gt;without any a priori programming skills&lt;/em&gt;.
In other words, most of my audience is looking for a &amp;quot;graphical&amp;quot; text mining environment that can be used without first having to learn how to program.
For example, to use NLTK, LingPipe, OpenNLP, StandfordNLP, UIMA, etc., you will first have to learn how to program in the chosen framework's API language.
Therefore, the only entirely true answer is that only commercial tools can offer a &lt;strong&gt;pure&lt;/strong&gt; &amp;quot;graphical user interface&amp;quot; and require no programming experience.&lt;/p&gt;
&lt;p&gt;However, GATE can be used &lt;em&gt;mostly&lt;/em&gt; without having to write code - with the exception of its &amp;quot;JAPE-glue&amp;quot;.
&lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch8.html#chap:jape"&gt;JAPE&lt;/a&gt; stands for &amp;quot;Java Annotation Patterns Engine&amp;quot; and is GATE's solution to make data inter-operable between different text mining resources that commonly have different I/O requirements.
Furthermore, JAPE can be used to design entire rule-based annotation resources of their own right.
However, JAPE &amp;quot;grammars&amp;quot; consist of rules where the left-hand side of the grammatical rule matches (existing) GATE annotations using a (clear and simple) syntax, while the right-hand side of those rules can contain Java code that will somehow modify those annotations.
Therefore, GATE rids you from the need of writing code of your own, except for (small) blocks of (simple) code for the right-hand sides of JAPE's rules.
Luckily, GATE's extensive documentation provides lots of &lt;a class="reference external" href="https://gate.ac.uk/wiki/jape-repository/"&gt;examples&lt;/a&gt; to start with for the novice.&lt;/p&gt;
&lt;p&gt;Overall, this makes GATE the only free open source text mining software that provides a graphical interface &lt;em&gt;and&lt;/em&gt; requires (nearly) no programming skills to use it.
As I stated initially, this fact alone makes it the best fit for my typical tutorial audiences, because most of them are neither computer scientists nor do they (want to) know how to code.&lt;/p&gt;
&lt;p&gt;As mentioned in the introduction, &amp;quot;out-of-the-box&amp;quot; GATE isn't always the fastest solution.
However, due to its open source nature that only means that if you need to go faster, you always can replace any slow pieces with whatever you consider a better fit (if you know how to program, that is...)
For example, the &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch23.html#sec:parsers:taggerframework"&gt;Generic Tagger&lt;/a&gt; framework is a &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch4.html#x7-690004"&gt;CREOLE&lt;/a&gt; &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch3.html#x6-540003.7"&gt;Processing Resource&lt;/a&gt; that allows you to take any existing &lt;a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;sequence tagger&lt;/a&gt; and use it to create annotations for a GATE pipeline.
This is pretty nifty, because you can use whatever Part-of-Speech tagger or Named Entity Recognition system you like.
You can even use a generic sequence tagger, train your own model, and integrate it in your text mining and information extraction pipelines, all without having to learn how to program first.&lt;/p&gt;
&lt;p&gt;However, precisely due to the highly generic nature of the Generic Tagger framework, it is not very efficient.
To create GATE annotations with it, this tagger &amp;quot;wrapper&amp;quot; operates as follows &lt;strong&gt;on each input&lt;/strong&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A file with the input text for the tagger is written to disk.&lt;/li&gt;
&lt;li&gt;A new tagger sub-process is launched by the wrapper, reading the input from the file.&lt;/li&gt;
&lt;li&gt;The tagger's results are written back to disk.&lt;/li&gt;
&lt;li&gt;The wrapper resource reads the result file, generates the annotations, and deletes the two temporary files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I have highlighted &amp;quot;on each input&amp;quot;, because this loop might be run for each processed document, for each sentence in each document, or, even worse, for each and every token in your documents.
If you have already thought that doing this for each document is pretty bad, doing that loop for each token grinds your pipeline to a standstill.
Second, if you are a programmer, the expression &amp;quot;a new sub-process is launched&amp;quot; in the second point should be alarming to you.
If the tagger uses some large resources, like a dictionary (which they quite frequently do), starting up a new tagger process can be extremely expensive.
In general, of all concurrent programming concepts, launching a new &lt;em&gt;process&lt;/em&gt; is the probably most expensive resource you can create &amp;quot;within&amp;quot; a program and should be done as sparingly as possible.
The reason the plug-in is designed in this peculiar way isn't because the framework was written by inexperienced programmers, however.
It is that way because due to this design, it truly generic: Most I/O formats and tagger can be handled with this wrapper.&lt;/p&gt;
&lt;p&gt;However, while the Generic Tagger is pretty cool to have on board so you can to try out &amp;quot;foreign&amp;quot; sequence taggers, the way it is implemented makes it rather useless for a &amp;quot;real&amp;quot; pipeline, i.e., beyond experimentation.
For example, just tagging all gene mentions in a few thousand &lt;a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed"&gt;PubMed&lt;/a&gt; sentences with this wrapper takes &lt;em&gt;days&lt;/em&gt;.
But PubMed has over 24 million abstracts and (I think to recall) roughly around 100 million sentences, so go figure...&lt;/p&gt;
&lt;p&gt;Therefore, I am releasing my own CREOLE processing resource that works similarly to the Generic Tagger, but does not do any disk I/O for inter-process communication or launch more than one &amp;quot;singleton&amp;quot; process for the entire pipeline you are designing.
However, this puts some restrictions on the kinds of taggers you can use:&lt;/p&gt;
&lt;p&gt;1. The tagger must support a &lt;strong&gt;streaming I/O&lt;/strong&gt; model.
That is, the tagger must be able to read from some &amp;quot;input stream&amp;quot;, such as UNIX' &lt;cite&gt;STDIN&lt;/cite&gt;, and write to some &amp;quot;output stream&amp;quot;, commonly UNIX' &lt;cite&gt;STDOUT&lt;/cite&gt;.
Another way of putting this is that your tagger should be able to handle UNIX' piped command syntax, something like this: &lt;tt class="docutils literal"&gt;cat plain_text.txt | some_tagger &amp;gt; tagged_text.txt&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;2. The tagger must work with POSIX' classical line-based interface.
That is, the tagger must take one continuous block of text as &lt;em&gt;input&lt;/em&gt;, &lt;strong&gt;terminated with a newline&lt;/strong&gt; character.
For example, it should take one token, sentences or block of text as input (not containing any newlines), and, once it receives a newline character, start tagging that input.&lt;/p&gt;
&lt;p&gt;3. The tagger must produce &lt;strong&gt;one annotation per line&lt;/strong&gt; as &lt;em&gt;output&lt;/em&gt;, and those annotations must be &lt;strong&gt;in the same order&lt;/strong&gt; as the (input) text spans which they annotate.
Those annotations commonly are expected to be in the OTPL (one token per line) format.
For example, the output line &lt;tt class="docutils literal"&gt;Nouns noun NN &lt;span class="pre"&gt;B-NP&lt;/span&gt; O&lt;/tt&gt; might annotate the token &amp;quot;Noun&amp;quot; (verbatim, as found in the input text) with the lemma &amp;quot;noun&amp;quot;, the PoS-tag &amp;quot;NN&amp;quot;, the BIO-chunk &amp;quot;B-NP&amp;quot; and the BIO-NER-tag &amp;quot;O&amp;quot; (&amp;quot;outside&amp;quot; any entity mention).&lt;/p&gt;
&lt;p&gt;If you have a tagger that follows those requirements (it turns out, most sequence taggers I know of work precisely like this), you can instead use my &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework"&gt;Online Tagger&lt;/a&gt; framework.
What it does differently to the Generic Tagger is the following:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;On Processing Resource &lt;strong&gt;initialization&lt;/strong&gt;, you have to specify the location of the tagger and GATE launches the tagger with the supplies parameters (directory where to run the tagger and any arguments, such as dictionaries to load or command-line flags to set).&lt;/li&gt;
&lt;li&gt;The Processing Resource &lt;strong&gt;configuration&lt;/strong&gt; is nearly the same, but some of the defaults have been adapted to better reflect the nature of the on-line processing model.&lt;/li&gt;
&lt;li&gt;Once your pipeline is &lt;strong&gt;running&lt;/strong&gt;, the text is piped into the tagger sub-process and results are read from the output stream, while intermediary files are no longer created.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please clone the tagger from GitHub (&lt;tt class="docutils literal"&gt;git clone &lt;span class="pre"&gt;https://github.com/fnl/OnlineTaggerFramework&lt;/span&gt;&lt;/tt&gt;) into your local CREOLE &lt;a class="reference external" href="https://gate.ac.uk/sale/tao/splitch3.html#x6-530003.6"&gt;user plugin directory&lt;/a&gt;.
Then you can load my plug-in from the CREOLE Plugin Manager and once you instantiate a new &lt;tt class="docutils literal"&gt;GenericOnlineTagger&lt;/tt&gt; Processing Resource, you will be asked to supply the initial configuration data to launch the tagger in its own sub-process (tagger binary path, directory to run in [if any], runtime flags and arguments [if any]).&lt;/p&gt;
&lt;p&gt;If you run into any issue using this plug-in, please consider filing a &lt;a class="reference external" href="https://github.com/fnl/OnlineTaggerFramework/issues"&gt;bug report&lt;/a&gt; on GitHub so I can fix the problem for everybody using it.
I hope the this plug-in will make you enjoy the new-found efficiency when integrating sequence taggers into your GATE pipelines!&lt;/p&gt;
</content><category term="text mining"></category><category term="nlp"></category><category term="Java"></category></entry><entry><title>segtok - a segmentation and tokenization library</title><link href="http://fnl.es/segtok-a-segmentation-and-tokenization-library.html" rel="alternate"></link><published>2015-01-12T00:00:00+01:00</published><updated>2015-01-12T00:00:00+01:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2015-01-12:/segtok-a-segmentation-and-tokenization-library.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Surprisingly, it is hard to find a good command-line tool for sentence segmentation and word tokenization that works well with European languages.
Here, I present &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;, a &lt;strong&gt;Python 2.7 and 3&lt;/strong&gt; package, API, and Unix command-line tool to remedy this shortcoming.&lt;/p&gt;
&lt;div class="section" id="text-processing-pipelines"&gt;
&lt;h2&gt;Text processing pipelines&lt;/h2&gt;
&lt;p&gt;This is the …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Surprisingly, it is hard to find a good command-line tool for sentence segmentation and word tokenization that works well with European languages.
Here, I present &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;, a &lt;strong&gt;Python 2.7 and 3&lt;/strong&gt; package, API, and Unix command-line tool to remedy this shortcoming.&lt;/p&gt;
&lt;div class="section" id="text-processing-pipelines"&gt;
&lt;h2&gt;Text processing pipelines&lt;/h2&gt;
&lt;p&gt;This is the second in a series of posts that will present lean and elegant text processing tools to take you across the void from your document collection to the linguistic processing tools (&lt;em&gt;and back&lt;/em&gt;, but more on that in the future).
In the &lt;a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;last post&lt;/a&gt;, I discussed sequence taggers, as they form the entry point to natural language processing (NLP).
Today I am presenting existing tools and a little library of mine for pre-processing of Germanic and Romance language text (essentially, because I am not knowledgeable of any others...).
Text processing pipelines roughly consist of the following three steps:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Document extraction&lt;/li&gt;
&lt;li&gt;Text pre-processing&lt;/li&gt;
&lt;li&gt;Language processing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are many good tools around for document extraction, in particular Apache &lt;a class="reference external" href="http://tika.apache.org/"&gt;Tika&lt;/a&gt; is a great software library I highly recommend for this task.
(As a matter of fact, if you want to improve your programming skills and see some real-life, clean implementations of nearly all Java/GoF software patterns, have a look at the innards of Tika.)
If you want to extract data from PDFs in particular, you should be looking for your preferred tool in the huge collection of software that is based on the excellent &lt;a class="reference external" href="http://www.foolabs.com/xpdf/"&gt;xpdf&lt;/a&gt; library or even OCR libraries like &lt;a class="reference external" href="https://code.google.com/p/tesseract-ocr/"&gt;Tesseract&lt;/a&gt;.
As for language processing, there are several tools for chunking and tagging with dynamic graphical models that you can choose from, as outlined in an &lt;a class="reference external" href="http://fnl.es/a-review-of-sparse-sequence-taggers.html"&gt;earlier post&lt;/a&gt; of mine, and for uncovering more involved semantic relationships, dependency parsers like &lt;a class="reference external" href="https://github.com/syllog1sm/redshift"&gt;RedShift&lt;/a&gt; are available.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="overview-and-motivation"&gt;
&lt;h2&gt;Overview and motivation&lt;/h2&gt;
&lt;p&gt;However, for the intermediate pre-processing, short from cooking up your own solution, the currently best solution is to use one of the large natural language processing (NLP) frameworks.
There are a few sentence segmentation and tokenization libraries around, particularly in Perl, but they do not have the desired properties to handle more complex cases or are long &lt;a class="reference external" href="http://mailman.uib.no/public/corpora/2007-October/005429.html"&gt;forgotten&lt;/a&gt;.
The only more recent, statistics-based tool I could find is &lt;a class="reference external" href="https://code.google.com/p/splitta/"&gt;splitta&lt;/a&gt;, but development seems to have died off yet again.
If you check out its Issues page, nobody seems to be fixing the problems, it does not work with Python 3, and its command-line implementation is not really ready for text processing with Unix.
Another example is &lt;a class="reference external" href="https://www.tm-town.com/natural-language-processing"&gt;Pragmatic Segmenter&lt;/a&gt;, a rule-based segmenter written in Ruby.
But if you feed it examples with abbreviations or other occurrences of problematic issues I will discuss in this post, you will see that it performs worse than even the statistical approaches provided by the frameworks discussed next.&lt;/p&gt;
&lt;p&gt;This leaves you with &lt;a class="reference external" href="http://www.nltk.org"&gt;NLTK&lt;/a&gt;'s &lt;a class="reference external" href="http://www.nltk.org/_modules/nltk/tokenize/punkt.html"&gt;PunktTokenizer&lt;/a&gt;, &lt;a class="reference external" href="http://alias-i.com/lingpipe/"&gt;LingPipe&lt;/a&gt;'s &lt;a class="reference external" href="http://alias-i.com/lingpipe/demos/tutorial/sentences/read-me.html"&gt;SentenceModel&lt;/a&gt;, &lt;a class="reference external" href="http://opennlp.apache.org/"&gt;OpenNLP&lt;/a&gt;'s &lt;a class="reference external" href="http://opennlp.sourceforge.net/api/opennlp/tools/sentdetect/SentenceDetectorME.html"&gt;SentenceDetectorME&lt;/a&gt;, and quite a few more frameworks that have APIs to bridge that gap.
(Again, if you enjoy looking at Java [Kingdom of Nouns...] source code, check out LingPipe - while commercial, it is really well designed.)
However, such a heavy-handed approach is to break a butterfly with a wheel, and in my personal opinion/experience, none of them are doing a particularly good job at segmenting, either.
If you don't believe me (never do!), just compare their performance/results against the rule-based library I am about to present here - you will see that these statistical segmenters produce a significant number of false positives on  orthographically (mostly) correct texts.
Their strength compared to this library here, however, is language independence: the library discussed here (so far?) only works with Indo-European languages, while the mentioned segmenters from the above frameworks can be trained on nearly any language and domain.
Particularly, the unsupervised PunktTokenizer only needs sufficient text and the presence of a terminal marker to learn the segmentation rules on its own.
Similarly, if you want to parse noisy text with bad spelling (Twitter and other &amp;quot;social&amp;quot; media sources), you might be best advised to use those frameworks.
So while I do think these libraries are all great as a whole - and I would recommend any one of them - it is mildly annoying that you have to learn a framework if all you want to do is common text pre-processing.&lt;/p&gt;
&lt;p&gt;If you are analyzing corporate documents, patents, news articles, scientific texts, technical manuals, web pages, etc., that tend to have good orthography these statistical tools are not quite up to it and introduce far too many splits, at least for my taste.
This then affects your downstream language processing, because the errors made by the pre-processing will be propagated and, in the worst case, even amplified.
Therefore, text processing pipelines commonly end up doing both (2) and (3) using one such framework, but years of experience have shown me that you soon will be wanting to explore methods beyond whatever particular framework you chose offers.
That then can mean that you have to re-conceptualize all of (2) to add a different tool or even need to move to a whole new framework.
If the performance of the newly integrated framework then isn't that stellar either, it becomes disputable if it even was worth the effort.
Last but not least, this framework-based software development approach violates one of the most fundamental Unix philosophies:&lt;/p&gt;
&lt;blockquote&gt;
“A  program should do one thing and it should do it well.
Programs should handle &lt;em&gt;text streams&lt;/em&gt;, because that is a universal interface.”
(Doug McIlroy)&lt;/blockquote&gt;
&lt;p&gt;(Yes, I know, it seems this library is violating the “one thing” rule because it does two things: segmenting and tokenization.
But as you will see, the library comes with two independent scripts and APIs for each step.)&lt;/p&gt;
&lt;p&gt;The next two sections are for newcomers to this topic and explains why segmenting and tokenizing isn't that trivial as it might appear.
If you are an expert, you can skip the next two sections and read on where &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt; is introduced.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-is-sentence-segmentation-and-tokenization"&gt;
&lt;h2&gt;What is sentence segmentation and tokenization?&lt;/h2&gt;
&lt;p&gt;Nearly any text mining and/or linguistic analysis starts with a sentence and word segmentation step. That is, determining all the individual spans in a piece of text that constitute its sentences or words.
Identifying sentences is important because they form logical units of thought and represent the borders of many grammatical effects.
All our communication - statements, questions, and commands - are expressed by sentences or at least a meaningful sentence-like fragment, i.e., a phrase.
Words on the other hand are the atomic units that form the sentences, and ultimately, our language.
While words are built up from a sequence of symbols in most languages, these symbols have no semantics of their own (except for any single-symbol words in that language, naturally).
Therefore, nearly any meaningful text processing task will require the segmentation of the sequence of symbols (characters in computer lingo) into sentences and words.
These words are, at least in the context of computational text processing, often called &lt;strong&gt;tokens&lt;/strong&gt;.
Beyond the actual words consisting of letters, a token includes atomic units consisting of other symbols.
For example, the sentence terminals (., !, and ? are three such tokens), a currency symbol, or even chains of symbols (for example, the ellipsis: …).
By following through with this terminology, the process of segmenting text into these atomic units is commonly called &lt;em&gt;tokenization&lt;/em&gt; and a computer subroutine doing this segmentation is known as a &lt;em&gt;tokenizer&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sentence-segmentation-and-tokenization-is-hard"&gt;
&lt;h2&gt;Sentence segmentation and tokenization is hard&lt;/h2&gt;
&lt;p&gt;While this segmentation step might initially sound like a rather trivial problem, it turns out the rabbit hole is deep and no perfect solution has been found to date.
Furthermore, the problem is made harder by the different symbols and their usage in distinct languages.
For example, just finding word boundaries in Chinese is non-trivial, because there is not boundary marker (unlike the whitespace used by Indo-European languages).
And when looking into technical documents, the problem can grow even more out of hand.
Names of chemical compounds, web addresses, mathematical expressions, etc. are all complicating the way one would normally define a set of word boundary detection rules.
Another source of problems are texts from public internet fora, such as Twitter.
Words are not spelled as expected, the spaces might be missing, and emoticons and other ASCII-art can make defining the correct tokenization strategy a rather difficult endeavor.
Similarly, the sentence terminal marker in many Indo-European languages is the full-stop dot – which coincidentally is also used as the abbreviation marker in those languages.
For example, detecting the (right!) three sentences in in the following text fragment is not that trivial, at least for a computer program.&lt;/p&gt;
&lt;blockquote&gt;
Hello, Mr. Man. He smiled!! This, i.e. that, is it.&lt;/blockquote&gt;
&lt;p&gt;If the text fragments are large or the span contains many dots, even humans will start to make many errors when trying to identify all the sentence boundaries.
Certain proper nouns (gene names, or “amnesty international”, for example) might demand that the sentence begins with a lower case letter instead of the expected upper-case.
A simple typo might have been the cause for a sentence starting with a lower-case letter, too.
Again in public internet fora, users sometimes resort to using only lower- or upper-case for their  messages or write in an orthographically invalid mix of letter casing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="introducing-segtok-a-python-library-for-these-two-issues"&gt;
&lt;h2&gt;Introducing segtok – a Python library for these two issues&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip3 install segtok
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The Unix approach to software (one thing only, with a text-based interface [that can be used with pipes]) allows you to integrate programs at each stage of your tool-chain and makes it simple to quickly exchange any parts.
If you use a large framework, on the other hand, you are constrained by it and might feel tempted to accept that some of the things you will be doing with it aren't done quite as efficiently as possible.
And the more you make use of that large framework, the less attractive it is to switch your tooling and move out of that &amp;quot;comfort zone&amp;quot;.
I think this issue has a direct, detrimental effect on our ability to experiment and adapt to new tools, software, and methods.&lt;/p&gt;
&lt;p&gt;Due to the many different ways this problem can be solved and the inherent complexity if considering all languages, &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;, the library presented here, is confined to processing orthographically regular Germanic (e.g., English) and Romance (e.g., Spanish) texts.
It has a strong focus on those two and German, which all use Latin letters and standard symbols (like .?!”'([{, etc.).
This is mostly based on the fact that I only know those three languages to some reasonable degree (my tourist-Italian does not count...) - while help/contributions to make segtok work in more languages would be very welcome!
Furthermore, &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt; was made to cope with text having the following properties:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Capable of (correctly!) handling the whole Unicode space.&lt;/li&gt;
&lt;li&gt;A sentence termination marker must be present.&lt;/li&gt;
&lt;li&gt;Texts that follow a mostly regular writing style - in particular, segtok is not tuned for Twitter's highly particular orthography.&lt;/li&gt;
&lt;li&gt;It can handle technical texts (containing, e.g., chemical compounds) and internet URIs (IP addresses, URLs and e-mail addresses).&lt;/li&gt;
&lt;li&gt;The tool is able to handle (valid) cases of sentences starting with a lower-case letter and correctly splits sentences enclosed by parenthesis and/or quotation marks.&lt;/li&gt;
&lt;li&gt;It is able to handle some of the more common cases of heavy abbreviation use (e.g., academic citations).&lt;/li&gt;
&lt;li&gt;It treats all &lt;em&gt;Unicode&lt;/em&gt; dashes (there are quite a few of them in Unicode land) “The Right Way” - a functionality surprisingly absent from most tools.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, the two scripts that come with segtok have a very simple plain-text, line-based interfaces that work well when joined with Unix pipe streams.
The first script, &lt;tt class="docutils literal"&gt;segmenter&lt;/tt&gt;, segments sentences in (plain) text files into one sentence per line.
The other, &lt;tt class="docutils literal"&gt;tokenizer&lt;/tt&gt;, splits tokens on single lines (usually, the sentences from the &lt;tt class="docutils literal"&gt;segmenter&lt;/tt&gt;) by adding whitespaces where necessary.
On the other hand, if you are a Python developer, you can use the functions (&amp;quot;Look Ma, no nouns!&amp;quot;...) provided by this library to incorporate this approach in your own software (the tool is MIT licensed, btw.).
Segtok is designed to handle texts with characters from the entire Unicode space, not just ASCII or Latin-1 (ISO-8859-1).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sentence-segmentation-with-segtok"&gt;
&lt;h2&gt;Sentence segmentation with segtok&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;segtok.segmenter&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;split_single&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_multi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On the sentence level, segtok can detect sentences that are contained inside brackets or quotation marks and maintains those brackets as part of the sentence;
For example:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;(A sentence in parenthesis!)&lt;/li&gt;
&lt;li&gt;Or a sentence with &amp;quot;a quote!&amp;quot;&lt;/li&gt;
&lt;li&gt;'How about handling single quotes?'&lt;/li&gt;
&lt;li&gt;[Square brackets are fine, too.]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The segmenter is constrained to only segment on single lines ( &lt;tt class="docutils literal"&gt;split_single&lt;/tt&gt; - sentences are not allowed to cross line boundaries) or to consecutive lines ( &lt;tt class="docutils literal"&gt;split_multi&lt;/tt&gt; - splitting is allowed across newlines inside “paragraphs” separated by two or more newlines).
(If you really want to extract sentences that cross consecutive newline characters, please remove those line-breaks from your text first.
Segtok assumes your content has some minimal semantical meaning, while superfluous newlines are nothing more than noise.)
It gracefully handles enumerations, dots, multiple terminals, ellipsis, and similar issues:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A. The first assumption.&lt;/li&gt;
&lt;li&gt;2. And that is two.&lt;/li&gt;
&lt;li&gt;(iii) Third things here.&lt;/li&gt;
&lt;li&gt;What the heck??!?!&lt;/li&gt;
&lt;li&gt;A terminal ellipsis...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In essence, a valid sentence terminal must be represented by one of the allowed Unicode markers.
That is, the many Unicode variants of ., ?, and !, and the ideographic full-stop: “。” (a single character).
Therefore, &lt;em&gt;this library cannot guess a sentence boundary if the marker is absent&lt;/em&gt;!
After the marker, up to one quotation mark and one bracket may be present.
Finally, the marker must be separated from the following non-space symbol by at least one whitespace character or a newline.&lt;/p&gt;
&lt;p&gt;This requires that the sentence boundaries do obey some limited amount of regularity.
But at the same time, the pesky requirement that a marker is followed by upper-case letters is absent from this strategy.
In addition, this means that “inner” abbreviation markers are never a candidate (such as in “U.S.A.”).
On the other hand, any markers that do not follow this “minimal” pattern will always result in false negatives (i.e., not be split).
While missing markers and markers not followed by a space character do occur, those cases are very infrequent in orthographically correct texts.&lt;/p&gt;
&lt;p&gt;After these &lt;em&gt;potential&lt;/em&gt; markers have been established, the method goes back and looks at the surrounding text to determine if that marker is not at a sentence boundary after all.
This step recuperates cases like initials (“A. Name”), species names (“S. pombe”) and abbreviations inside brackets, which are common with citations (“[A. Name, B. Other. Title of the work. Proc. Natl. Acad. Sci. 2010]”).
Obvious and common abbreviations (in English, Spanish, and German, so far) followed by a marker are dropped, too.
There are several other enhancements to the segmenter (e.g., checking for the presence of lower-case words that are unlikely start a sentence) that can be studied in the source code and unit tests.
In summary, while coming at a computational cost, this second check is what allows segtok to keep the number of false positive splits to an acceptable low if compared to existing methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="segtok-s-tokenization-strategy"&gt;
&lt;h2&gt;Segtok's tokenization strategy&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;segtok.tokenizer&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;symbol_tokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_tokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;web_tokenizer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;segtok.tokenizer&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;split_possessive_markers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_contractions&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The tokenization approach uses a similar approach.
First, a maximal token split is made, and then several functions wrap this basic approach, encapsulating successively more complex rules that join neighboring tokens back together based on their orthographic properties.
The basic, maximum split rule is to segment everything that is separated by spaces and then within the remaining non-space spans, split anything that is alphanumeric from any other symbols:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;a123, an &lt;span class="pre"&gt;alpha-/-beta...&lt;/span&gt;&lt;/tt&gt; → &lt;tt class="docutils literal"&gt;a123&amp;nbsp; ,&amp;nbsp; an&amp;nbsp; alpha&amp;nbsp; &lt;span class="pre"&gt;-/-&lt;/span&gt;&amp;nbsp; beta&amp;nbsp; ...&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;This functionality is provided by the &lt;tt class="docutils literal"&gt;symbol_tokenizer&lt;/tt&gt; .
Next, the non-alphanumeric &lt;em&gt;symbols&lt;/em&gt; are further analyzed to determine if they should form part of a neighboring alphanumeric &lt;em&gt;word&lt;/em&gt;.
If so, the symbols are merged back together with their alphanumeric spans.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Abbreviation markers are attached back on to the proceeding word (“Mr.”).&lt;/li&gt;
&lt;li&gt;Words with internal dots, dashes, apostrophes, and commas are joined together again (“192.168.1.0”, “Abel-Ryan's”, “a,b-symmetry”).&lt;/li&gt;
&lt;li&gt;The spaces inside a word-hyphen-spaces-word sequence are dropped.&lt;/li&gt;
&lt;li&gt;Superscript and subscript digits, optionally affixed with plus or minus, are attached to a proceeding word that is likely to be a physical unit (“m³”) or part of a chemical formula, respectively (“[Al₂(S₁O₄)₃]²⁻” → “[”, “Al₂”, “(”, “S₁O₄”, “)₃”, “]²⁻”).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This set of functionality is provided by the &lt;tt class="docutils literal"&gt;word_tokenizer&lt;/tt&gt;.
Finally, if desired, a Web-mode function will further ensure that valid e-mail addresses and URLs (including fragments and parameters, but without actual space characters) are always maintained as single tokens (&lt;tt class="docutils literal"&gt;web_tokenizer&lt;/tt&gt;).
All this ensures that while a decent amount of splitting is made, the common over-splitting of tokens is avoided.
Particularly, when processing biomedical documents, Web content, or patents, too much tokenization might have quite a significant negative impact on any subsequent, more advanced processing techniques.
As before with the segmenter, I believe this recovery of false positives is the particular strength of this library.&lt;/p&gt;
&lt;p&gt;After the tokenization step, the API provides two functions to optionally split off English possessive markers (“Fred's”, “Argus'”) and even contractions (“isn't” → “is n't” [note the attachment of the letter n], “he'll” → “he 'll”, “I've” → “I 've”, etc.) as their own tokens, which can be useful for downstream linguistic parsing (&lt;tt class="docutils literal"&gt;split_possessive_markers&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;split_contractions&lt;/tt&gt;).
To use them, just wrap your tokenizer with the preferred method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;SimpleTokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;split_multi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;split_contractions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;  &lt;span class="c1"&gt;# None to signal sentence terminals&lt;/span&gt;


&lt;span class="c1"&gt;# An even shorter usage example:&lt;/span&gt;
&lt;span class="n"&gt;my_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;split_contractions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt;
             &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;split_multi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;my_text&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="feedback-and-conclusions"&gt;
&lt;h2&gt;Feedback and Conclusions&lt;/h2&gt;
&lt;p&gt;All this functionality and the API itself are briefly documented on &lt;a class="reference external" href="https://pypi.python.org/pypi/segtok"&gt;segtok&lt;/a&gt;'s &amp;quot;homepage&amp;quot;.
As there is not very much functionality around, I hope that between this guide here and the overview there, the library should be fairly easy to use.
Furthermore, in command-line mode, using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--help&lt;/span&gt;&lt;/tt&gt; option will explain you all options provided by the two scripts the PyPI package installs.&lt;/p&gt;
&lt;p&gt;If you are be looking for new features, you are welcome to extend the library or request a new feature on the tool's GitHub &lt;a class="reference external" href="https://github.com/fnl/segtok/issues"&gt;Issues&lt;/a&gt; page (no guarantees, though... ;-)).
As a forum for discussing this tool, please use this &lt;a class="reference external" href="http://www.reddit.com/r/Python/comments/2sala9/segtok_a_rulebased_sentence_segmenter_and_word/"&gt;Reddit&lt;/a&gt; thread.
In addition, if you use this library and run into any problems, I would be glad to receive bug reports there, too.
Overall, I have attempted to keep the strategy used by segtok as slim as possible.
So if you are using any heavy language processing or sequence analysis tools after segtok, it should have no impact on your throughput at all.&lt;/p&gt;
&lt;p&gt;I have created this library after being disappointed by the other approaches in the wild, and for regular texts, my experience is that it works substantially superior in at least one of segmentation capabilities and/or runtime performance.
As I do not wish to bash any existing tool, I will only name one sentence segmentation approach I like very much: Punkt Tokenizer by Kiss and Strunk, 2006.
PT is a unsupervised, statistical approach to segmentation that “learns” whether to split sentences at sentence terminal markers.
While quite impressive and very versatile due to its unsupervised nature, I can state clearly that segtok's segmenter works substantially better on Germanic and Romance texts that (mostly) have a proper orthography.
Unsurprisingly, segtok's sentence segmenter is substantially faster than a comparable Python &lt;a class="reference external" href="http://www.nltk.org/_modules/nltk/tokenize/punkt.html"&gt;implementation&lt;/a&gt; of the Punkt Tokenizer by NLTK.&lt;/p&gt;
&lt;/div&gt;
</content><category term="text mining"></category><category term="nlp"></category><category term="Python"></category></entry><entry><title>A review of sparse sequence taggers</title><link href="http://fnl.es/a-review-of-sparse-sequence-taggers.html" rel="alternate"></link><published>2014-10-02T00:00:00+02:00</published><updated>2014-10-02T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2014-10-02:/a-review-of-sparse-sequence-taggers.html</id><summary type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Right now, use &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt; &lt;em&gt;unless&lt;/em&gt; you want to go beyond first-order and/or linear models, need the fastest possible training cycles, or are a Scala programmer, in which case you would be best advised to choose &lt;a class="reference external" href="http://factorie.cs.umass.edu/"&gt;Factorie&lt;/a&gt;.
OK, so that's that for a stressed out generation;
Read …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;
Right now, use &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt; &lt;em&gt;unless&lt;/em&gt; you want to go beyond first-order and/or linear models, need the fastest possible training cycles, or are a Scala programmer, in which case you would be best advised to choose &lt;a class="reference external" href="http://factorie.cs.umass.edu/"&gt;Factorie&lt;/a&gt;.
OK, so that's that for a stressed out generation;
Read on if you want to know why I recommend those two tools.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Overview:&lt;/strong&gt;
The goal of this review is to identify the &amp;quot;best&amp;quot; generic CRF- or MEMM-based sequence tagger software with a free (MIT/BSD-like) license.
We will only take discriminative models into account, so if your beef are generative models and/or non-sparse data (e.g., HMMs), you have come to the wrong place.
This article will look into their abilities to define and generate features, training times, tagging throughput, and tagging performance by way of working through a common sequence labeling problem: tagging the parts-of-speech of natural language.
While PoS tagging can be considered a &amp;quot;solved&amp;quot; problem, PoS tagging performance differences are still a source of &lt;a class="reference external" href="http://aclweb.org/aclwiki/index.php?title=POS_Tagging_%28State_of_the_art%29"&gt;academic&lt;/a&gt; controversy and therefore an ideal testing ground.&lt;/p&gt;
&lt;p&gt;Nearly any interesting natural language processing (NLP) task starts with word &lt;a class="reference external" href="http://text-processing.com/demo/tag/"&gt;tagging&lt;/a&gt;:
That is, resolving each word's grammatical sense - it's &lt;em&gt;part-of-speech&lt;/em&gt; (&lt;strong&gt;PoS&lt;/strong&gt;), the phrases they group into, and the semantic meaning the words carry.
PoS refers to a word's morphology, e.g., if it is used a noun or an adjective, or the inflection of the verb.
Words of the same morphology can often be grouped (we say, &lt;strong&gt;chunked&lt;/strong&gt;) into phrases, such as &amp;quot;a noun phrase&amp;quot;.
As for the semantic meaning, text miners are usually interested in identifying the relevant entity class/type (such as person, location, date/time, ...) the word refers to.
Furthermore, in NLP, words are commonly called &lt;strong&gt;tokens&lt;/strong&gt; to use a name that also covers symbols and numbers, including dots, brackets, or commas.
These tokens form the atomic sequence units for many statistical NLP methods.&lt;/p&gt;
&lt;p&gt;Similarly, in bioinformatics, you might want to identify properties of biological sequences, e.g., DNA binding sites or predict locations of post-translational modifications in proteins.
In general, any sequence tagger could be used to classify elements in any kind of sequence you can split into discrete units.
Therefore, in bioinformatics, those units might be nucleic acids (DNA bases) or amino acids (proteins).
However, the devil lays in the details:
The implementations I am interested in here are for &amp;quot;information-sparse sequences&amp;quot;, such as text.
The difference is the element (and resulting feature) sparsity: while there are easily 20,000 different tokens contained in any average text collection (such as a book), there are only four DNA bases and twenty-something amino acids (depending on the species).
All amino acids and bases will be - compared to text tokens, at least - frequently used in their respective sequences, with the obviously lowest sparsity for the four &lt;em&gt;standard&lt;/em&gt; DNA bases (long story hidden here...).
So to define some scope of this review, I am interested in learning patterns from extremely sparse data;
If your data is &amp;quot;dense&amp;quot;, you might be better off looking into more general algorithms, such as hidden Markov models (HMMs).&lt;/p&gt;
&lt;p&gt;You can play around with a few example NLP taggers by following the &lt;a class="reference external" href="http://text-processing.com/demo/tag/"&gt;tagging&lt;/a&gt; link, and you will see that depending on the system and its training data, results can vary widely.
This is due to implementation details, graphical model capabilities, and the sequence features used by the particular model instance.
The common approach to this kind of problem is learning a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Discriminative_model"&gt;discriminative&lt;/a&gt;, dynamic graphical model; commonly, a maximum entropy (MaxEnt, aka. logistic regression) based decision function worked into a reverse &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Markov_model"&gt;Markov model&lt;/a&gt; or into the more complex Markov random field, in this particular case called a &lt;em&gt;Conditional Random Field&lt;/em&gt; (&lt;strong&gt;CRF&lt;/strong&gt;).
In the former &lt;em&gt;MaxEnt Markov model&lt;/em&gt; (&lt;strong&gt;MEMM&lt;/strong&gt;) scenario, you are only allowed to use the current state (element in your sequence, including any meta-data assigned to that element) and the last tag(s) to predict the current tag (aka. label).
The latter CRF model allows you to integrate features not just from the current state, but from any state in the sequence being labeled.
So with CRFs you can even use states from the &amp;quot;future&amp;quot; (i.e., elements later in the sequence that the one currently being tagged) to predict the current label (aka. tag).&lt;/p&gt;
&lt;p&gt;While outside the scope of this article, in case you are now asking yourself &amp;quot;Why do I then even want a MEMM instead of a CRF?&amp;quot;:
Defenders of MEMMs claim that &amp;quot;their&amp;quot; model has an edge because it does not tend to overfit the data (we say, it has a weaker &amp;quot;domain adaptation&amp;quot;) as easily as a CRF can (due to the way features can be generated from any part of the sequence) and therefore produces better tagging results on input that is not similar to (of a &amp;quot;different domain&amp;quot; than) the training data.
Second, due to its feature selection limitation, training of a MEMM tends to be faster than training a CRF with &amp;quot;long-range&amp;quot; features from distant positions in the sequence.&lt;/p&gt;
&lt;p&gt;Both regular Markov models and random fields use the notion of &lt;em&gt;Markov order&lt;/em&gt;.
That is, the number of former (already assigned) labels in a linear chain (sequence) that may be used to calculate the probabilities of each possible label on the current state.
To be more precise, it is the &lt;em&gt;transition probability&lt;/em&gt; from one (first order) or more (second, third, ... order) former labels to the label on the current state that is the statistic being modeled.
This &amp;quot;&lt;tt class="docutils literal"&gt;n&lt;/tt&gt;th-order Markov&amp;quot; limit defines how many prior tags the model will consider when tagging the current state:
For first order Markov chains you only can make use of the last tag, for second order models you get to use the last two tags, and so forth.
Due to the expense of going beyond first-order models, all but one tool we will be looking at do not support more than first-order models:
Linear chain models scale exponentially in the Markov order + 1, meaning that a second order model already has &amp;quot;number of labels&amp;quot;-cubed possible label transitions.
Going beyond second-order Markov models is only desirable if the number of labels and (as we will see) sometimes even states is small (i.e., &amp;quot;dense data&amp;quot;).
In language processing, these states normally are all the unique, observed tokens, also known as the &lt;em&gt;vocabulary&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A quick, shameless self-plug: if you are not familiar with these concepts, have a look at my &lt;a class="reference external" href="http://fnl.es/an-introduction-to-statistical-text-mining.html"&gt;introduction to text mining&lt;/a&gt; slides - or come visit my class in the context of the Madrid summer school on &lt;a class="reference external" href="http://www.dia.fi.upm.es/?q=es/ASDM"&gt;Advanced Statistics and Data Mining&lt;/a&gt; next summer (beginning of July)! The lecture will take you from basic Bayesian statistics all the way to the dynamic, graphical models being discussed here.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="sequence-tagger-selection"&gt;
&lt;h2&gt;Sequence tagger selection&lt;/h2&gt;
&lt;p&gt;Recently, I have become anxious to once and for all resolve my doubts about the &amp;quot;best&amp;quot; sparse sequence tagger in terms of ease of use (documentation/UI), feature modeling capabilities, training times, tagging throughput (tokens/second) and the resulting accuracy.
All tools use the same optimization procedures for the learning process, that is a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Limited-memory_BFGS"&gt;L-BFGS&lt;/a&gt; optimizer and a few light-weight gradient descent implementations as alternatives.
However, implementation details, the graphical model abilities, the features the system can work with, the facilities it provides to generate them, the system's throughput, the provided documentation, and its availability (both in open source and free software terms) varies greatly between libraries.
Available software for this task that I considered were &lt;a class="reference external" href="http://crfpp.sourceforge.net/"&gt;CRF++&lt;/a&gt; (Kudo), &lt;a class="reference external" href="http://www.chokkan.org/software/crfsuite/"&gt;CRF Suite&lt;/a&gt; (Okazaki), &lt;a class="reference external" href="http://factorie.cs.umass.edu/"&gt;Factorie&lt;/a&gt; (McCallum &amp;amp;al.), &lt;a class="reference external" href="http://flexcrfs.sourceforge.net/"&gt;FlexCRFs&lt;/a&gt; (Phan, Nguyen &amp;amp; Nguyen), &lt;a class="reference external" href="http://alias-i.com/lingpipe/index.html"&gt;LingPipe&lt;/a&gt; (Carpenter &amp;amp;al.), &lt;a class="reference external" href="http://mallet.cs.umass.edu/"&gt;MALLET&lt;/a&gt; (McCallum &amp;amp;al.), &lt;a class="reference external" href="http://www.umiacs.umd.edu/~hal/megam/"&gt;MEGAM&lt;/a&gt; (Daume III), [Apache] &lt;a class="reference external" href="http://opennlp.apache.org/"&gt;OpenNLP&lt;/a&gt; (Kottmann &amp;amp;al.), &lt;a class="reference external" href="http://nlp.stanford.edu/software/tagger.shtml"&gt;Stanford Tagger&lt;/a&gt; (Manning, Jurafsky &amp;amp; Liang), &lt;a class="reference external" href="http://www.lsi.upc.edu/~nlp/SVMTool/"&gt;SVM Tool&lt;/a&gt; (Giménez &amp;amp; Marquez), &lt;a class="reference external" href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"&gt;TreeTagger&lt;/a&gt; (Schmidt), and &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt; (Lavergne).
There are more options &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Conditional_random_field#Software"&gt;around&lt;/a&gt;, particular in C# and for the .Net platform, but as I do not have the money to pay for the Windows tax, I did not consider them.
If you know of a relevant, &lt;em&gt;generic&lt;/em&gt; sparse sequence tagger implementation I missed (see my filtering criteria below), please &lt;a class="reference external" href="http://fnl.es/pages/about.html"&gt;contact me&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I immediately discarded the Stanford Tagger and the SVM Tool, because they are both orders of magnitude &lt;a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf"&gt;slower&lt;/a&gt; than most other tools considered (and the same goes for MALLET, &lt;a class="reference external" href="http://www.chokkan.org/software/crfsuite/benchmark.html"&gt;too&lt;/a&gt;).
It is worth mentioning that the Stanford Tagger was one of the earliest tools with the software made available for research and a very high accuracy, and as such usually serves as the performance &amp;quot;baseline&amp;quot; for newcomers.
Second, CRF Suite claims to be the fastest first order CRF around and &lt;a class="reference external" href="http://www.chokkan.org/software/crfsuite/benchmark.html"&gt;demonstrates&lt;/a&gt; that CRF++ is significantly slower, which lead me to discard the latter.
That same benchmark claims that CRF Suite is faster than Wapiti, but not only has Lavergne developed several newer versions since then, the difference is far less pronounced, so that tool was not out of the race for me.
Being a free software advocate in the sense of all its aspects - cost, freedom of usage, modifiability, and open source - I feel very uncomfortable about using software with a license that tries to restrict my freedom, including its commercial application.
Therefore, I discarded FlexCRFs, LingPipe, MEGAM, and TreeTagger from the list because of their non-free nature (only being &amp;quot;free for research&amp;quot;, or GPL'ed).
While the GPL is not strictly out of my scope, it creates too many headaches for too many use-cases because it still poses usage restrictions (that I nonetheless support as a necessary evil given the overall copyright SNAFU).
Moreover, excluding the GPL only affects FlexCRFs, which anyways is very similar to CRF++ or CRF Suite.
Two of the already discarded tools would also not make it across this &amp;quot;free software barrier&amp;quot;, by the way (Stanford Tagger and SVM Tool).&lt;/p&gt;
&lt;p&gt;So this left me with CRF Suite, Factorie, OpenNLP, and Wapiti to compare against each other.
Given these harsh pre-filtering criteria, to be honest, I was astonished that I was left with not just one, but four viable and completely free &amp;quot;tools of the trade&amp;quot;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="implementation-considerations"&gt;
&lt;h2&gt;Implementation considerations&lt;/h2&gt;
&lt;p&gt;CRF Suite and Wapiti are both written in C, while Factorie is being coded in Scala, and OpenNLP is based on Java.
So this makes for yet another classical &amp;quot;binary, platform-specific code versus the Java Virtual Machine&amp;quot; comparison!
(Spoiler alert: it does not matter - as you should know already...)
But there a real, noteworthy differences between the taggers; starting with the implemented graphical models and optimization procedures:
Factorie's PoS tagger implementation only makes use of a forward learning procedure, while the other three use the more common (and more expensive) &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm"&gt;forward-backward optimization&lt;/a&gt; approach during training.
So this difference makes up for an interesting test: will forward learning alone be good enough in terms of accuracy, and if so, how much faster will training be?
(You could also read a &lt;a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf"&gt;paper&lt;/a&gt; about this...)
Furthermore, Factorie is a library that allows you to design &lt;em&gt;any kind&lt;/em&gt; of graphical models from basic factor classes (let that sink in if you know what I mean...), so it actually can represent any model you want (including non-linear models).
For the PoS tagging, Factorie uses this forward-only learning approach to maximize a first-order linear-chain CRF.
Next up, Wapiti allows you to choose between a (non-dynamic, pure) MaxEnt model, a MEMM or a CRF model.
Similar to the above &lt;a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf"&gt;paper&lt;/a&gt; linked to, Wapiti can even do dynamic model selection, falling back on simpler models where feasible in the sequence
(Note that Factorie's PoS tagger does not use dynamic model selection.)
Finally, OpenNLP only provides a MEMM implementation, while CRF Suite only provides a CRF.
These implementation details alone might be enough to make your decision:
If you want more than a first-order linear-chain model (say, second-order, or a non-linear graph), your only choice is Factorie.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="software-state-and-documentation"&gt;
&lt;h2&gt;Software state and documentation&lt;/h2&gt;
&lt;p&gt;First, a quick look at the code, implementation, the documentation, and each tool's multi-processor capabilities.
Two remarkable things about &lt;strong&gt;Wapiti&lt;/strong&gt; are how simple and lean the interface is, and its capability of running in multi-threaded mode.
While code is the typical, long spaghetti-code of C, it is clean and very well documented.
The only main downside is that the documentation is a bit sparse; Everything is in there, but they could have done a bit better detailing some of the capabilities, and/or providing examples.
I had to figure out myself that you always need to use both the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-c&lt;/span&gt; &lt;span class="pre"&gt;-s&lt;/span&gt;&lt;/tt&gt; switches when doing (feature-sparse) text labeling and it took me some time to understand how to do feature extraction (designing &amp;quot;patterns&amp;quot;).
The Wapiti authors do not provide a default set of feature pattern templates, only a few pre-trained PoS models for English, German, and Arabic newswire text.&lt;/p&gt;
&lt;p&gt;Similarly, &lt;strong&gt;Factorie&lt;/strong&gt; is able to run in multi-threaded mode and claims to be scalable across machines for hyper-parameter search (which I have not tried).
To use Factorie, I often have to refer back to the code-base, because few of the specifics of Factorie are entirely documented for now.
This means, to use Factorie, you should better know some Scala, as it might be tough having to go through the code otherwise.
In my opinion, some parts of the codebase could have been coded just as well in Java, but that only affects few regions of the library.
One should also note that this opinion is purely subjective and might be of little relevance, so you might be better of judging for yourself.
Finally, the documentation certainly assumes you are an expert for graphical models with plenty of background knowledge in that domain.
Given its state and direction, in comparison to the other tools here, it is probably safe to judge that this library is targeted at the probabilistic programming crack with a background on graphical models, not someone looking for a quick and dirty sequence tagging solution.
The main up-side is that, together with OpenNLP, this is the only library offering a full NLP pipeline (segmentation, tokenization, tagging, and parsing).
But again, except for PoS tagging, the other NLP functionality has to be deduced from the code, as there is not much more documentation on the NLP pipeline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRF Suite&lt;/strong&gt; comes with a nice interface, and although it does not support multi-threading, the C code is well written and very clear.
The functions are short and precise, so it is a nice example how C code can look if you put some effort into it.
The only code-wise downside I detected was the accompanying Python code for preparing and pruning data and benchmarking.
It is clearly written by a C expert with little knowledge of idiomatic Python, no offense (see the performance issues for feature extraction below).
However, I think this is a minor issue given the good documentation and high-quality C code, which in the end is the part that matters.
The worst issue with CRF Suite, however, seems to be that the original author has stopped maintaining the code.
The repository on GitHub has a handful of very good pull requests from serious developers that fix things like a minor memory leak and two or three other issues, but the author has never accepted the requests.
Neither have there been any updates to the library.
To me this means that CRF Suite development seems dead and it would have to be significantly better than the other tools here to make it worth using it.&lt;/p&gt;
&lt;p&gt;Finally, &lt;strong&gt;OpenNLP&lt;/strong&gt; has the expected high quality code found in Apache projects, and it is well documented, too.
The only downsides worthwhile mentioning are that there is no built-in parallel processing support and that it only comes with a MaxEnt Markov model.
As mentioned already, it is important to realize that OpenNLP offers a full NLP pipeline, unlike CRF Suite and Wapiti.&lt;/p&gt;
&lt;p&gt;To summarize, unique capabilities of Wapiti are its built-in template-based feature extraction mechanism and its ability to quickly choose either CRF or MEMM as the target model (more on this below).
A unique capability of Factorie is that it provides you with the necessary base classes to quickly code your own graphical models of any Markov order, both linear and non-linear.
However, admittedly, both Wapiti and Factorie are behind CRF Suite and OpenNLP in terms of documentation and in my opinion, Factorie is not consistently using idiomatic Scala, probably due to its many different developers.
Finally, both OpenNLP and Factorie include a full, documented and undocumented (respectively) NLP pipeline.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="feature-modeling"&gt;
&lt;h2&gt;Feature modeling&lt;/h2&gt;
&lt;p&gt;This leads to the next important consideration: modeling features; for example, via templates (or &amp;quot;patterns&amp;quot;) that define the features used by a model's binary indicator functions.
For example, an indicator function for assigning the PoS tag &amp;quot;VBZ&amp;quot; might be triggered when observing the bigram &amp;quot;I went&amp;quot; and having already assigned the PoS label &amp;quot;PRP&amp;quot; to the token &amp;quot;I&amp;quot;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
f(last_tag, current_tag, states, pos) =
    last_tag == &amp;quot;PRP&amp;quot; &amp;amp;&amp;amp; current_tag == &amp;quot;VBZ&amp;quot; &amp;amp;&amp;amp;
    states[pos-1] == &amp;quot;I&amp;quot; &amp;amp;&amp;amp; states[pos] == &amp;quot;went&amp;quot;
&lt;/pre&gt;
&lt;p&gt;This example is called a &amp;quot;combined (bigram) transition/label and state/token feature&amp;quot;
(And would be minimally encoded as &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;b:%x[-1,0]/%x[0,0]&lt;/span&gt;&lt;/tt&gt; in Wapiti's pattern template language.)
It is a rather &amp;quot;expensive&amp;quot; feature template, as it can easily lead to millions of individual indicator functions:
The number of indicator functions created from this template will be the squared number of PoS tags (label transitions or &amp;quot;bigrams&amp;quot;) times the number of unique token bigrams in the whole training data.
In general, for any template - even a constant one - there will be at least as many indicator functions as there are different tags.&lt;/p&gt;
&lt;p&gt;Except for Wapiti, all taggers come with a pre-defined set of feature templates for common NLP tasks.
Depending on your requirements, this might be either very practical or practically useless, particularly for domain-specific language (tweets, for example) and/or NER tagging.
For NER tagging, your entities might have unique morphological or orthographic properties;
For example, gene names might be used not just as nouns, but as adjectives, too (as in &amp;quot;p53-activated DNA repair&amp;quot;) and contain Roman or Arab numbers, Greek symbols, non-standard dashes and a few other orthographic surprises.
In addition, the entity tag might depend on &amp;quot;knowing the future&amp;quot;, such as the up-coming head token of the noun phrase currently being tagged (e.g., the head &amp;quot;gene&amp;quot; in &amp;quot;the ABC transporter gene&amp;quot; when looking at the token &amp;quot;ABC&amp;quot;).&lt;/p&gt;
&lt;p&gt;The predefined &lt;strong&gt;Factorie&lt;/strong&gt; features are, however, pretty good - so rich indeed, that they are more complete than any other set of features I used or provided myself in the experiments here (see &lt;tt class="docutils literal"&gt;FowardPosTagger.scala&lt;/tt&gt;, &lt;a class="reference external" href="https://github.com/factorie/factorie/blob/master/src/main/scala/cc/factorie/app/nlp/pos/ForwardPosTagger.scala"&gt;features&lt;/a&gt;, for PoS tagging).
That means training could be slow for Factorie, because it needs to optimize over a much larger indicator (feature) function space (turns out it is not, as we will see).
As with all systems except for Wapiti, this means you need to do some coding of your own to adapt the features for NER, while it might or might not be necessary for PoS tagging and phrase chunking tasks.
The real issue with Factorie is figuring out how to define your own NER tagger, as the documentation so far only covers PoS taggers.
More generally speaking, the documentation on generating features and models for your taggers is rather thin in terms of &amp;quot;applied&amp;quot; examples (see User Guide - Learning).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OpenNLP&lt;/strong&gt;, too, comes with a pre-selected list of feature templates for standard NLP tasks.
To change this list, you either have to write your own Java code or, at least for NER tagging, the documentation states that you can conjure up XML configuration files to extract different features.
As I am allergic to any use of XML other than its intended use-case - providing structure to unstructured data - and particularly against the use of XML as a vehicle for configuration files (hello Java/Maven world!), I did not even try this path.
In other words, for OpenNLP I will be using the predefined features and have not experimented with the &amp;quot;XML feature configuration&amp;quot; option, so I cannot tell how well it works or how easy it is to use.
As for PoS tagging, OpenNLP uses pretty much the &lt;em&gt;de facto&lt;/em&gt; standard features (prefixes, suffixes, orthographic features, and a window size of 5 [-2,+2] for the n-grams; see the &lt;tt class="docutils literal"&gt;DefaultPOSContextGenerator&lt;/tt&gt; &lt;a class="reference external" href="http://svn.apache.org/viewvc/opennlp/trunk/opennlp-tools/src/main/java/opennlp/tools/postag/DefaultPOSContextGenerator.java?view=markup"&gt;class&lt;/a&gt;), so that seemed good enough for this test.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRF Suite&lt;/strong&gt; comes with a set of Python scripts to convert simple OWPL files (one word [state] per line, with sentences [sequences] separated by an extra empty line, such as the CoNLL format) into the &amp;quot;per-label feature list files&amp;quot; CRF Suite uses as input.
To create different features, you modify the &amp;quot;template&amp;quot; defined inside the relevant Python script.
For most cases, I think the predefined templates do a pretty good job at generating features for standard NLP tagging tasks.
Additional features uniquely generated by CRF Suite and not OpenNLP or Factorie (for PoS tagging) are quadrigrams, pentagrams, and &amp;quot;long-range interactions&amp;quot;.
The latter are bigrams created from the current word and a word at position +/- 2 to 9 from the current word.
If you commonly work with Python, you might even easily assimilate the Python feature generation process, adapting it to your own needs.
CRF Suite's feature handling has an important shortcoming, however:
It is impossible to work with combined &amp;quot;label bigrams&amp;quot; (1st order Markov transition features) together with other (state) features from the token stream to form more advanced indicator functions.
That is, CRF Suite only models either label transition probabilities or the features from the current state, but does not allow you to create &amp;quot;mixed&amp;quot; indicator functions as described in the beginning of this section (&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;b:%s[-1,0]/%x[0,0]&lt;/span&gt;&lt;/tt&gt;).
This is an important conceptual shortcoming, because it is not possible to define features that condition on both the previous label (i.e., the transition) and the current token.
However, as opposed to the other tools here (that only work with discrete features), CRF Suite provides support for continuos features.
For example, when using &lt;a class="reference external" href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"&gt;word embedding&lt;/a&gt; techniques, you might want to directly include the numeric word vectors, which you can simply pass on to CRF Suite.
While this shortcoming is commonly is circumvented by discretizing the real-valued features if continuos feature support is unavailable, it is a noteworthy difference.&lt;/p&gt;
&lt;p&gt;As for &lt;strong&gt;Wapiti&lt;/strong&gt;, you have to figure out how to generate features for each task on your own;
The authors do not provide any predefined templates for &amp;quot;standard&amp;quot; NLP tasks.
But Wapiti provides you with a mechanism to define &amp;quot;patterns&amp;quot;, much like CRF++' feature templates.
Once you fully understand the mechanism, this is indeed quite powerful and it felt like I &amp;quot;missed&amp;quot; it in the other tools.
Particularly, this means that feature extraction is done in C, so it will beat a script-based extraction process, while not requiring any C programming knowledge.
To provide an even playing field, at first I defined the same features &amp;quot;patterns&amp;quot; as CRF Suite does via its Python feature generation scripts.
The problem is that Wapiti uses all possible label and state combinations for its initial training matrix, not just all combinations present in the data.
In other words, it is the only tool that does no feature space reduction &lt;em&gt;prior&lt;/em&gt; to going into training.
For example, if you define a pattern such as the current token (&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;u:%[0,0]&lt;/span&gt;&lt;/tt&gt;), it creates one feature for each label in you training set times the number of unique tokens in your data, no matter if a token is observed with that label in the data or not.
So for token n-grams or label bigrams, the training matrix can quickly grow to extraordinary sizes.
It is worth noting that you can use your own feature extraction method and just feed Wapiti with extracted features directly (i.e., strings that start with &amp;quot;a&amp;quot; or &amp;quot;b&amp;quot;, depending on whether you want to use only the current state or integrate the transition, too).
The advantage - I assume, at least - is that the optimizer might decide to transfer some probability mass to those zero observations.
However, it is not clear to me if or how much performance Wapiti gains from such transfers, particularly when contrasting this unique feature with the greatly increased space penalty:
While Wapiti does compacting and supports sparse matrices during training, as it initially starts of with all features, training becomes rather sluggish when very large feature spaces are defined.
By using the same feature templates as CRF Suite, I ended up with an initial matrix containing a few hundred million features.
This simply was too much for my weak dual-core i5 processor to handle in realistic time.
In the end, I decided to cut down on the number of PoS feature templates with respect to CRF Suite or Factorie.
In particular, I removed the feature templates only CRF Suite has otherwise, thereby reducing the initial setup to 44 million &amp;quot;features&amp;quot; (indicator functions).
After compacting, Wapiti's final model contained 1.8 million indicator functions (&amp;quot;features functions&amp;quot;, or worse, sometimes just called &amp;quot;features&amp;quot;) for the PoS tagging trials (see below).
As should be noted, that reduced set was enough to out-compete all but Factorie in terms of accuracy while using each tool's default parameter settings.&lt;/p&gt;
&lt;p&gt;In summary, with OpenNLP, Factorie, and CRF Suite you will need to work with their respective feature generation API (in Java, Scala, and Python, respectively) to model features beyond anything but newswire PoS tagging, phrase chunking, and some basic NER.
CRF Suite, similar to Factorie, has rich, pre-defined feature templates and can handle them, because unused indicator functions are dropped before the actual training starts, thereby keeping the initial feature weight matrix manageable.
In addition, it is the only tool in this review that can handle real-valued features.
Pre-training feature (space) &lt;em&gt;reduction&lt;/em&gt; is done by all tools except Wapiti.
Feature &lt;em&gt;compaction&lt;/em&gt; after training is done by all tools except OpenNLP, where I could not confirm if any compaction had occurred.
Wapiti provides a very powerful pattern language to define feature templates, including mixed state and transition label templates, which are otherwise only possible to generate with Factorie.
While Wapiti's template (pattern) language lends to a great flexibility when modeling features, it has to be used with care unless training times are not an issue due to the maximal (non-reduced) feature matrix used during the first training cycles.
At the end of the day, in terms of feature generation, once you learn how to use Wapiti's pattern &amp;quot;language&amp;quot;, it will be very efficient and spares you from writing code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="training-time"&gt;
&lt;h2&gt;Training time&lt;/h2&gt;
&lt;p&gt;Next, I looked into the training run-times to see how long each tool takes to create a PoS model.
To make an equal, but simple comparison, I used the &lt;a class="reference external" href="http://www.cnts.ua.ac.be/conll2000/chunking/"&gt;CoNLL 2000&lt;/a&gt; PoS tags to train the models using the default feature templates as discussed in the last section.
Both Factorie and OpenNLP needed slight, but simple modifications to the downloaded CoNLL files.
For Factorie, the reversed parenthesis tags in the CoNLL files had to be fixed.
The main observation here is that Factorie is not very helpful in terms of error messages to understand the problem;
It just throws some obscure exception at you.
This means you will have to figure out what went wrong when you get errors on your own.
OpenNLP's problem was simpler to identify: as documented, it expects one sentence per line, with token-tag pairs separated by underscores instead of the de facto standard OWPL format.&lt;/p&gt;
&lt;p&gt;To train the the taggers for &lt;em&gt;Part-of-Speech&lt;/em&gt;, the commands I used were:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# CRF Suite
crfsuite learn -m pos.model train.txt

# Factorie
java -cp factorie-1.1-SNAPSHOT-nlp-jar-with-dependencies.jar \
     cc.factorie.app.nlp.pos.ForwardPosTrainer -Xmx2g \
     --owpl --train-file=train.txt --test-file=empty.txt \
     --model=pos.model --save-model=true

# OpenNLP
opennlp POSTaggerTrainer -type maxent -model pos.model -data train.txt

# Wapiti
wapiti train -c -s -p patterns.txt train.txt pos.model
&lt;/pre&gt;
&lt;p&gt;The input data is provided in &lt;tt class="docutils literal"&gt;train.txt&lt;/tt&gt;, and the models are saved to &lt;tt class="docutils literal"&gt;pos.model&lt;/tt&gt;.
To measure the training times, I prefixed each command with &lt;tt class="docutils literal"&gt;time&lt;/tt&gt; and used &lt;a class="reference external" href="http://stackoverflow.com/a/556411"&gt;the sum of user+sys&lt;/a&gt; as the measured, total time it took each process to complete.
This means, the measurement includes all relevant CPU time (i.e., over all processor cores) that was consumed by the run.
This might seem unfair to multi-threaded code, which might have an actual runtime lower than the result.
However, this is entirely depended on your machine and its cores, so a direct &amp;quot;total CPU time&amp;quot; comparison seemed fair to me.
In my case, it also is a rather minor issue, because I anyway only have two (hyper-threaded) cores on my laptop.
To be fair, this most significantly only affects Wapiti runtime, so I report total time there, too.
There are other opinions about performance measurements, e.g., that one should only measure post-warm-up training time (minus JVM, input data reading, etc.) or only a single training cycle/iteration should be measured.
I think it is more practical to measure and compare whatever the &amp;quot;out-of-the-box&amp;quot; performance of each tool is.
Each training process is run thrice and the shortest measured time is the one I report here.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="28%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="28%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="40%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Training Time&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;CRF Suite&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;3.63 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;10m 22s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Factorie&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;0.34 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;02m 18s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;OpenNLP&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;???? M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;02m 03s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Wapiti&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;1.56 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;19m 06s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Wapiti-4*&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;1.56 M&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;10m 09s*&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;[* absolute runtime (&amp;quot;real&amp;quot;) in multi-threaded mode using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-t&lt;/span&gt; 4&lt;/tt&gt; switch to make full use of my hyper-threaded dual-core i5 processor]&lt;/p&gt;
&lt;p&gt;As mentioned earlier, with respect to initial feature template richness, CRF Suite and Factorie are taking the lead, with Wapiti and OpenNLP using less templates.
The models' feature sizes shown here are as reported by each tool &lt;em&gt;after&lt;/em&gt; all feature pruning steps (compaction;
For Wapiti that number is calculated from &amp;quot;initial features&amp;quot; minus &amp;quot;removed features&amp;quot;.)
While OpenNLP does not report final feature set sizes, I &lt;em&gt;assume&lt;/em&gt; it to be in a similar range (somewhere around a million features).
So in terms of feature compaction, Factorie has a clear edge over the competition.&lt;/p&gt;
&lt;p&gt;In terms of training times, Factorie and OpenNLP easily outpace both CRF Suite and Wapiti, but this should not be entirely surprising:
OpenNLP uses a simpler model (MEMM), so it clearly must be faster.
One noteworthy point is the training speed of Factorie - probably due to forward-only learning, it achieves similar training times for its CRF as OpenNLP on a MEMM.
On the other end, as expected, Wapiti is by far the most resource-hungry tagger.
As Wapiti's &lt;em&gt;learning&lt;/em&gt; procedure can easily make use of multiple CPU cores with a simple switch, it gained significantly in terms of absolute (&amp;quot;real&amp;quot;) training time from running in multi-threaded mode on my dual core machine, at least.
This is important, because while tagging is what is called &amp;quot;embarrassingly parallel&amp;quot;, learning/optimization is not.
Still, this means the top model training implementation is provided by Factorie, as OpenNLP has a much simpler model to train.
The remaining question in this respect will be if OpenNLP and Factorie can keep up with the accuracy of the other two CRFs and how fast they all perform their tagging.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="tagging-quality"&gt;
&lt;h2&gt;Tagging quality&lt;/h2&gt;
&lt;p&gt;This section will resolve the final remaining question:
Which implementation can provide you with the most efficient tagger?
I have a SATA-3-attached SSD drive where the data is read from (but a slow i5 CPU...) and took the CoNLL 2000 test set sequences to measure accuracy on the 47,377 tokens using the models I had trained in the last step.
So while my measurements do not include writing the tagged tokens back to the device and reading data should not be an issue with a SSD, my CPU isn't exactly &amp;quot;Speedy Gonzales...&amp;quot;
I timed each system while tagging 100 times those 47,377 tokens in a single row, read from one file (i.e., about 200,000 sentences or roughly 30,000 scientific abstracts) to make a fair comparison of each system's token throughput, marginalizing any warm-up &amp;quot;penalties&amp;quot;.&lt;/p&gt;
&lt;p&gt;To run the the taggers on the generated models, the commands I used were:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# CRF Suite
cat test.txt | pos.py &amp;gt; features.txt
crfsuite tag -m pos.model -tq features.txt

# Factorie
java -cp factorie-1.1-SNAPSHOT-nlp-jar-with-dependencies.jar \
     cc.factorie.app.nlp.pos.ForwardPosTester -Xmx2g \
     --owpl --model=pos.model --test-file=test.txt

# OpenNLP
opennlp POSTaggerEvaluator -model pos.model -data test.txt

# Wapiti
wapiti label -m pos.model -c test.txt &amp;gt; /dev/null
&lt;/pre&gt;
&lt;p&gt;And here are the results, in terms of error rate (&lt;tt class="docutils literal"&gt;1 - accuracy&lt;/tt&gt; over 47k tokens) and throughput (on 201k sentences with 4.7M tokens, as measured with &lt;tt class="docutils literal"&gt;time&lt;/tt&gt;, sys+user):&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="27%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="31%" /&gt;
&lt;col width="2%" /&gt;
&lt;col width="38%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Error Rate&lt;/strong&gt;&lt;/th&gt;
&lt;th class="head"&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class="head"&gt;&lt;strong&gt;Tokens/Second&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;CRF Suite&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;3.00 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;30,000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Factorie&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;2.19 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;23,800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;OpenNLP&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;2.88 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;19,500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Wapiti&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;2.20 %&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;21,200&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note that for the CRF Suite, I pre-generated the features from the Python script.
If not, the numbers would look quite bad, as the whole Python-based feature extraction process using the &lt;tt class="docutils literal"&gt;pos.py&lt;/tt&gt; script is several times slower than the tagger itself!
(A good indicator that the underlying Python code could arguably use some polish...)
In terms of tagging throughput, rather to my astonishment, it seems fair to say that the libraries perform roughly equal and there are by and large no noteworthy differences.
It seems that CRF Suite is faster, but then we actually cheated, because we pre-generated the label features.
So at best there is a minor chance that the CRF Suite could be faster than the others, if it had a very fast feature extraction mechanism.
Another remark maybe is that Factorie automatically detects the available cores and equally distributes the tagging load among them
(Note that the throughput calculation is based on CPU time (sys+user), so in absolute numbers on my CPU, Factorie tags at about 47k tokens/second.)
While the tagging process is an &amp;quot;embarrassingly parallel&amp;quot; problem (you could just split up the input between as many cores as you have and run your tagger with GNU &lt;a class="reference external" href="https://www.gnu.org/software/parallel/"&gt;parallel&lt;/a&gt;), it is a nice little extra thrown into the mix.
Overall, in terms of raw tagging throughput, there might be no real &amp;quot;winner&amp;quot;.&lt;/p&gt;
&lt;p&gt;Regarding accuracy, you might want to know that a baseline tagger (using the majority PoS tag and tagging all unseen words as noun) already achieves an accuracy of 90% (or, an error rate of 10%) in standard PoS scenarios.
Probably due to the inability to use mixed features and because it does its feature compaction &lt;em&gt;prior&lt;/em&gt; to the training, the CRF Suite has the worst performance in terms of accuracy, closely followed by OpenNLP.
OpenNLP's shortcoming most likely can be attributed to its model choice, which only really starts to shine in cross-domain experiments.
So in terms of high quality tagging, at least if you have training data specifically for that domain, you will be better off with Factorie or Wapiti - not all that unexpected, given our feature modeling and model implementation insights.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusions"&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;If you only need to do standard PoS tagging, chunking, and/or NER, and don't mind the tagging quality or performance too much, just go with the tool in your favorite language: OpenNLP for Java developers, Factorie for Scala hackers, and Wapiti for C/C++ or Python programmers
(There is a &lt;a class="reference external" href="https://github.com/adsva/python-wapiti"&gt;Python wrapper&lt;/a&gt; for Wapiti available, if Python (for both 2 and 3) is your deal.)
The trade-offs are simply not big enough to make a huge (order-of-magnitude) difference.
But then, if that were your case, you probably would not have read until here...&lt;/p&gt;
&lt;p&gt;Because there is no conceivable advantage in terms of training times, tagging throughput, or accuracy, no support for mixed transition/state features, and, particularly, as the code seems unmaintained, I would not recommend the use of CRF Suite. The missing ability to scale training across computing cores is similar to OpenNLP, and another possible issue. Nonetheless, the tool (and the wrapper) has gotten some love from Mikhail Korobov, who has a commercial interest in (maintaining) it, too. As just hinted, there is a Python wrapper for CRF Suite &lt;a class="reference external" href="https://github.com/tpeng/python-crfsuite"&gt;available&lt;/a&gt;, too, much like Wapiti. So if you need to work with real-valued features (e.g., for adding word-vector representations) and/or do not mind the mentioned issues, CRF Suite might still be an interesting option for you. (Have a look at the discussion linked below.)&lt;/p&gt;
&lt;p&gt;At the end of the day, if you are interested in generating high-performance quality annotations while using mixture (state + transition) features, there really is only the choice between Wapiti and Factorie:
Foremost, they are the only two tools ready for the multi-core world of today.
An error rate reduction of about 25% on the &amp;quot;solved&amp;quot; PoS tagging problem at no cost in throughput is not to be underestimated.
Wapiti definitely is the most attractive tagger in terms of off-the-shelf usability: feature generation is simple with the patterns with no need for doing any coding, and the overall implementation complexity vs. usability balance is excellent.
The only tradeoff are possibly longer training times (in single-core mode or vs. Factorie), so you will need to develop and combine your feature templates with care.
Finally, if you also are looking for true flexibility and the full power of graphical models, or want to venture into higher order Markov space, I see no way around Factorie.
In this case, it might be worth living with the sparse documentation and having to study Scala source code.
However, the team around McCallum are very actively working on this library, and the documentation is certainly getting better and more extensive every other time I come back and a few months have passed.
Another advantage is the fact that Factorie offers a (largely undocumented) full NLP pipeline, starting from sentence segmentation all the way to dependency parsing.&lt;/p&gt;
&lt;p&gt;Nonetheless, if you'd ask me to declare a &lt;em&gt;global&lt;/em&gt; &amp;quot;winner&amp;quot;, unless you are a probabilistic programming or at least Scala expert, I'd say that honor right now still goes to &lt;a class="reference external" href="http://wapiti.limsi.fr/"&gt;Wapiti&lt;/a&gt;.
But once Factorie fixes the mentioned issues and makes the library more accessible to a &amp;quot;general public&amp;quot;, that might change, as it certainly already takes the lead in terms of model flexibility and implementation performance.&lt;/p&gt;
&lt;p&gt;If you feel like &lt;strong&gt;discussing&lt;/strong&gt; what is written here, I've posted a link to this article on &lt;a class="reference external" href="http://www.reddit.com/r/LanguageTechnology/comments/2i7fe1/a_review_of_free_sparse_sequence_taggers_for_nlp/"&gt;Reddit&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="text mining"></category><category term="nlp"></category><category term="probabilistic programming"></category></entry><entry><title>An Introduction to Statistical Text Mining</title><link href="http://fnl.es/an-introduction-to-statistical-text-mining.html" rel="alternate"></link><published>2014-07-07T00:00:00+02:00</published><updated>2014-07-07T00:00:00+02:00</updated><author><name>Florian Leitner</name></author><id>tag:fnl.es,2014-07-07:/an-introduction-to-statistical-text-mining.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Update&lt;/em&gt; 2015-07-24: Please check out the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-from-bayes-rule-to-de"&gt;latest slides&lt;/a&gt; for this course, which now includes a quick introduction to dependency parsing, a more terse start, and several corrections and improvements all over.&lt;/p&gt;
&lt;p&gt;Last week we had a really great time at the first hands-on text mining workshop in the context of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Update&lt;/em&gt; 2015-07-24: Please check out the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-from-bayes-rule-to-de"&gt;latest slides&lt;/a&gt; for this course, which now includes a quick introduction to dependency parsing, a more terse start, and several corrections and improvements all over.&lt;/p&gt;
&lt;p&gt;Last week we had a really great time at the first hands-on text mining workshop in the context of the &lt;a class="reference external" href="http://www.dia.fi.upm.es/ASDM"&gt;Advanced Statistics and Data Mining&lt;/a&gt; Summer School. This one-week course is an introduction to text mining from the &amp;quot;bottom up&amp;quot; to a bunch of motivated summer students, with the practical parts based on Python and the &lt;a class="reference external" href="http://www.nltk.org/"&gt;NLTK&lt;/a&gt;. The presentation was part of the 9th iteration of the Summer School that is located in the sunniest capital of Europe: Madrid (well, &lt;em&gt;ex aequo&lt;/em&gt; with Athens, at least). In its context, I presented 15 hours worth of practical and theoretical background on machine learning for text mining to fourteen participants from all over the world. With this post I am sharing the slides and tutorial files (&lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt; Notebooks) with the world for free (see the Creative Commons &lt;a class="reference external" href="https://creativecommons.org/licenses/by-sa/3.0/"&gt;BY-SA&lt;/a&gt; licensing details). While much of the material should speak for itself, it might not &amp;quot;save&amp;quot; you from visiting a text mining class (maybe mine, next summer?).&lt;/p&gt;
&lt;div class="section" id="overview"&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;The theory mostly focuses on explaining the methods and statistics behind machine learning &lt;em&gt;for text mining&lt;/em&gt; - without requiring a particular background other than sharp high-school maths. The practicals on the other hand make use of Python, particularly online &lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt; Notebooks. Therefore, prior contact with Python would be useful to profit the most from the practicals, but it is not required to follow the course. IPython's Notebooks act very much like MATLAB Notebooks, but run online in any modern browser, are based on open source software that requires no license fee, and provide facilities to place documentation and mathematical formulas between the code snippets, evaluation results, and graphical plots. Therefore, these notebooks serve as take home material for the students to study and play around with, even well after the course. To illustrate the power of IPython, learning how to work in this environment immediately enables you to run distributed, high-performance computations &amp;quot;off the shelf&amp;quot; (e.g., via &lt;a class="reference external" href="http://star.mit.edu/cluster/"&gt;StarCluster&lt;/a&gt; on Amazon EC2 Clusters).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-1-introduction"&gt;
&lt;h2&gt;Day 1 - Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/statistical-text-mining-introduction-florian-leitner"&gt;Lecture 1&lt;/a&gt; starts with a light-weight introduction to text mining, natural language understanding and generation, and a statistics approach to artificial intelligence in general. It provides a taste of both what is to come in the course and what might be of interest to look into afterwards. It closes with a brief reprise of elementary Bayesian statistics and conditional probability, using the Monty Hall problem as a practical example of &lt;em&gt;Bayes' Rule&lt;/em&gt;. The &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/IPython%20Notebook%20Introduction.ipynb"&gt;practical #1&lt;/a&gt; introduces the use of &lt;a class="reference external" href="http://ipython.org/"&gt;IPython&lt;/a&gt; and its online Notebook, as well as some aspects of &lt;a class="reference external" href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt; (particularly, contrasting NumPy/SciPy to &lt;a class="reference external" href="http://www.r-project.org/"&gt;R&lt;/a&gt;). The most important aspects of the &lt;strong&gt;Natural Language ToolKit&lt;/strong&gt; (&lt;a class="reference external" href="http://www.nltk.org/"&gt;NLTK&lt;/a&gt;) Python library are presented during the &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Introduction%20to%20NumPy%20and%20NLTK.ipynb"&gt;second part&lt;/a&gt; of the practical; The NTLK is frequently used during the course. As an exercise to become familiar with Python and the NLTK, participants are encouraged to implement a function to let two NLTK chat-bots converse between each other.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-2-language-modeling"&gt;
&lt;h2&gt;Day 2 - Language Modeling&lt;/h2&gt;
&lt;p&gt;On the second day, &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-25-language-modeling-florian-leitner"&gt;lecture 2&lt;/a&gt; introduces the chain rule of conditional probability and builds on that to take you from the &lt;em&gt;Markov property&lt;/em&gt; over &lt;em&gt;language modeling&lt;/em&gt; to &lt;em&gt;smoothing techniques&lt;/em&gt;. In class, you learn how to operate on n-gram frequency tables and we work out the conditional probability distributions of bi- and trigram models. This should ensure  students obtain well-grounded foundations of statistical language processing. After a &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Building%20a%20Language%20Model%20in%207%20Steps.ipynb"&gt;quick overview&lt;/a&gt;, the accompanying &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Language%20Modelling%20with%20NLTK.ipynb"&gt;exercises #2&lt;/a&gt; demonstrate how to build language models with the NLTK and participants are tasked with generating their own models using advanced smoothing techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-3-string-processing"&gt;
&lt;h2&gt;Day 3 - String Processing&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-35-string-processing"&gt;third day lecture&lt;/a&gt; focuses on &lt;em&gt;string processing&lt;/em&gt; and the algorithmic aspects of text mining. The slides contain an overview of state machines, like regular expressions, PATRICIA tries, and &lt;em&gt;Minimal Acyclic Deterministic Finite [State] Automata&lt;/em&gt; (MADFA). Particular attention is payed to string metrics and similarity measures. The last theoretical part is a thorough introduction to &lt;em&gt;Locality Sensitive Hashing&lt;/em&gt; (LSH), and its use as a tool to efficiently cluster millions of documents or implement approximate string matching over very large-scale term dictionaries is discussed. During the &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Locality%20Sensitive%20Hashing.ipynb"&gt;3rd exercises&lt;/a&gt; students apply LSH to a toy problem: improving the speed of &lt;a class="reference external" href="http://norvig.com/spell-correct.html"&gt;Peter Norvig&lt;/a&gt;-style &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Spelling%20Correction%20using%20LSH.ipynb"&gt;spelling correction&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-4-text-classification"&gt;
&lt;h2&gt;Day 4 - Text Classification&lt;/h2&gt;
&lt;p&gt;After the more algorithmic topics, the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-45-text-classification"&gt;lecture of day four&lt;/a&gt; takes the participants into the realm of &amp;quot;real&amp;quot; machine learning. For starters, ways to calculate syntactic and semantic document similarity are presented, culminating in &lt;em&gt;Latent Semantic Analysis&lt;/em&gt;. The slides introduce the Naive Bayes classifier as an example to prepare the audience for the &lt;em&gt;Maximum Entropy classifier&lt;/em&gt; (Multinomial Logistic Regression), which forms the central topic of this session. To &amp;quot;cool down&amp;quot; a bit after a math-heavy section, &lt;em&gt;sentiment analysis&lt;/em&gt; is discussed as a popular domain for text classification. Finally, the important topic of evaluating set-based classifier performance via Accuracy, F-measure, and MCC Score are discussed. The &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Twitter%20Sentiment%20Analysis.ipynb"&gt;fourth practical&lt;/a&gt; then walks the participants through a Maximum Entropy sentiment classifier and encourages the audience to improve its performance by designing better features and making clever use of all the gained knowledge in the course so far.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="day-5-graphical-models"&gt;
&lt;h2&gt;Day 5 - Graphical Models&lt;/h2&gt;
&lt;p&gt;On the final day, &lt;em&gt;probabilistic graphical models&lt;/em&gt; dominate the lecture. With the &lt;a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-55-information-extraction"&gt;slides for day #5&lt;/a&gt;, first the main differences between Bayesian Networks and Markov Random Fields are introduced. Connected to yesterday's text classification topics, &lt;em&gt;Latent Dirichlet Allocation&lt;/em&gt; is presented as the first graphical model. Then, for the remainder of the talk, we move into &lt;em&gt;dynamic&lt;/em&gt; (temporal) models and their applications. We revisit the Markov chain and go from &lt;em&gt;Hidden Markov Models&lt;/em&gt; all the way to &lt;em&gt;Conditional Random Fields&lt;/em&gt;. To gear participants up for practical text mining, we look into actual applications of the presented models, for example, (semantic) &lt;em&gt;Word Representations&lt;/em&gt; or &lt;em&gt;Named Entity Recognition&lt;/em&gt;. The participants are introduced to the prospect of using the assigned labels for more advanced tasks. For example, &lt;em&gt;Relationship Extraction&lt;/em&gt; now becomes feasible by combining the shallow parse output as features for classifiers presented in other sessions of the Advanced Statistics and Data Mining Summer School. The rank-based performance measures of ROC and PR curves are discussed as way to evaluate the labeled results. During the &lt;a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Shallow%20Parsing%20with%20NLTK.ipynb"&gt;last practical&lt;/a&gt;, implementing a &lt;strong&gt;shallow parser&lt;/strong&gt; using NLTK and the &lt;a class="reference external" href="http://nlp.stanford.edu/software/tagger.shtml"&gt;Stanford Taggers&lt;/a&gt; is demonstrated in class. Overall, the practicals should provide the students with the basic software tools to embark on their own text mining adventures right after the course.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="remarks"&gt;
&lt;h2&gt;Remarks&lt;/h2&gt;
&lt;p&gt;You can &lt;a class="reference external" href="mailto:florian.leitner&amp;#64;gmail.com"&gt;contact me&lt;/a&gt; if you would like to discuss having this tutorial in the context of your own venue or have questions and comments about the slides' content. Other than that, I hope to have awakened your interest in statistical text mining and/or to have provided you with useful material, both as a student or teacher.&lt;/p&gt;
&lt;/div&gt;
</content><category term="text mining"></category><category term="python"></category><category term="nltk"></category></entry></feed>