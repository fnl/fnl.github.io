<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
				<meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>fnl.es &middot; articles in the "Misc" category</title>
        <link rel="shortcut icon" href="http://fnl.es/favicon.ico" />
<link href="http://fnl.es/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="fnl.es Atom Feed" />
        <link href="http://fnl.es/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="fnl.es RSS Feed" />

        <link rel="stylesheet" href="http://fnl.es/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://fnl.es/theme/css/pygments.css" type="text/css" />
        <!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=3728421; 
var sc_invisible=1; 
var sc_security="a5624e69"; 
</script>
<script type="text/javascript"
src="http://www.statcounter.com/counter/counter.js"></script>
        <!-- End of StatCounter Code for Default Guide -->
    </head>
    <body>
        <!-- Start of StatCounter Code for Default Guide -->
<noscript><div class="statcounter"><a title="hit counter"
href="http://statcounter.com/free-hit-counter/"
target="_blank"><img class="statcounter"
src="http://c.statcounter.com/3728421/0/a5624e69/1/"
alt="hit counter"></a></div></noscript>
        <!-- End of StatCounter Code for Default Guide -->
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://fnl.es/category/misc.html">Misc</a></li>
                <li><a class="flagred" href="http://fnl.es">Home</a></li>
<li><a class="flagyellow" href="http://fnl.es/pages/about.html">About</a></li>
<li><a class="flagyellow" href="http://fnl.es/pages/projects.html">Projects</a></li>
<li><a class="flagred" href="http://fnl.es/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://fnl.es">fnl.es</a></h1>
                <h2><a href = "mailto:flo@fnl.es" id="webmaster">fnl</a> en España</h2>            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Jan 2015</h4>

								<div class="mobile-hide">
									<a class="twitter-timeline" width="180px" data-dnt="true" data-chrome="noheader nofooter transparent" href="https://twitter.com/flowing" data-widget-id="393739277646852096"></a>
									<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
								</div>
                <div class="post">
<h2 class="title">
                        <a href="http://fnl.es/segtok-a-segmentation-and-tokenization-library.html" rel="bookmark" title="Permanent Link to &quot;segtok - a segmentation and tokenization library&quot;">segtok - a segmentation and tokenization library</a>
                    </h2>

                    <p><strong>tl;dr</strong> Surprisingly, it is hard to find a good, UNIX-y tool for sentence segmentation and linguistic word tokenization with all the bells and whistles needed to work with European languages. Here, I present <a class="reference external" href="https://pypi.python.org/pypi/segtok">segtok</a>, a Python package and command-line tool to remedy this shortcoming.</p>
<div class="section" id="what-is-sentence-segmentation-and-tokenization">
<h2>What is sentence segmentation and tokenization?</h2>
<p>Nearly any text mining and/or linguistic analysis starts with a sentence and word segmentation step. That is, determining all the individual spans in a piece of text that constitute its sentences or words.
Identifying sentences is important because they form logical units of thought and represent the borders of many grammatical effects.
All our communication - statements, questions, and commands - are expressed by sentences or at least a meaningful sentence-like fragment, i.e., a phrase.
Words on the other hand are the atomic units that form the sentences, and ultimately, our language.
While words are built up from a sequence of symbols in most languages, these symbols have no semantics of their own (except for any single-symbol words in that language, naturally).
Therefore, nearly any meaningful text processing task will require the segmentation of the sequence of symbols (characters in computer lingo) into sentences and words.
These words are, at least in the context of computational text processing, often called <strong>tokens</strong>.
Beyond the actual words consisting of letters, a token includes atomic units consisting of other symbols.
For example, the sentence terminals (., !, and ? are three such tokens), a currency symbol, or even chains of symbols (for example, the ellipsis: …).
By following through with this terminology, the process of segmenting text into these atomic units is commonly called <em>tokenization</em> and a computer subroutine doing this segmentation is known as a <em>tokenizer</em>.</p>
</div>
<div class="section" id="sentence-segmentation-and-tokenization-is-hard">
<h2>Sentence segmentation and tokenization is hard</h2>
<p>While this segmentation step might initially sound like a rather trivial problem, it turns out the rabbit hole is deep and no perfect solution has been found to date.
Furthermore, the problem is made harder by the different symbols and their usage in distinct languages.
For example, just finding word boundaries in Chinese is non-trivial, because there is not boundary marker (unlike the whitespace used by Indo-European languages).
And when looking into technical documents, the problem can easily grow even more out of hand.
Names of chemical compounds, web addresses, mathematical expressions, etc. are all complicating the way one would normally define a set of word boundary detection rules.
Another source of problems are texts from public internet fora, such as Twitter.
Words are not spelled as expected, the spaces might be missing, and emoticons and other ASCII-art can make defining the correct tokenization strategy a rather difficult endeavor.
Similarly, the sentence terminal marker in many Indo-European languages is the full-stop dot – which coincidentally is also used as the abbreviation marker in those languages.
For example, detecting the two sentences in in the following text fragment is not that trivial, at least for a computer program.</p>
<blockquote>
Hello, Mr. Man. He smiled!</blockquote>
<p>If the text fragments are large or the span contains many dots, even humans will start to make many errors when trying to identify all the sentence boundaries.
Certain proper nouns (gene names, or “amnesty international”, for example) might demand that the sentence begins with a lower case letter instead of the expected upper-case.
A simple typo might have been the cause for a sentence starting with a lower-case letter, too.
Again in public internet fora, users sometimes resort to using only lower- or upper-case for their  messages or write in an orthographically invalid mix of letter casing.</p>
</div>
<div class="section" id="introducing-segtok-a-python-library-for-these-two-issues">
<h2>Introducing segtok – a Python library for these two issues</h2>
<div class="highlight"><pre>pip3 install segtok
</pre></div>
<p>Despite the ubiquitous need for this functionality in nearly any kind of text processing, there are not very many stand-alone modules around for sentence segmentation and tokenization.
And there will be even fewer programs around if you require that they are able to handle the subtleties of orthographically valid symbol sequences.
This means that you either have to utilize functionality provided within large frameworks (for example, <a class="reference external" href="http://www.nltk.org">NLTK</a> in Python or <a class="reference external" href="http://alias-i.com/lingpipe/">LingPipe</a> in Java), or you have to write your own code.
Both approaches have their downsides, but in particular, using large frameworks violates a Unix philosophy I have come to love more than anything in my day-to-day IT life:</p>
<blockquote>
“A  program should do one thing and it should do it well.
Programs should handle <em>text streams</em>, because that is a universal interface.”
(Doug McIlroy)</blockquote>
<p>(Yes, I know, it seems I am violating the “one thing” rule because segtok does two things.
But as you will see, the library comes with two independent scripts and APIs for each step.)
This approach to software allows you to easily use other programs for each element of your text processing tool-chain and allows you to quickly swap those elements.
If you use large frameworks, you are easily constrained to it or have to accept that the one thing you want it to do isn't done efficiently.
And the more you use one large framework only, the less attractive it is to switch your tooling.
This has a direct, detrimental effect on your ability to experiment and adapt to new tools.</p>
<p>Due to the many different ways this problem can be solved and the inherent complexity if considering all languages, the solution presented here is confined to processing somewhat regular Germanic (e.g., English) and Romance (e.g., Spanish) languages with a strong focus on those two and German, which all use Latin letters and standard symbols (like .?!”'([{, etc.).
The tool being presented here – <a class="reference external" href="https://pypi.python.org/pypi/segtok">segtok</a> - is made to cope with text with the following properties:</p>
<ul class="simple">
<li>Indo-European languages (i.e., Unicode), preferentially Germanic and Romance ones.</li>
<li>Texts that follow a mostly regular writing style - in particular, segtok is not tuned for Twitter's highly particular orthography.</li>
<li>It can handle technical texts (e.g. chemicals) and internet URIs (IP addresses, URLs and e-mail addresses).</li>
<li>The tool is able to handle cases of sentences starting with a lower-case letter and sentences enclosed by parenthesis or quotation marks.</li>
<li>Is able to handle some cases with heavy abbreviation use (e.g., academic citations).</li>
<li>It treats all <em>Unicode</em> dashes (there are quite a few!) “The Right Way” - a functionality surprisingly absent from most tools.</li>
</ul>
<p>Overall, the two scripts that come with segtok have a very simple text-line-based interface that works well with Unix pipes.
One script, <cite>segmenter</cite>, segments sentences in (plain) text files into one sentence per line.
The other, <cite>tokenizer</cite>, splits tokens on single lines (usually, the sentences from the <cite>segmenter</cite>) by adding whitespaces where necessary.
On the other hand, if you are a Python developer, you can use the API provided by this library to incorporate the functionality in your own programs.
Segtok is designed to handle texts with characters from the entire Unicode space, not just ASCII or Latin-1 (ISO-8859-1).</p>
</div>
<div class="section" id="sentence-segmentation-with-segtok">
<h2>Sentence segmentation with segtok</h2>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">segtok.segmenter</span> <span class="kn">import</span> <span class="n">split_single</span><span class="p">,</span> <span class="n">split_multi</span>
</pre></div>
<p>On the sentence level, segtok can detect sentences that are contained inside brackets or quotation marks and maintains those brackets as part of the sentence;
For example:</p>
<ul class="simple">
<li>(A sentence in parenthesis!)</li>
<li>Or a sentence with &quot;a quote!&quot;</li>
<li>'How about handling single quotes?'</li>
<li>[Square brackets are fine, too.]</li>
</ul>
<p>The segmenter is constrained to only segment on single lines ( <cite>split_single</cite> - sentences are not allowed to cross line boundaries) or to consecutive lines ( <cite>split_multi</cite> - splitting is allowed across newlines inside “paragraphs” separated by two or more newlines).
It gracefully handles enumerations, dots, multiple terminals, ellipsis, and similar issues:</p>
<ul class="simple">
<li><ol class="first upperalpha">
<li>The first assumption.</li>
</ol>
</li>
<li><ol class="first arabic" start="2">
<li>And that is two.</li>
</ol>
</li>
<li><ol class="first lowerroman" start="3">
<li>Third things here.</li>
</ol>
</li>
<li>What the heck??!?!</li>
<li>A terminal ellipsis...</li>
</ul>
<p>In essence, a valid sentence terminal must be represented by one of the allowed Unicode markers.
That is, the many Unicode variants of ., ?, and !, and the ideographic full-stop: “。”.
Therefore, <em>this library cannot guess a sentence boundary if the marker is absent</em>!
After the marker, up to one quotation mark and one bracket may be present.
Finally, the marker must be separated from the following non-space symbol by at least one whitespace character or a newline.</p>
<p>This requires that the sentence boundaries do obey some limited amount of regularity.
But at the same time, the pesky requirement that a marker is followed by upper-case letters is absent from this strategy.
In addition, this immediately discards “inner” abbreviation markers from the list of candidates (such as in “U.S.A.”).
On the other hand, any markers that do not follow this “minimal” pattern will always result in false negatives (i.e., not be split).
While missing markers do occur, they are relatively infrequent in orthographically correct texts.</p>
<p>After these <em>potential</em> markers have been established, the library goes back and looks at the surrounding text to determine if the symbol is not a boundary marker.
This step recuperates initials (“A. Name”), species names (“S. pombe”) and abbreviations inside brackets, which are common with citations (“[A. Name, B. Other. Title of the work. Proc. Natl. Acad. Sci. 2010]”).
Obvious and/or common abbreviations (in English, Spanish, and German, so far) are recovered, too.
In other words, while coming at a computational cost, this second check allows segtok to keep the number of false positive splits to an acceptable low.</p>
<p>There are several other enhancements to the segmenter (e.g., checking for the presence of lower-case words that are unlikely start a sentence) that can be studied in the source code and unit tests.</p>
</div>
<div class="section" id="segtok-s-tokenization-strategy">
<h2>Segtok's tokenization strategy</h2>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">segtok.tokenizer</span> <span class="kn">import</span> <span class="n">symbol_tokenizer</span><span class="p">,</span> <span class="n">word_tokenizer</span><span class="p">,</span> <span class="n">web_tokenizer</span>
<span class="kn">from</span> <span class="nn">segtok.tokenizer</span> <span class="kn">import</span> <span class="n">split_possessive_markers</span><span class="p">,</span> <span class="n">split_contractions</span>
</pre></div>
<p>The tokenization approach is built up similarly.
First, a maximal token split is made, and then several functions wrap this basic approach, encapsulating successively more complex rules that join tokens back together.
The basic, maximum split rule is to segment everything that is separated by spaces and then within the remaining non-space spans, split anything that is alphanumeric from any other symbols:</p>
<p>a123, an alpha/beta stabilizer... → a123  ,  an  alpha  /  beta  stabilizer  ...</p>
<p>This functionality is provided by the <cite>symbol_tokenizer</cite> .
Next, the symbols are further analyzed to detect if they form part of the surrounding words.
If so, they are again merged together with their alphanumeric parts.</p>
<ul class="simple">
<li>The abbreviation marker is attached back on to the proceeding word (“Mr.”).</li>
<li>Words with internal dots, dashes, apostrophes, and commas are joined (“192.168.1.0”, “Abel-Ryan's”, “a,b-symmetry”).</li>
<li>The spaces inside a word-hyphen-spaces-word sequences are dropped.</li>
<li>Superscript digits are attached to a proceeding alphanumeric token if present.</li>
</ul>
<p>This set of functionality is provided by the <cite>word_tokenizer</cite>.
Finally, if desired, a Web-mode function will further ensure that valid e-mail addresses and URLs are always maintained as single tokens (<cite>web_tokenizer</cite>).
This ensures that while a decent amount of splitting is made, over-splitting is avoided.
Particularly, when processing biomedical documents, Web content, or patents, too much tokenization might have quite a significant negative impact on any subsequent, more advanced processing techniques.
After this tokenization step, the API provides two functions to optionally split off English possessive markers (“Fred's”, “Argus'”) and even contractions (“isn't” → “is n't”, “he'll” → “he 'll”, “I've” → “I 've”, etc.), which can be useful for any downstream linguistic parsing (<cite>split_possessive_markers</cite> and <cite>split_contractions</cite>).</p>
</div>
<div class="section" id="feedback-and-conclusions">
<h2>Feedback and Conclusions</h2>
<p>All this functionality and the API itself are briefly documented on <a class="reference external" href="https://pypi.python.org/pypi/segtok">segtok</a>'s &quot;homepage&quot;.
As there is not very much functionality around, I hope that between this guide and the overview there the library should be fairly easy to use.
Furthermore, in command-line mode, using the <cite>--help</cite> option will explain you all functionality provided by the two scripts this package installs.</p>
<p>If you would be looking for new features, you are welcome to extend the library or request a new feature on the tool's GitHub <a class="reference external" href="https://github.com/fnl/segtok/issues">Issues</a> page.
In addition, if you use this library and run into any problems, I would be glad to receive bug reports there, too.</p>
<p>Overall, I have attempted to keep the strategy used by segtok as slim as possible.
So if you are using any heavy language processing or sequence analysis tools after segtok, it should have no impact on your throughput at all.</p>
<p>I have compared this library with several other approaches in the wild, and for regular texts, my experience is that it works substantially superior in both segmentation capabilities and runtime performance to all alternatives I have looked into, at least.
As I do not wish to bash any existing tool, I will only name one sentence segmentation approach I like very much: Punkt Tokenizer by Kiss and Strunk, 2006.
PT is a unsupervised, statistical approach to segmentation that “learns” whether to split sentences at sentence terminal markers.
While quite impressive and very versatile due to its unsupervised nature, I can state clearly that segtok's segmenter works substantially better on modern Germanic and Romance texts that (mostly) follow a proper orthography.
Unsurprisingly, segtok's sentence segmenter is substantially faster than a comparable Python <a class="reference external" href="http://www.nltk.org/_modules/nltk/tokenize/punkt.html">implementation</a> of the Punkt Tokenizer by <a class="reference external" href="http://www.nltk.org">NLTK</a>.</p>
</div>


                    <div class="clear"></div>
                    <div class="info">
                        <div class="tags">                            <a href="http://fnl.es/tag/text-mining.html">text mining</a>                            <a href="http://fnl.es/tag/nlp.html">nlp</a>                            <a href="http://fnl.es/tag/python.html">Python</a>                        </div>

                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div class="pages">
                    <a href="http://fnl.es/category/misc2.html" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 1 of 25</span>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    <a class="atom" href="http://fnl.es/feeds/all.atom.xml">RSS Feed</a>
                    </p>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    </body>
</html>