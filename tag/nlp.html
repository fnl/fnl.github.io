<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
				<meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>fnl.es &middot; articles tagged "nlp"</title>
        <link rel="shortcut icon" href="http://fnl.es/favicon.ico" />
<link href="http://fnl.es/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="fnl.es Atom Feed" />
        <link href="http://fnl.es/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="fnl.es RSS Feed" />

        <link rel="stylesheet" href="http://fnl.es/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://fnl.es/theme/css/pygments.css" type="text/css" />
        <!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=3728421; 
var sc_invisible=1; 
var sc_security="a5624e69"; 
</script>
<script type="text/javascript"
src="http://www.statcounter.com/counter/counter.js"></script>
        <!-- End of StatCounter Code for Default Guide -->
    </head>
    <body>
        <!-- Start of StatCounter Code for Default Guide -->
<noscript><div class="statcounter"><a title="hit counter"
href="http://statcounter.com/free-hit-counter/"
target="_blank"><img class="statcounter"
src="http://c.statcounter.com/3728421/0/a5624e69/1/"
alt="hit counter"></a></div></noscript>
        <!-- End of StatCounter Code for Default Guide -->
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://fnl.es/tag/nlp.html">nlp</a></li>
                <li><a class="flagred" href="http://fnl.es">Home</a></li>
<li><a class="flagyellow" href="http://fnl.es/pages/about.html">About</a></li>
<li><a class="flagyellow" href="http://fnl.es/pages/projects.html">Projects</a></li>
<li><a class="flagred" href="http://fnl.es/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://fnl.es">fnl.es</a></h1>
                <h2><a href = "mailto:flo@fnl.es" id="webmaster">fnl</a> en España</h2>            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Oct 2014</h4>

								<div class="mobile-hide">
									<a class="twitter-timeline" width="180px" data-dnt="true" data-chrome="noheader nofooter transparent" href="https://twitter.com/flowing" data-widget-id="393739277646852096"></a>
									<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
								</div>
                <div class="post">
<h2 class="title">
                        <a href="http://fnl.es/a-review-of-sparse-sequence-taggers.html" rel="bookmark" title="Permanent Link to &quot;A review of sparse sequence taggers&quot;">A review of sparse sequence taggers</a>
                    </h2>

                    <div class="section" id="introduction">
<h2>Introduction</h2>
<p><strong>tl;dr</strong>
Right now, use <a class="reference external" href="http://wapiti.limsi.fr/">Wapiti</a> <em>unless</em> you want to go beyond first-order and/or linear models, want the best possible performance, or are a Scala programmer, in which case you would be best advised to choose <a class="reference external" href="http://factorie.cs.umass.edu/">Factorie</a>.
OK, so that's that for a stressed out generation;
Read on if you want to know why I recommend those two tools.</p>
<p><strong>Overview:</strong>
The goal of this review is to identify the &quot;best&quot; generic CRF- or MEMM-based sequence tagger software with a free (MIT/BSD-like) license.
We will only take discriminative models into account, so if your beef are generative models and/or non-sparse data (e.g., HMMs), you have come to the wrong place.
This article will look into their abilities to define and generate features, training times, tagging throughput, and tagging performance by way of working through a common sequence labeling problem: tagging the parts-of-speech of natural language.
While PoS tagging can be considered a &quot;solved&quot; problem, PoS tagging performance differences are still a source of <a class="reference external" href="http://aclweb.org/aclwiki/index.php?title=POS_Tagging_%28State_of_the_art%29">academic</a> controversy and therefore an ideal testing ground.</p>
<p>Nearly any interesting natural language processing (NLP) task starts with word <a class="reference external" href="http://text-processing.com/demo/tag/">tagging</a>:
That is, resolving each word's grammatical sense - it's <em>part-of-speech</em> (<strong>PoS</strong>), the phrases they group into, and the semantic meaning the words carry.
PoS refers to a word's morphology, e.g., if it is used a noun or an adjective, or the inflection of the verb.
Words of the same morphology can often be grouped (we say, <strong>chunked</strong>) into phrases, such as &quot;a noun phrase&quot;.
As for the semantic meaning, text miners are usually interested in identifying the relevant entity class/type (such as person, location, date/time, ...) the word refers to.
Furthermore, in NLP, words are commonly called <strong>tokens</strong> to use a name that also covers symbols and numbers, including dots, brackets, or commas.
These tokens form the atomic sequence units for many statistical NLP methods.</p>
<p>Similarly, in bioinformatics, you might want to identify properties of biological sequences, e.g., DNA binding sites or predict locations of post-translational modifications in proteins.
In general, any sequence tagger could be used to classify elements in any kind of sequence you can split into discrete units.
Therefore, in bioinformatics, those units might be nucleic acids (DNA bases) or amino acids (proteins).
However, the devil lays in the details:
The implementations I am interested in here are for &quot;information-sparse sequences&quot;, such as text.
The difference is the element (and resulting feature) sparsity: while there are easily 20,000 different tokens contained in any average text collection (such as a book), there are only four DNA bases and twenty-something amino acids (depending on the species).
All amino acids and bases will be - compared to text tokens, at least - frequently used in their respective sequences, with the obviously lowest sparsity for the four <em>standard</em> DNA bases (long story hidden here...).
So to define some scope of this review, I am interested in learning patterns from extremely sparse data;
If your data is &quot;dense&quot;, you might be better off looking into more general algorithms, such as hidden Markov models (HMMs).</p>
<p>You can play around with a few example NLP taggers by following the <a class="reference external" href="http://text-processing.com/demo/tag/">tagging</a> link, and you will see that depending on the system and its training data, results can vary widely.
This is due to implementation details, graphical model capabilities, and the sequence features used by the particular model instance.
The common approach to this kind of problem is learning a <a class="reference external" href="https://en.wikipedia.org/wiki/Discriminative_model">discriminative</a>, dynamic graphical model; commonly, a maximum entropy (MaxEnt, aka. logistic regression) based decision function worked into a reverse <a class="reference external" href="http://en.wikipedia.org/wiki/Markov_model">Markov model</a> or into the more complex Markov random field, in this particular case called a <em>Conditional Random Field</em> (<strong>CRF</strong>).
In the former <em>MaxEnt Markov model</em> (<strong>MEMM</strong>) scenario, you are only allowed to use the current state (element in your sequence, including any meta-data assigned to that element) and the last tag(s) to predict the current tag (aka. label).
The latter CRF model allows you to integrate features not just from the current state, but from any state in the sequence being labeled.
So with CRFs you can even use states from the &quot;future&quot; (i.e., elements later in the sequence that the one currently being tagged) to predict the current label (aka. tag).</p>
<p>While outside the scope of this article, in case you are now asking yourself &quot;Why do I then even want a MEMM instead of a CRF?&quot;:
Defenders of MEMMs claim that &quot;their&quot; model has an edge because it does not tend to overfit the data (we say, it has a weaker &quot;domain adaptation&quot;) as easily as a CRF can (due to the way features can be generated from any part of the sequence) and therefore produces better tagging results on input that is not similar to (of a &quot;different domain&quot; than) the training data.
Second, due to its feature selection limitation, training of a MEMM tends to be faster than training a CRF with &quot;long-range&quot; features from distant positions in the sequence.</p>
<p>Both regular Markov models and random fields use the notion of <em>Markov order</em>.
That is, the number of former (already assigned) labels in a linear chain (sequence) that may be used to calculate the probabilities of each possible label on the current state.
To be more precise, it is the <em>transition probability</em> from one (first order) or more (second, third, ... order) former labels to the label on the current state that is the statistic being modeled.
This &quot;<tt class="docutils literal">n</tt>th-order Markov&quot; limit defines how many prior tags the model will consider when tagging the current state:
For first order Markov chains you only can make use of the last tag, for second order models you get to use the last two tags, and so forth.
Due to the expense of going beyond first-order models, all but one tool we will be looking at do not support more than first-order models:
Linear chain models scale exponentially in the Markov order + 1, meaning that a second order model already has &quot;number of labels&quot;-cubed possible label transitions.
Going beyond second-order Markov models is only desirable if the number of labels and (as we will see) sometimes even states is small (i.e., &quot;dense data&quot;).
In language processing, these states normally are all the unique, observed tokens, also known as the <em>vocabulary</em>.</p>
<p>A quick, shameless self-plug: if you are not familiar with these concepts, have a look at my <a class="reference external" href="http://fnl.es/an-introduction-to-statistical-text-mining.html">introduction to text mining</a> slides - or come visit my class in the context of the Madrid summer school on <a class="reference external" href="http://www.dia.fi.upm.es/?q=es/ASDM">Advanced Statistics and Data Mining</a> next summer (beginning of July)! The lecture will take you from basic Bayesian statistics all the way to the dynamic, graphical models being discussed here.</p>
</div>
<div class="section" id="sequence-tagger-selection">
<h2>Sequence Tagger Selection</h2>
<p>Recently, I have become anxious to once and for all resolve my doubts about the &quot;best&quot; sparse sequence tagger in terms of ease of use (documentation/UI), feature modeling capabilities, training times, tagging throughput (tokens/second) and the resulting accuracy.
All tools use the same optimization procedures for the learning process, that is a <a class="reference external" href="http://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> optimizer and a few light-weight gradient descent implementations as alternatives.
However, implementation details, the graphical model abilities, the features the system can work with, the facilities it provides to generate them, the system's throughput, the provided documentation, and its availability (both in open source and free software terms) varies greatly between libraries.
Available software for this task that I considered were <a class="reference external" href="http://crfpp.sourceforge.net/">CRF++</a> (Kudo), <a class="reference external" href="http://www.chokkan.org/software/crfsuite/">CRF Suite</a> (Okazaki), <a class="reference external" href="http://factorie.cs.umass.edu/">Factorie</a> (McCallum &amp;al.), <a class="reference external" href="http://flexcrfs.sourceforge.net/">FlexCRFs</a> (Phan, Nguyen &amp; Nguyen), <a class="reference external" href="http://alias-i.com/lingpipe/index.html">LingPipe</a> (Carpenter &amp;al.), <a class="reference external" href="http://mallet.cs.umass.edu/">MALLET</a> (McCallum &amp;al.), <a class="reference external" href="http://www.umiacs.umd.edu/~hal/megam/">MEGAM</a> (Daume III), [Apache] <a class="reference external" href="http://opennlp.apache.org/">OpenNLP</a> (Kottmann &amp;al.), <a class="reference external" href="http://nlp.stanford.edu/software/tagger.shtml">Stanford Tagger</a> (Manning, Jurafsky &amp; Liang), <a class="reference external" href="http://www.lsi.upc.edu/~nlp/SVMTool/">SVM Tool</a> (Giménez &amp; Marquez), <a class="reference external" href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/">TreeTagger</a> (Schmidt), and <a class="reference external" href="http://wapiti.limsi.fr/">Wapiti</a> (Lavergne).
There are more options <a class="reference external" href="http://en.wikipedia.org/wiki/Conditional_random_field#Software">around</a>, particular in C# and for the .Net platform, but as I do not have the money to pay for the Windows tax, I did not consider them.
If you know of a relevant, <em>generic</em> sparse sequence tagger implementation I missed (see my filtering criteria below), please <a class="reference external" href="http://fnl.es/pages/about.html">contact me</a>.</p>
<p>I immediately discarded the Stanford Tagger and the SVM Tool, because they are both orders of magnitude <a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf">slower</a> than most other tools considered (and the same goes for MALLET, <a class="reference external" href="http://www.chokkan.org/software/crfsuite/benchmark.html">too</a>).
It is worth mentioning that the Stanford Tagger was one of the earliest tools with the software made available for research and a very high accuracy, and as such usually serves as the performance &quot;baseline&quot; for newcomers.
Second, CRF Suite claims to be the fastest first order CRF around and <a class="reference external" href="http://www.chokkan.org/software/crfsuite/benchmark.html">demonstrates</a> that CRF++ is significantly slower, which lead me to discard the latter.
That same benchmark claims that CRF Suite is faster than Wapiti, but not only has Lavergne developed several newer versions since then, the difference is far less pronounced, so that tool was not out of the race for me.
Being a free software advocate in the sense of all its aspects - cost, freedom of usage, modifiability, and open source - I feel very uncomfortable about using software with a license that tries to restrict my freedom, including its commercial application.
Therefore, I discarded FlexCRFs, LingPipe, MEGAM, and TreeTagger from the list because of their non-free nature (only being &quot;free for research&quot;, or GPL'ed).
While the GPL is not strictly out of my scope, it creates too many headaches for too many use-cases because it still poses usage restrictions (that I nonetheless support as a necessary evil given the overall copyright SNAFU).
Moreover, excluding the GPL only affects FlexCRFs, which anyways is very similar to CRF++ or CRF Suite.
Two of the already discarded tools would also not make it across this &quot;free software barrier&quot;, by the way (Stanford Tagger and SVM Tool).</p>
<p>So this left me with CRF Suite, Factorie, OpenNLP, and Wapiti to compare against each other.
Given these harsh pre-filtering criteria, to be honest, I was astonished that I was left with not just one, but four viable and completely free &quot;tools of the trade&quot;!</p>
</div>
<div class="section" id="implementation-considerations">
<h2>Implementation Considerations</h2>
<p>CRF Suite and Wapiti are both written in C, while Factorie is being coded in Scala, and OpenNLP is based on Java.
So this makes for yet another classical &quot;binary, platform-specific code versus the Java Virtual Machine&quot; comparison!
(Spoiler alert: it does not matter - as you should know already...)
But there a real, noteworthy differences between the taggers; starting with the implemented graphical models and optimization procedures:
Factorie's PoS tagger implementation only makes use of a forward learning procedure, while the other three use the more common (and more expensive) <a class="reference external" href="http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm">forward-backward optimization</a> approach during training.
So this difference makes up for an interesting test: will forward learning alone be good enough in terms of accuracy, and if so, how much faster will training be?
(You could also read a <a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf">paper</a> about this...)
Furthermore, Factorie is a library that allows you to design <em>any kind</em> of graphical models from basic factor classes (let that sink in if you know what I mean...), so it actually can represent any model you want (including non-linear models).
For the PoS tagging, Factorie uses this forward-only learning approach to maximize a first-order linear-chain CRF.
Next up, Wapiti allows you to choose between a (non-dynamic, pure) MaxEnt model, a MEMM or a CRF model.
Similar to the above <a class="reference external" href="http://aclweb.org/anthology//P/P12/P12-2071.pdf">paper</a> linked to, Wapiti can even do dynamic model selection, falling back on simpler models where feasible in the sequence
(Note that Factorie's PoS tagger does not use dynamic model selection.)
Finally, OpenNLP only provides a MEMM implementation, while CRF Suite only provides a CRF.
These implementation details alone might be enough to make your decision:
If you want more than a first-order linear-chain model (say, second-order, or a non-linear graph), your only choice is Factorie.</p>
<p>One main question of my review was:
Is there any perceivable difference in tagging throughput with any of those four tools given the model and feature selection differences, and how does this influence tagging accuracy?
Resolving this question and regarding each toolkit's implementation details should help you with your choice of the &quot;right&quot; system for your own sequence labeling work.
First, a quick look at the code, implementation, the documentation, and each tool's multi-processor capabilities.</p>
</div>
<div class="section" id="software-state-and-documentation">
<h2>Software state and documentation</h2>
<p>Two remarkable things about <strong>Wapiti</strong> are how simple and lean the interface is, and its capability of running in multi-threaded mode.
While code is the typical, long spaghetti-code of C, it is clean and well documented.
(Or, in other words, the &quot;classic&quot; C code SNAFU...)
The only main downside is that the documentation is a bit sparse; Everything is in there, but they could have done a bit better detailing some of the capabilities, and/or providing examples.
I had to figure out myself that you always should use the <tt class="docutils literal"><span class="pre">-c</span> <span class="pre">-s</span></tt> switches when doing (feature-sparse) text labeling and it took me some time to understand how to do feature extraction (&quot;patterns&quot;).
The Wapiti authors do not provide a default set of feature pattern templates, only a few pre-trained PoS models for English, German, and Arabic newswire text.</p>
<p>Similarly, <strong>Factorie</strong> is able to run in multi-threaded mode, however, the code base is a bit of a mess and it sometimes honestly feels like the code could have just as well been written in plain, old imperative Java:
Despite being coded in Scala, the code-base shows to little use of functional programming paradigms and contains many long, imperative code blocks.
However, given that it is heavily used by McCallum's group and several other groups, too, being applied to large corpora, I think it is safe to assume that despite any perceived lesser code quality, the overall functionality should be unaffected.
One should also note that this judgment is purely subjective, so you might be better of deciding on this issue for yourself.
To use Factorie, I often have to refer back to the code-base, because few of the specifics of Factorie are entirely documented for now.
This means, to use Factorie, you should better know some Scala, as it might be tough having to go through the code otherwise.
Finally, the documentation certainly assumes you are an expert for graphical models with plenty of background knowledge in that domain.
Given its state and direction, in comparison to the other tools here, it is probably safe to judge that this library is targeted at the probabilistic programming crack with a background on graphical models, not someone looking for a quick and dirty sequence tagging solution.</p>
<p><strong>CRF Suite</strong> comes with a nice interface, and although it does not support multi-threading, the C code is well written and very clear.
The functions are short and precise, so it is a nice example how C code can look if you put some effort into it.
The only code-wise downside I detected was the accompanying Python code for preparing and pruning data and benchmarking.
It is clearly written by a C expert with little knowledge of idiomatic Python, no offense intended (see the performance issues for feature extraction below).
However, I think this is a minor issue given the good documentation and high-quality C code, which in the end is the part that matters.
The worst issue with CRF Suite, however, seems to be that the original author has stopped maintaining the code.
The repository on GitHub has a handful of very good pull requests from serious developers that fix things like a minor memory leak and two or three other issues, but the author has never accepted the requests.
Neither have there been any updates to the library.
To me this means that CRF Suite development seems dead and it would have to be significantly better than the other tools here to make it worth using it.</p>
<p>Finally, <strong>OpenNLP</strong> has the expected high quality code found in Apache projects, and it is well documented, too.
The only real downsides are that there is no built-in parallel processing support and that it only comes with a MaxEnt Markov model.
This absence of multi-threaded support in the CRF Suite and OpenNLP libraries might seem deal-breakers, but I would not make my decision based on that issue (for now...)</p>
<p>Unique capabilities of Wapiti are its built-in template-based feature extraction mechanism and its ability to quickly choose either CRF or MEMM as the target model (more on this below).
A unique capability of Factorie is that it provides you with the necessary classes (traits) to implement your own graphical models of any Markov order, both linear and non-linear.
However, admittedly, both Wapiti and Factorie are behind CRF Suite and OpenNLP in terms of documentation and source code readability.</p>
</div>
<div class="section" id="feature-modeling">
<h2>Feature Modeling</h2>
<p>This leads to the next important consideration: modeling features, for example, via templates (or &quot;patterns&quot;) that lead to features that are subsequently wrapped as binary indicator functions.
For example, an indicator function for assigning the PoS tag &quot;VBZ&quot; might be triggered when observing the bigram &quot;I went&quot; and having already assigned the PoS label &quot;PRP&quot; to the token &quot;I&quot;.
This example is called a &quot;combined (bigram) transition/label and state/token feature&quot;
(And would be minimally encoded as <tt class="docutils literal"><span class="pre">b:%x[-1,0]/%x[0,0]</span></tt> in Wapiti's pattern template language.)
It is a rather &quot;expensive&quot; feature template, as it can easily lead to millions of individual indicator functions:
The number of indicator functions created from this template will be the squared number of PoS tags (label transitions or &quot;bigrams&quot;) times the number of unique token bigrams in the whole training data.
In general, for any template - even a constant one - there will be at least as many indicator functions as there are different tags.</p>
<p>Except for Wapiti, all taggers come with a pre-defined set of feature templates for common NLP tasks.
Depending on your point-of-view, this might be good or annoying, particularly for NER tagging.
For NER tagging, your entities might have unique morphological or orthographic properties;
For example, gene names might be used not just as nouns, but as adjectives, too (as in &quot;p53-activated DNA repair&quot;) and contain Roman or Arab numbers, Greek symbols, dashes and a few other orthographic surprises.
In addition, a entity tag might depend on &quot;future&quot; context, such as the head token of a noun phrase (e.g., &quot;gene&quot; in &quot;the ABC transporter gene&quot; when looking at &quot;ABC&quot;).</p>
<p>The predefined <strong>Factorie</strong> features are, however, pretty good - so rich indeed, that they are more complete than any other set of features I used or provided myself in the experiments here (see <tt class="docutils literal"><span class="pre">FowardPosTagger.scala::features</span></tt> for PoS tagging).
That means training could be slow for Factorie, because it needs to optimize over a much larger indicator (feature) function space (turns out it is not, however - see below).
As with all systems except for Wapiti, this means you need to do some coding of your own to adapt the features, while it probably is not necessary for PoS tagging and phrase chunking tasks.
The only beef I experienced with Factorie was that it does not explain input errors and problems; It just throws some obscure exception at you.
This means you will have to figure out what went wrong when you get errors on your own.
The other issue is figuring out how to define your own NER tagger, as the documentation so far only covers PoS taggers.
More generally speaking, the documentation on generating features for your taggers is rather thin (see User Guide - Learning).</p>
<p><strong>OpenNLP</strong>, too, comes with a pre-selected list of feature templates for standard NLP tasks.
To change this list, you either have to write your own Java code or, at least for NER tagging, the documentation states that you can conjure up XML configuration files to extract different features.
As I am allergic to any use of XML other than its intended use-case - providing structure to unstructured data - and particularly against the use of XML as a vehicle for configuration files (hello Java/Maven world!), I did not even try this path.
In other words, for OpenNLP I will be using the predefined features and have not experimented with the &quot;XML feature configuration&quot; option, so I cannot tell how well it works or how easy it is to use.
As for PoS tagging, OpenNLP uses pretty much the <em>de facto</em> standard features (prefixes, suffixes, orthographic features, and a window size of 5 [-2,+2] for the n-grams; see the <tt class="docutils literal">DefaultPOSContextGenerator</tt> class), so that seemed good enough for this test.</p>
<p><strong>CRF Suite</strong> comes with a set of Python scripts to convert simple OWPL files (one word [state] per line, with sentences [sequences] separated by an extra empty line, such as the CoNLL format) into the &quot;per-label feature list files&quot; CRF Suite uses as input.
To create different features, you modify the &quot;template&quot; defined inside the relevant Python script.
For most cases, I think the predefined templates do a pretty good job at generating features for standard NLP tagging tasks.
Additional features uniquely generated by CRF Suite and not OpenNLP or Factorie (for PoS tagging) are quadrigrams, pentagrams, and &quot;long-range interactions&quot;.
The latter are bigrams created from the current word and a word at position +/- 2 to 9 from the current word.
If you commonly work with Python, you might even easily assimilate the Python feature generation process, adapting it to your own needs.
CRF Suite's feature handling has an important shortcoming, however:
It is impossible to work with combined &quot;label bigrams&quot; (1st order Markov transition features) together with other (state) features from the token stream to form more advanced indicator functions.
In other words, CRF Suite (vs. Wapiti and Factorie, at least) cannot create mixed bigram label (transition) features with state elements from the token stream.
That is, CRF Suite only models either label transitions or the features from the current state, but does not allow you to model a feature as described in the beginning of this section (<tt class="docutils literal"><span class="pre">b:%s[-1,0]/%x[0,0]</span></tt>).
This might be important, because it is not possible to define features that condition on both the previous label and an element in the current sequence, including the current token itself.</p>
<p>As for <strong>Wapiti</strong>, you have to figure out how to generate features for each task on your own;
The authors do not provide any predefined templates for the tasks we are looking into.
But Wapiti provides you with a mechanism to define &quot;patterns&quot;, much like CRF++' feature templates.
Once you fully understand the mechanism, this is indeed quite powerful and it felt like I &quot;missed&quot; it in the other tools.
To provide an even playing field, at first I defined the same features &quot;patterns&quot; as CRF Suite does via its Python feature generation scripts.
The problem is that Wapiti uses all possible label and state combinations for its initial training matrix, not just all combinations present in the data.
In other words, it is the only tool that does no feature space reduction prior to going into training.
For example, if you define a pattern such as the current token (<tt class="docutils literal"><span class="pre">u:%[0,0]</span></tt>), it creates one feature for each label in you training set times the number of unique tokens in your data, no matter if a token is observed with that label in the data or not.
So for token n-grams or label bigrams, the training matrix can quickly grow to extraordinary sizes.
While Wapiti does compacting and supports sparse matrices during training, as it initially starts of with all features, training becomes rather sluggish when a very large feature space was defined.
By using the same feature templates as CRF Suite, I ended up with an initial matrix containing a few hundred million features.
This was too much for my weak dual-core i5 processor to handle in realistic time.
In the end, I decided to cut down on the number of PoS feature templates with respect to CRF Suite or Factorie.
In particular, I removed the unique feature templates only the CRF Suite has, reducing the initial search space to 44 million indicator functions.
After compacting, Wapiti's final model contained 1.8 million indicator functions (&quot;features functions&quot;, or worse, sometimes just called &quot;features&quot;) for the PoS tagging trials (see below).</p>
<p>In summary, with OpenNLP, Factorie, and CRF Suite you will need to work with their respective feature generation API (in Java, Scala, and Python, respectively) for anything but PoS tagging, phrase chunking, and very basic NER.
CRF Suite, similar to Factorie, has rich, pre-defined feature templates and can handle them, because unused indicator functions are dropped before the actual training starts, thereby keeping the initial feature matrix manageable.
Pre-training feature (space) <em>reduction</em> is done by all tools except Wapiti.
Feature <em>compaction</em> after training is done by all tools except OpenNLP, where I could not confirm if any compaction had occurred.
Wapiti provides a very powerful pattern language to define feature templates, including mixed state and transition label templates, which are otherwise only possible to generate with Factorie.
While Wapaiti's template (pattern) language lends to a great flexibility when modeling features, it has to be used with care unless training times are not an issue due to the maximal (non-reduced) feature matrix used for the first training cycles.
At the end of the day, in terms of feature generation, once you learn how to use Wapiti's pattern &quot;language&quot;, it will be very efficient and spares you from writing code.</p>
</div>
<div class="section" id="training-times">
<h2>Training Times</h2>
<p>Next, I looked into the training run-times to see how long it takes each tool to create a PoS model.
To make an equal, but simple comparison, I used the <a class="reference external" href="http://www.cnts.ua.ac.be/conll2000/chunking/">CoNLL 2000</a> PoS tags to train the models using the default feature templates of each tool as discussed above.
Both Factorie and OpenNLP needed slight, but simple modifications to the downloaded CoNLL files.
For Factorie, the reversed parenthesis tags in the CoNLL files had to be fixed.
The main observation here is that Factorie is not very helpful in terms of error messages to understand the problem.
OpenNLP's problem was simpler to identify: as documented, it expects one sentence per line, with token-tag pairs separated by underscores instead of the de facto standard OWPL format.
To train the the taggers for <em>Part-of-Speech</em>, the commands I used were:</p>
<pre class="literal-block">
# CRF Suite
crfsuite learn -m pos.model train.txt

# Factorie
java -Xmx2g -cp factorie-1.1-SNAPSHOT-nlp-jar-with-dependencies.jar \
     cc.factorie.app.nlp.pos.ForwardPosTrainer \
     --owpl --train-file=train.txt --test-file=empty.txt \
     --model=pos.model --save-model=true

# OpenNLP
opennlp POSTaggerTrainer -type maxent -model pos.model -data train.txt

# Wapiti
wapiti train -c -s -p patterns.txt train.txt pos.model
</pre>
<p>The input data is provided in <tt class="docutils literal">train.txt</tt>, and the models are saved to <tt class="docutils literal">pos.model</tt>.
To measure the training times, I prefixed each command with <tt class="docutils literal">time</tt> and used <a class="reference external" href="http://stackoverflow.com/a/556411">the sum of user+sys</a> as the measured, total time it took each process to complete.
This means, the measurement includes all relevant CPU time (i.e., over all processor cores) that was consumed by the run.
This might seem unfair to multi-threaded code, which might have an actual runtime lower than the result.
However, this is entirely depended on your machine and its cores, so a direct &quot;total CPU time&quot; comparison seemed fair to me.
There are other opinions about performance measurements, e.g., that one should only measure post-warm-up training time (minus JVM, input data reading, etc.) or only a single training cycle/iteration should be measured.
I think it is more practical to measure and compare whatever the &quot;out-of-the-box&quot; performance of each tool is.
Each training process is run thrice and the shortest measured time is the one I report here.</p>
<table border="1" class="docutils">
<colgroup>
<col width="28%" />
<col width="2%" />
<col width="28%" />
<col width="2%" />
<col width="40%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"><strong>Software</strong></th>
<th class="head">&nbsp;</th>
<th class="head"><strong>Features</strong></th>
<th class="head">&nbsp;</th>
<th class="head"><strong>Training Time</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr><td>CRF Suite</td>
<td>&nbsp;</td>
<td>3.63 M</td>
<td>&nbsp;</td>
<td>10m 22s</td>
</tr>
<tr><td>Factorie</td>
<td>&nbsp;</td>
<td>0.34 M</td>
<td>&nbsp;</td>
<td>02m 18s</td>
</tr>
<tr><td>OpenNLP</td>
<td>&nbsp;</td>
<td>???? M</td>
<td>&nbsp;</td>
<td>02m 03s</td>
</tr>
<tr><td>Wapiti</td>
<td>&nbsp;</td>
<td>1.56 M</td>
<td>&nbsp;</td>
<td>19m 06s</td>
</tr>
</tbody>
</table>
<p>As mentioned earlier, with respect to initial feature template richness, CRF Suite and Factorie are taking the lead, with Wapiti and OpenNLP using less templates.
The models' feature sizes shown here are as reported by each tool <em>after</em> all feature pruning steps (compaction;
For Wapiti that number is calculated from &quot;initial features&quot; minus &quot;removed features&quot;.)
While OpenNLP does not report final feature set sizes, I <em>assume</em> it to be in a similar range (somewhere around a million features).
So in terms of feature compaction, Factorie has a clear edge over the competition.</p>
<p>In terms of training times, Factorie and OpenNLP easily outpace both CRF Suite and Wapiti, but this should not be entirely surprising:
OpenNLP uses a simpler model (MEMM), so it clearly must be faster.
One noteworthy point is the training speed of Factorie - probably due to forward-only learning, it achieves similar training times for its CRF as OpenNLP on a MEMM.
On the other end, as expected, Wapiti is by far the most resource-hungry tagger.
While Wapiti's learning can be multi-threaded with a single switch, it was not clear to me how much actually gained in terms of absolute time from running Wapiti in that mode on my two (hyper-threaded) cores.
To me, this means the &quot;winner&quot; in this section (model training) is clearly Factorie, and the slowest implementation is Wapiti.
The remaining question in this respect will be if the Factorie model can keep up with the accuracy of the two CRFs and how fast it performs its tagging.</p>
</div>
<div class="section" id="tagging-quality">
<h2>Tagging Quality</h2>
<p>Finally, we are looking into the second main question (apart from feature generation) of this exercise:
Which implementation can provide you with the most efficient tagger?
I have a fast, SATA3-attached SSD drive where the data is read from (but a slow i5 CPU...) and took the CoNLL 2000 test set sequences to measure accuracy on the 47,377 tokens using the models I had trained in the last step.
So while my measurements do not include writing the tagged tokens back to the device, my CPU isn't exactly fast...
Then I timed each system while tagging 100 times those 47,377 tokens in a single row, read from one file (i.e., about 200,000 sentences or roughly 30,000 scientific abstracts) to make a fair comparison of each system's token throughput, marginalizing any warm-up &quot;penalties&quot;.</p>
<p>To run the the taggers on the generated models, the commands I used were:</p>
<pre class="literal-block">
# CRF Suite
cat test.txt | pos.py &gt; features.txt
crfsuite tag -m pos.model -tq features.txt

# Factorie
java -Xmx2g -cp factorie-1.1-SNAPSHOT-nlp-jar-with-dependencies.jar \
     cc.factorie.app.nlp.pos.ForwardPosTester \
     --owpl --model=pos.model --test-file=test.txt

# OpenNLP
opennlp POSTaggerEvaluator -model pos.model -data test.txt

# Wapiti
wapiti label -m pos.model -c test.txt &gt; /dev/null
</pre>
<p>And here are the results, in terms of error rate (<tt class="docutils literal">1 - accuracy</tt> over 47k tokens) and throughput (on 201k sentences with 4.7M tokens, as measured with <tt class="docutils literal">time</tt>, sys+user):</p>
<table border="1" class="docutils">
<colgroup>
<col width="27%" />
<col width="2%" />
<col width="31%" />
<col width="2%" />
<col width="38%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"><strong>Software</strong></th>
<th class="head">&nbsp;</th>
<th class="head"><strong>Error Rate</strong></th>
<th class="head">&nbsp;</th>
<th class="head"><strong>Tokens/Second</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr><td>CRF Suite</td>
<td>&nbsp;</td>
<td>3.00 %</td>
<td>&nbsp;</td>
<td>30,000</td>
</tr>
<tr><td>Factorie</td>
<td>&nbsp;</td>
<td>2.19 %</td>
<td>&nbsp;</td>
<td>23,800</td>
</tr>
<tr><td>OpenNLP</td>
<td>&nbsp;</td>
<td>2.88 %</td>
<td>&nbsp;</td>
<td>19,500</td>
</tr>
<tr><td>Wapiti</td>
<td>&nbsp;</td>
<td>2.20 %</td>
<td>&nbsp;</td>
<td>21,200</td>
</tr>
</tbody>
</table>
<p>Note that for the CRF Suite, I pre-generated the features from the Python script.
If I would not have separated out that step from its throughput, as the whole feature extraction process using the <tt class="docutils literal">pos.py</tt> script is several times slower than the actual tagger!
This is another indicator that the underlying Python code could probably use some polish...
In terms of tagging throughput, rather to my astonishment, it seems fair to say that the libraries perform roughly equal and there are by and large no noteworthy differences.
It seems that CRF Suite is faster, but then we actually cheated, because we pre-generated the label features.
So at best there is a minor chance that the CRF Suite could be faster than the others, if it had a very fast feature extraction mechanism.
Another remark maybe is that Factorie automatically detects the available cores and equally distributes the tagging load among them.
While the tagging process is an &quot;embarrassingly parallel&quot; problem (you could just split up the input between as many cores as you have and run a separate process for each), it is a nice little extra in the mix.
So in terms of <em>both</em> training time and tagging throughput Factorie seems the clear winner and definitely is ready-made for the multi-core area.</p>
<p>Probably due to the inability to use mixed features and because it does its feature compaction <em>prior</em> to the training, the CRF Suite has the worst performance in terms of accuracy, closely followed by OpenNLP.
So in terms of high quality tagging you will be better of with Factorie or Wapiti - not all that unexpected, given our feature modeling and model implementation discussions before.
You might want to know that a baseline tagger (using the majority PoS tag and tagging all unseen words as noun) already achieves an accuracy of 90% (or, an error rate of 10%) in standard PoS scenarios.</p>
</div>
<div class="section" id="conclusions">
<h2>Conclusions</h2>
<p>If you only need to do standard PoS tagging, chunking, and NER, and don't mind the tagging quality or performance too much, just go with the tool in your favorite language: OpenNLP for Java developers, Factorie for Scala hackers, and Wapiti for C/C++ or Python programmers
(There is a <a class="reference external" href="https://github.com/adsva/python-wapiti">Python wrapper</a> for Wapiti available, if Python (for both 2 and 3) is your deal.)
The trade-offs are simply not big enough to make a huge (order-of-magnitude) difference.
But then, if that were your case, you probably would not have read until here...
Because there is no conceivable advantage in terms of training times, tagging throughput, or accuracy, no support for mixed transition/state features, and as the code seems unmaintained, I would not recommend the use of CRF Suite, however.</p>
<p>At the end of the day, if you are interested in generating high-performance quality annotations while using mixture (state + transition) features, there really is only the choice between Wapiti and Factorie:
An error rate reduction of about 25% on the &quot;solved&quot; PoS tagging problem at no cost in throughput is not to be underestimated.
Wapiti definitely is the most attractive tagger in terms of off-the-shelf usability: feature generation is simple and there is no need for doing any coding, and the overall implementation complexity vs. usability tradeoff is excellent.
The only possible downside are significantly longer training times, so you will need to develop and combine your feature templates with care.
Finally, if you also are looking for true flexibility and the full power of graphical models, or want to risk using higher order Markov models, I see no way around Factorie.
In this case, it might be worth living with the sparse documentation and having to study Scala source code (with a &quot;scientist's handwriting&quot; ;-)).
However, the team around McCallum are very actively working on this library, so the code will hopefully become cleaner and more functional over time and the documentation is certainly getting better and more extensive every other time I come back and a few months have passed.
Nonetheless, if you'd ask me to declare a <em>global</em> &quot;winner&quot;, unless you are a probabilistic programming or at least Scala expert, I'd say that honor right now still goes to <a class="reference external" href="http://wapiti.limsi.fr/">Wapiti</a>, but once Factorie fixes the UI and documentation deficits, it certainly already has the lead in terms of model flexibility and performance.</p>
</div>


                    <div class="clear"></div>
                    <div class="info">
                        <div class="tags">                            <a href="http://fnl.es/tag/text-mining.html">text mining</a>                            <a href="http://fnl.es/tag/nlp.html" class="selected">nlp</a>                            <a href="http://fnl.es/tag/probabilistic-programming.html">probabilistic programming</a>                        </div>

                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    <a class="atom" href="http://fnl.es/feeds/all.atom.xml">RSS Feed</a>
                    </p>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    </body>
</html>