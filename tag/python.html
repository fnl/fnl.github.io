<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
				<meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>fnl.es &middot; articles tagged "python"</title>
        <link rel="shortcut icon" href="http://fnl.es/favicon.ico" />
<link href="http://fnl.es/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="fnl.es Atom Feed" />
        <link href="http://fnl.es/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="fnl.es RSS Feed" />

        <link rel="stylesheet" href="http://fnl.es/theme/css/screen.css" type="text/css" />
        <link rel="stylesheet" href="http://fnl.es/theme/css/pygments.css" type="text/css" />
        <!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=3728421; 
var sc_invisible=1; 
var sc_security="a5624e69"; 
</script>
<script type="text/javascript"
src="http://www.statcounter.com/counter/counter.js"></script>
        <!-- End of StatCounter Code for Default Guide -->
    </head>
    <body>
        <!-- Start of StatCounter Code for Default Guide -->
<noscript><div class="statcounter"><a title="hit counter"
href="http://statcounter.com/free-hit-counter/"
target="_blank"><img class="statcounter"
src="http://c.statcounter.com/3728421/0/a5624e69/1/"
alt="hit counter"></a></div></noscript>
        <!-- End of StatCounter Code for Default Guide -->
<div id="header">
            <ul id="nav">
                <li class="ephemeral selected"><a href="http://fnl.es/tag/python.html">python</a></li>
                <li><a class="flagred" href="http://fnl.es">Home</a></li>
<li><a class="flagyellow" href="http://fnl.es/pages/about.html">About</a></li>
<li><a class="flagyellow" href="http://fnl.es/pages/projects.html">Projects</a></li>
<li><a class="flagred" href="http://fnl.es/archives.html">Archives</a></li>
            </ul>
            <div class="header_box">
                <h1><a href="http://fnl.es">fnl.es</a></h1>
                <h2><a href="mailto:flo@fnl.es" id="webmaster">fnl</a> en Espa√±a</h2>            </div>
        </div>
        <div id="wrapper">
            <div id="content">
                <h4 class="date">Jul  7,  2014</h4>

								<div class="mobile-hide">
									<a class="twitter-timeline" width="180px" data-dnt="true" data-chrome="noheader nofooter transparent" href="https://twitter.com/flowing" data-widget-id="393739277646852096"></a>
									<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
								</div>
                <div class="post">
<h2 class="title">
                        <a href="http://fnl.es/an-introduction-to-statistical-text-mining.html" rel="bookmark" title="Permanent Link to &quot;An Introduction to Statistical Text Mining&quot;">An Introduction to Statistical Text Mining</a>
                    </h2>

                    <p>Last week we had a really great time at the first hands-on text mining workshop in the context of the <a class="reference external" href="http://www.dia.fi.upm.es/ASDM">Advanced Statistics and Data Mining</a> Summer School. This one-week course is an introduction to text mining from the &quot;bottom up&quot; to a bunch of motivated summer students, with the practical parts based on Python and the <a class="reference external" href="http://www.nltk.org/">NLTK</a>. The presentation was part of the 9th iteration of the Summer School that is located in the sunniest capital of Europe: Madrid (well, <em>ex aequo</em> with Athens, at least). In its context, I presented 15 hours worth of practical and theoretical background on machine learning for text mining to fourteen participants from all over the world. With this post I am sharing the slides and tutorial files (<a class="reference external" href="http://ipython.org/">IPython</a> Notebooks) with the world for free (but limited to non-commercial use - see the Creative Commons <a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/3.0/">BY-NC-SA</a> licensing details). While much of the material should speak for itself, it might not &quot;save&quot; you from visiting a text mining class (maybe mine, next summer?).</p>
<div class="section" id="overview">
<h2>Overview</h2>
<p>The theory mostly focuses on explaining the methods and statistics behind machine learning <em>for text mining</em> - without requiring a particular background other than sharp high-school maths. The practicals on the other hand make use of Python, particularly online <a class="reference external" href="http://ipython.org/">IPython</a> Notebooks. Therefore, prior contact with Python would be useful to profit the most from the practicals, but it is not required to follow the course. IPython's Notebooks act very much like MATLAB Notebooks, but run online in any modern browser, are based on open source software that requires no license fee, and provide facilities to place documentation and mathematical formulas between the code snippets, evaluation results, and graphical plots. Therefore, these notebooks serve as take home material for the students to study and play around with, even well after the course. To illustrate the power of IPython, learning how to work in this environment immediately enables you to run distributed, high-performance computations &quot;off the shelf&quot; (e.g., via <a class="reference external" href="http://star.mit.edu/cluster/">StarCluster</a> on Amazon EC2 Clusters).</p>
</div>
<div class="section" id="day-1-introduction">
<h2>Day 1 - Introduction</h2>
<p><a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/statistical-text-mining-introduction-florian-leitner">Lecture 1</a> starts with a light-weight introduction to text mining, natural language understanding and generation, and a statistics approach to artificial intelligence in general. It provides a taste of both what is to come in the course and what might be of interest to look into afterwards. It closes with a brief reprise of elementary Bayesian statistics and conditional probability, using the Monty Hall problem as a practical example of <em>Bayes' Rule</em>. The <a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/IPython%20Notebook%20Introduction.ipynb">practical #1</a> introduces the use of <a class="reference external" href="http://ipython.org/">IPython</a> and its online Notebook, as well as some aspects of <a class="reference external" href="http://www.numpy.org/">NumPy</a> (particularly, contrasting NumPy/SciPy to <a class="reference external" href="http://www.r-project.org/">R</a>). The most important aspects of the <strong>Natural Language ToolKit</strong> (<a class="reference external" href="http://www.nltk.org/">NLTK</a>) Python library are presented during the <a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Introduction%20to%20NumPy%20and%20NLTK.ipynb">second part</a> of the practical; The NTLK is frequently used during the course. As an exercise to become familiar with Python and the NLTK, participants are encouraged to implement a function to let two NLTK chat-bots converse between each other.</p>
</div>
<div class="section" id="day-2-language-modeling">
<h2>Day 2 - Language Modeling</h2>
<p>On the second day, <a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-25-language-modeling-florian-leitner">lecture 2</a> introduces the chain rule of conditional probability and builds on that to take you from the <em>Markov property</em> over <em>language modeling</em> to <em>smoothing techniques</em>. In class, you learn how to operate on n-gram frequency tables and we work out the conditional probability distributions of bi- and trigram models. This should ensure  students obtain well-grounded foundations of statistical language processing. After a <a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Building%20a%20Language%20Model%20in%207%20Steps.ipynb">quick overview</a>, the accompanying <a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Language%20Modelling%20with%20NLTK.ipynb">exercises #2</a> demonstrate how to build language models with the NLTK and participants are tasked with generating their own models using advanced smoothing techniques.</p>
</div>
<div class="section" id="day-3-string-processing">
<h2>Day 3 - String Processing</h2>
<p>The <a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-35-string-processing">third day lecture</a> focuses on <em>string processing</em> and the algorithmic aspects of text mining. The slides contain an overview of state machines, like regular expressions, PATRICIA tries, and <em>Minimal Acyclic Deterministic Finite [State] Automata</em> (MADFA). Particular attention is payed to string metrics and similarity measures. The last theoretical part is a thorough introduction to <em>Locality Sensitive Hashing</em> (LSH), and its use as a tool to efficiently cluster millions of documents or implement approximate string matching over very large-scale term dictionaries is discussed. During the <a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Locality%20Sensitive%20Hashing.ipynb">3rd exercises</a> students apply LSH to a toy problem: improving the speed of <a class="reference external" href="http://norvig.com/spell-correct.html">Peter Norvig</a>-style <a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Spelling%20Correction%20using%20LSH.ipynb">spelling correction</a>.</p>
</div>
<div class="section" id="day-4-text-classification">
<h2>Day 4 - Text Classification</h2>
<p>After the more algorithmic topics, the <a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-45-text-classification">lecture of day four</a> takes the participants into the realm of &quot;real&quot; machine learning. For starters, ways to calculate syntactic and semantic document similarity are presented, culminating in <em>Latent Semantic Analysis</em>. The slides introduce the Naive Bayes classifier as an example to prepare the audience for the <em>Maximum Entropy classifier</em> (Multinomial Logistic Regression), which forms the central topic of this session. To &quot;cool down&quot; a bit after a math-heavy section, <em>sentiment analysis</em> is discussed as a popular domain for text classification. Finally, the important topic of evaluating set-based classifier performance via Accuracy, F-measure, and MCC Score are discussed. The <a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Twitter%20Sentiment%20Analysis.ipynb">fourth practical</a> then walks the participants through a Maximum Entropy sentiment classifier and encourages the audience to improve its performance by designing better features and making clever use of all the gained knowledge in the course so far.</p>
</div>
<div class="section" id="day-5-graphical-models">
<h2>Day 5 - Graphical Models</h2>
<p>On the final day, <em>probabilistic graphical models</em> dominate the lecture. With the <a class="reference external" href="http://www.slideshare.net/asdkfjqlwef/text-mining-55-information-extraction">slides for day #5</a>, first the main differences between Bayesian Networks and Markov Random Fields are introduced. Connected to yesterday's text classification topics, <em>Latent Dirichlet Allocation</em> is presented as the first graphical model. Then, for the remainder of the talk, we move into <em>dynamic</em> (temporal) models and their applications. We revisit the Markov chain and go from <em>Hidden Markov Models</em> all the way to <em>Conditional Random Fields</em>. To gear participants up for practical text mining, we look into actual applications of the presented models, for example, (semantic) <em>Word Representations</em> or <em>Named Entity Recognition</em>. The participants are introduced to the prospect of using the assigned labels for more advanced tasks. For example, <em>Relationship Extraction</em> now becomes feasible by combining the shallow parse output as features for classifiers presented in other sessions of the Advanced Statistics and Data Mining Summer School. The rank-based performance measures of ROC and PR curves are discussed as way to evaluate the labeled results. During the <a class="reference external" href="http://nbviewer.ipython.org/github/fnl/asdm-tm-class/blob/master/Shallow%20Parsing%20with%20NLTK.ipynb">last practical</a>, implementing a <strong>shallow parser</strong> using NLTK and the <a class="reference external" href="http://nlp.stanford.edu/software/tagger.shtml">Stanford Taggers</a> is demonstrated in class. Overall, the practicals should have provide the students with the basic software tools to embark on their own text mining adventures right after the course.</p>
</div>
<div class="section" id="remarks">
<h2>Remarks</h2>
<p>You can <a class="reference external" href="mailto:florian.leitner&#64;gmail.com">contact me</a> if you would like to discuss having this tutorial in the context of your own venue or have questions and comments about the slides' content. Other than that, I hope to have awakened your interest in statistical text mining and/or to have provided you with useful material, both as a student or teacher.</p>
</div>


                    <div class="clear"></div>
                    <div class="info">
<a href="http://fnl.es/an-introduction-to-statistical-text-mining.html">posted at 12:00 P</a>&nbsp;&middot;&nbsp;<a href="http://fnl.es/category/misc.html" rel="tag">Misc</a>
                        <div class="tags">
                            <a href="http://fnl.es/tag/text-mining.html">text mining</a>
                            <a href="http://fnl.es/tag/python.html" class="selected">python</a>
                            <a href="http://fnl.es/tag/nltk.html">nltk</a>
                        </div>
                    </div>
                    <div class="clear"></div>
                </div>

                <div class="clear"></div>
                <div class="pages">
                    <a href="http://fnl.es/tag/python2.html" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 1 of 4</span>
                </div>

                <div class="clear"></div>
                <div id="footer">
                    <p>
                    <a class="atom" href="http://fnl.es/feeds/all.atom.xml">RSS Feed</a>
                    </p>
                </div>
            </div>
            <div class="clear"></div>
        </div>
    </body>
</html>